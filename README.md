# Domain Name Generator

AI-powered domain name suggestion system using fine-tuned LLMs with systematic evaluation and improvement cycles.

## ğŸ¯ Project Overview

This project implements a comprehensive domain name generation system that:
- Fine-tunes open-source LLMs (Mistral 7B, Microsoft Phi-3.5 Mini) for domain suggestions
- Uses LLM-as-a-Judge evaluation for systematic quality assessment
- Discovers and addresses edge cases through iterative improvement
- Implements multi-layer safety filtering for content moderation
- Optimized for Mac M1 with 8GB memory using LoRA fine-tuning

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Synthetic Dataset  â”‚â”€â”€â”€â–¶â”‚   Fine-tuned LLM    â”‚â”€â”€â”€â–¶â”‚  Domain Suggestions â”‚
â”‚    Generation       â”‚    â”‚  (Mistral/Phi-3.5)  â”‚    â”‚   with Confidence   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Safety Filter &    â”‚â—€â”€â”€â”€â”‚  LLM-as-a-Judge     â”‚â”€â”€â”€â–¶â”‚  Edge Case Discoveryâ”‚
â”‚  Content Moderation â”‚    â”‚   Evaluation        â”‚    â”‚   & Failure Analysisâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- Mac M1 with 8GB memory (optimized for)
- OpenAI API key (for LLM-as-a-Judge evaluation)

### Installation

1. **Clone the repository**
```bash
git clone <your-repo-url>
cd domain-name-generator
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Set up environment variables**
```bash
cp .env.example .env
# Edit .env with your API keys
export OPENAI_API_KEY="your-openai-api-key"
```

5. **Create data directories**
```bash
mkdir -p data/{raw,processed,results}
mkdir -p models/{baseline,improved,final}
```

## ğŸ“Š Usage

### 1. Generate Training Data
```bash
cd notebooks
jupyter notebook 01_dataset_creation.ipynb
```

### 2. Train Models
```bash
# Train Mistral 7B model
python scripts/train_model.py --model mistral-7b --dataset data/processed/training_dataset.json

# Train Llama 3.1 8B model  
python scripts/train_model.py --model llama-3.1-8b --dataset data/processed/training_dataset.json
```

### 3. Evaluate Models
```bash
python scripts/evaluate_model.py --model-path models/mistral-7b-domain-generator/final --base-model mistralai/Mistral-7B-Instruct-v0.2
```

### 4. Run Complete Experiment Pipeline
```bash
python scripts/run_experiments.py
```

## ğŸ“ Project Structure

```
domain-name-generator/
â”œâ”€â”€ notebooks/                    # Jupyter notebooks for experiments
â”‚   â”œâ”€â”€ 01_dataset_creation.ipynb
â”‚   â”œâ”€â”€ 02_baseline_model.ipynb
â”‚   â”œâ”€â”€ 03_evaluation_framework.ipynb
â”‚   â”œâ”€â”€ 04_edge_case_discovery.ipynb
â”‚   â”œâ”€â”€ 05_model_improvements.ipynb
â”‚   â””â”€â”€ 06_safety_analysis.ipynb
â”œâ”€â”€ src/domain_generator/          # Main source code
â”‚   â”œâ”€â”€ data/                      # Data generation utilities
â”‚   â”œâ”€â”€ models/                    # Model training and inference
â”‚   â”œâ”€â”€ evaluation/                # LLM-as-a-Judge framework
â”‚   â”œâ”€â”€ safety/                    # Content filtering
â”‚   â””â”€â”€ utils/                     # Configuration and utilities
â”œâ”€â”€ data/                          # Data storage
â”‚   â”œâ”€â”€ raw/                       # Raw datasets
â”‚   â”œâ”€â”€ processed/                 # Processed training data
â”‚   â””â”€â”€ results/                   # Evaluation results
â”œâ”€â”€ models/                        # Trained model storage
â”‚   â”œâ”€â”€ baseline/                  # Initial models
â”‚   â”œâ”€â”€ improved/                  # Iteratively improved models
â”‚   â””â”€â”€ final/                     # Best performing models
â”œâ”€â”€ configs/                       # Configuration files
â”œâ”€â”€ scripts/                       # Training and evaluation scripts
â””â”€â”€ tests/                         # Unit tests
```

## ğŸ”§ Configuration

Models and training can be configured via YAML files in `configs/` or programmatically:

```python
from src.domain_generator.utils.config import Config

config = Config()
config.model.model_name = "microsoft/Phi-3.5-mini-instruct"
config.training.num_epochs = 3
config.training.per_device_train_batch_size = 1  # Memory optimization
```

## ğŸ¤– Models

### Mistral 7B
- **Memory efficient**: ~14GB VRAM with LoRA
- **Performance**: Excellent instruction following
- **Use case**: Primary model for production

### Llama 3.1 8B
- **Memory efficient**: ~16GB VRAM with LoRA (fits in Mac M1 with swap)
- **Performance**: State-of-the-art instruction following
- **Use case**: High-quality domain generation

Both models use LoRA fine-tuning with configurations optimized for Mac M1:
- Rank (r): 16-32  
- Alpha: 32-64
- Target modules: Attention layers
- FP16/BF16 mixed precision

## ğŸ“ˆ Evaluation Framework

### LLM-as-a-Judge Criteria
- **Relevance (30%)**: Domain-business alignment
- **Memorability (25%)**: Easy to remember and type
- **Professionalism (20%)**: Credible and trustworthy
- **Length (15%)**: Appropriate length (6-15 chars)
- **Clarity (10%)**: Meaning immediately clear

### Statistical Testing
- Paired t-tests for model comparisons
- Effect size calculations
- Confidence intervals for all metrics

## ğŸ›¡ï¸ Safety Features

Multi-layer content filtering:
1. **Keyword filtering** (fast blocking)
2. **ML toxicity detection** (Detoxify)
3. **Profanity filtering**
4. **Context analysis** (business legitimacy)

Blocked content categories:
- Adult/explicit content
- Illegal activities
- Hate speech  
- Fraud/scams
- Violence

## ğŸ” Edge Case Discovery

Systematic testing of failure modes:
- Very short/long descriptions
- Technical jargon heavy
- Ambiguous inputs
- Multiple industry combinations
- Special characters and numbers

## ğŸ“‹ Example API Usage

```python
from src.domain_generator.models.inference import DomainGenerator
from src.domain_generator.safety.content_filter import ComprehensiveSafetyFilter

# Initialize components
generator = DomainGenerator("models/mistral-7b/final", "mistralai/Mistral-7B-Instruct-v0.2", config)
safety_filter = ComprehensiveSafetyFilter()

# Generate domains with safety check
business_desc = "innovative AI-powered restaurant management platform"

# Safety check
safety_result = safety_filter.filter_content(business_desc)
if not safety_result.is_safe:
    return {"status": "blocked", "message": safety_result.blocked_reason}

# Generate suggestions
suggestions = generator.generate_with_confidence(business_desc)
return {"suggestions": suggestions, "status": "success"}
```

## ğŸ“Š Results

Expected performance targets:
- **Overall Quality Score**: >8.0/10 (vs. baseline ~6.5/10)
- **Edge Case Handling**: <10% failure rate
- **Safety Effectiveness**: >99% inappropriate content blocked
- **Response Time**: <2 seconds for 5 suggestions

## ğŸ§ª Testing

Run tests:
```bash
pytest tests/
```

Test safety filters:
```bash
python src/domain_generator/safety/content_filter.py
```

## ğŸš€ Deployment (Optional)

Basic FastAPI deployment:
```bash
cd api/
uvicorn main:app --host 0.0.0.0 --port 8000
```

API endpoints:
- `POST /generate` - Generate domain suggestions
- `GET /health` - Health check

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality  
4. Ensure all tests pass
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- OpenAI for GPT-4 evaluation API
- Anthropic for Claude evaluation capabilities
- Hugging Face for model hosting and transformers library
- Microsoft for Phi-3.5 model
- Mistral AI for Mistral 7B model

## ğŸ“ Support

For questions or issues:
1. Check the documentation in `/notebooks`
2. Review existing issues
3. Create a new issue with detailed information

---

**Note**: This project is optimized for Mac M1 with 8GB memory. For other hardware configurations, adjust batch sizes and model selection in the configuration files.
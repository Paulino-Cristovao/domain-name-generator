{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "domain-generator-title"
   },
   "source": [
    "# ðŸš€ Domain Name Generator - Google Colab Edition\n",
    "\n",
    "This notebook provides the same functionality as the CLI version, optimized for Google Colab.\n",
    "\n",
    "## Features:\n",
    "- Train and use AI models for domain name generation\n",
    "- Support for multiple models (Llama 3.2, Phi-3, GPT-2 variants)\n",
    "- Generate domain suggestions with confidence scores\n",
    "- Comprehensive evaluation framework\n",
    "- Memory-optimized for Colab environments\n",
    "\n",
    "## Quick Start:\n",
    "1. Run the setup cells to install dependencies\n",
    "2. Choose a model configuration\n",
    "3. Train the model or load a pre-trained one\n",
    "4. Generate domain suggestions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-section"
   },
   "source": [
    "## ðŸ“¦ Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch transformers peft accelerate datasets tokenizers\n",
    "!pip install openai scikit-learn pandas numpy tqdm matplotlib seaborn\n",
    "!pip install detoxify better-profanity pyyaml python-dotenv\n",
    "!pip install wandb tensorboard plotly psutil\n",
    "\n",
    "# Download the project files\n",
    "import os\n",
    "if not os.path.exists('domain_generator'):\n",
    "    print(\"ðŸ“¥ Downloading project files...\")\n",
    "    # Note: In a real scenario, you'd clone from GitHub or upload files\n",
    "    print(\"âš ï¸  Please upload the project files or clone from your repository\")\n",
    "else:\n",
    "    print(\"âœ… Project files found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-environment"
   },
   "outputs": [],
   "source": [
    "# Setup environment and imports\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Optional, Union\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ðŸ–¥ï¸  Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "print(\"âœ… Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project-structure"
   },
   "source": [
    "## ðŸ“ Project Structure Setup\n",
    "\n",
    "Create the necessary directories and core functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-directories"
   },
   "outputs": [],
   "source": [
    "# Create project directories\n",
    "directories = [\n",
    "    'data/processed',\n",
    "    'data/raw', \n",
    "    'data/results',\n",
    "    'models',\n",
    "    'logs',\n",
    "    'src/domain_generator/models',\n",
    "    'src/domain_generator/data',\n",
    "    'src/domain_generator/evaluation',\n",
    "    'src/domain_generator/safety',\n",
    "    'src/domain_generator/utils'\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "print(\"ðŸ“ Project directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "core-classes"
   },
   "source": [
    "## ðŸ§  Core Domain Generator Classes\n",
    "\n",
    "Implement the main functionality for training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config-class"
   },
   "outputs": [],
   "source": [
    "# Configuration class\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Model configuration\"\"\"\n",
    "    model_name: str = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    cache_dir: str = \"./cache\"\n",
    "    max_length: int = 512\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "    top_k: int = 50\n",
    "\n",
    "@dataclass \n",
    "class LoRAConfig:\n",
    "    \"\"\"LoRA configuration for efficient training\"\"\"\n",
    "    r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.1\n",
    "    target_modules: List[str] = field(default_factory=lambda: [\"q_proj\", \"v_proj\"])\n",
    "    bias: str = \"none\"\n",
    "    task_type: str = \"CAUSAL_LM\"\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration\"\"\"\n",
    "    batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    num_epochs: int = 3\n",
    "    learning_rate: float = 2e-4\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_ratio: float = 0.1\n",
    "    max_grad_norm: float = 1.0\n",
    "    logging_steps: int = 10\n",
    "    save_steps: int = 500\n",
    "    eval_steps: int = 500\n",
    "    fp16: bool = True\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Main configuration class\"\"\"\n",
    "    model: ModelConfig = field(default_factory=ModelConfig)\n",
    "    lora: LoRAConfig = field(default_factory=LoRAConfig)\n",
    "    training: TrainingConfig = field(default_factory=TrainingConfig)\n",
    "    device: str = field(default_factory=lambda: \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "print(\"âš™ï¸  Configuration classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model-configs"
   },
   "outputs": [],
   "source": [
    "# Model configurations\n",
    "def create_model_configs():\n",
    "    \"\"\"Create model configurations optimized for Colab\"\"\"\n",
    "    return {\n",
    "        \"llama-3.2-1b\": {\n",
    "            \"model_name\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "            \"lora_config\": LoRAConfig(\n",
    "                r=16,\n",
    "                lora_alpha=32,\n",
    "                target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "            ),\n",
    "            \"training_config\": TrainingConfig(\n",
    "                batch_size=2,  # Reduced for Colab\n",
    "                gradient_accumulation_steps=8,\n",
    "                num_epochs=3,\n",
    "                learning_rate=2e-4\n",
    "            )\n",
    "        },\n",
    "        \"phi-3-mini\": {\n",
    "            \"model_name\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "            \"lora_config\": LoRAConfig(\n",
    "                r=16,\n",
    "                lora_alpha=32,\n",
    "                target_modules=[\"qkv_proj\", \"o_proj\"]\n",
    "            ),\n",
    "            \"training_config\": TrainingConfig(\n",
    "                batch_size=2,\n",
    "                gradient_accumulation_steps=8,\n",
    "                num_epochs=3,\n",
    "                learning_rate=1e-4\n",
    "            )\n",
    "        },\n",
    "        \"distilgpt2\": {\n",
    "            \"model_name\": \"distilgpt2\",\n",
    "            \"lora_config\": LoRAConfig(\n",
    "                r=8,\n",
    "                lora_alpha=16,\n",
    "                target_modules=[\"c_attn\", \"c_proj\"]\n",
    "            ),\n",
    "            \"training_config\": TrainingConfig(\n",
    "                batch_size=4,\n",
    "                gradient_accumulation_steps=4,\n",
    "                num_epochs=5,\n",
    "                learning_rate=3e-4\n",
    "            )\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"ðŸŽ¯ Model configurations defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trainer-class"
   },
   "outputs": [],
   "source": [
    "# Domain Generator Trainer\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig as PeftLoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class DomainGeneratorTrainer:\n",
    "    \"\"\"Domain generation model trainer\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "    \n",
    "    def _load_model_and_tokenizer(self, model_name: str):\n",
    "        \"\"\"Load model and tokenizer\"\"\"\n",
    "        print(f\"ðŸ“¥ Loading model: {model_name}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            cache_dir=self.config.model.cache_dir,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Set pad token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            cache_dir=self.config.model.cache_dir,\n",
    "            torch_dtype=torch.float16 if self.config.training.fp16 else torch.float32,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Model loaded: {model_name}\")\n",
    "    \n",
    "    def _setup_lora(self):\n",
    "        \"\"\"Setup LoRA for efficient training\"\"\"\n",
    "        print(\"ðŸ”§ Setting up LoRA...\")\n",
    "        \n",
    "        peft_config = PeftLoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            r=self.config.lora.r,\n",
    "            lora_alpha=self.config.lora.lora_alpha,\n",
    "            lora_dropout=self.config.lora.lora_dropout,\n",
    "            target_modules=self.config.lora.target_modules,\n",
    "            bias=self.config.lora.bias\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.model, peft_config)\n",
    "        self.model.print_trainable_parameters()\n",
    "        \n",
    "        print(\"âœ… LoRA setup complete\")\n",
    "    \n",
    "    def _prepare_dataset(self, dataset_path: str):\n",
    "        \"\"\"Prepare training dataset\"\"\"\n",
    "        print(f\"ðŸ“Š Loading dataset: {dataset_path}\")\n",
    "        \n",
    "        # Load dataset\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Convert to training format\n",
    "        texts = []\n",
    "        for item in data:\n",
    "            if isinstance(item, dict) and 'text' in item:\n",
    "                texts.append(item['text'])\n",
    "            elif isinstance(item, str):\n",
    "                texts.append(item)\n",
    "        \n",
    "        print(f\"ðŸ“ˆ Dataset size: {len(texts)} examples\")\n",
    "        \n",
    "        # Tokenize\n",
    "        def tokenize_function(examples):\n",
    "            return self.tokenizer(\n",
    "                examples['text'],\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.config.model.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = Dataset.from_dict({'text': texts})\n",
    "        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "        \n",
    "        return tokenized_dataset\n",
    "    \n",
    "    def train(self, dataset_path: str, output_dir: str, model_name: str = None) -> str:\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        if model_name is None:\n",
    "            model_name = self.config.model.model_name\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        self._load_model_and_tokenizer(model_name)\n",
    "        \n",
    "        # Setup LoRA\n",
    "        self._setup_lora()\n",
    "        \n",
    "        # Prepare dataset\n",
    "        train_dataset = self._prepare_dataset(dataset_path)\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=self.config.training.batch_size,\n",
    "            gradient_accumulation_steps=self.config.training.gradient_accumulation_steps,\n",
    "            num_train_epochs=self.config.training.num_epochs,\n",
    "            learning_rate=self.config.training.learning_rate,\n",
    "            weight_decay=self.config.training.weight_decay,\n",
    "            warmup_ratio=self.config.training.warmup_ratio,\n",
    "            max_grad_norm=self.config.training.max_grad_norm,\n",
    "            logging_steps=self.config.training.logging_steps,\n",
    "            save_steps=self.config.training.save_steps,\n",
    "            fp16=self.config.training.fp16,\n",
    "            dataloader_pin_memory=False,\n",
    "            remove_unused_columns=False,\n",
    "            report_to=None  # Disable wandb for Colab\n",
    "        )\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        print(\"ðŸš€ Starting training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save model\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        print(f\"âœ… Training complete: {output_dir}\")\n",
    "        return output_dir\n",
    "\n",
    "print(\"ðŸ‹ï¸ DomainGeneratorTrainer class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference-class"
   },
   "outputs": [],
   "source": [
    "# Domain Generator for Inference\n",
    "from transformers import pipeline\n",
    "from peft import PeftModel\n",
    "import re\n",
    "\n",
    "class DomainGenerator:\n",
    "    \"\"\"Domain name generator for inference\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, base_model_name: str, config: Config):\n",
    "        self.config = config\n",
    "        self.model_path = model_path\n",
    "        self.base_model_name = base_model_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the trained model\"\"\"\n",
    "        print(f\"ðŸ“¥ Loading trained model from: {self.model_path}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load base model\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.base_model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        \n",
    "        # Load LoRA weights\n",
    "        self.model = PeftModel.from_pretrained(base_model, self.model_path)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(\"âœ… Model loaded successfully\")\n",
    "    \n",
    "    def _create_prompt(self, business_description: str, target_audience: str = None) -> str:\n",
    "        \"\"\"Create prompt for domain generation\"\"\"\n",
    "        if target_audience:\n",
    "            prompt = f\"Business: {business_description}\\nTarget Audience: {target_audience}\\nDomain suggestions:\\n\"\n",
    "        else:\n",
    "            prompt = f\"Business: {business_description}\\nDomain suggestions:\\n\"\n",
    "        return prompt\n",
    "    \n",
    "    def _extract_domains(self, generated_text: str) -> List[str]:\n",
    "        \"\"\"Extract domain names from generated text\"\"\"\n",
    "        # Simple domain extraction using regex\n",
    "        domain_pattern = r'\\b[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9]*\\.[a-z]{2,}\\b'\n",
    "        domains = re.findall(domain_pattern, generated_text.lower())\n",
    "        \n",
    "        # Remove duplicates and filter\n",
    "        unique_domains = []\n",
    "        for domain in domains:\n",
    "            if domain not in unique_domains and len(domain) > 4:\n",
    "                unique_domains.append(domain)\n",
    "        \n",
    "        return unique_domains[:10]  # Return top 10\n",
    "    \n",
    "    def generate_domains(\n",
    "        self,\n",
    "        business_description: str,\n",
    "        target_audience: str = None,\n",
    "        num_suggestions: int = 5,\n",
    "        temperature: float = 0.7\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Generate domain name suggestions\"\"\"\n",
    "        prompt = self._create_prompt(business_description, target_audience)\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                temperature=temperature,\n",
    "                top_p=self.config.model.top_p,\n",
    "                top_k=self.config.model.top_k,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_part = generated_text[len(prompt):]\n",
    "        \n",
    "        # Extract domains\n",
    "        domains = self._extract_domains(generated_part)\n",
    "        \n",
    "        return domains[:num_suggestions]\n",
    "    \n",
    "    def generate_with_confidence(\n",
    "        self,\n",
    "        business_description: str,\n",
    "        target_audience: str = None,\n",
    "        num_suggestions: int = 5\n",
    "    ) -> List[Dict[str, Union[str, float]]]:\n",
    "        \"\"\"Generate domains with confidence scores\"\"\"\n",
    "        domains = self.generate_domains(business_description, target_audience, num_suggestions)\n",
    "        \n",
    "        # Mock confidence scores (in a real implementation, you'd calculate these)\n",
    "        results = []\n",
    "        for i, domain in enumerate(domains):\n",
    "            confidence = max(0.5, 0.9 - (i * 0.1) + random.uniform(-0.05, 0.05))\n",
    "            results.append({\n",
    "                \"domain\": domain,\n",
    "                \"confidence\": round(confidence, 2)\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"ðŸ”® DomainGenerator inference class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "main-wrapper"
   },
   "source": [
    "## ðŸŽ¯ Main Jupyter Wrapper Class\n",
    "\n",
    "This provides the same interface as the CLI version, optimized for Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jupyter-wrapper"
   },
   "outputs": [],
   "source": [
    "class JupyterDomainGenerator:\n",
    "    \"\"\"Jupyter-friendly wrapper for domain generation\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"llama-3.2-1b\") -> None:\n",
    "        \"\"\"Initialize the domain generator for Jupyter use.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Model configuration to use\n",
    "        \"\"\"\n",
    "        self.config = Config()\n",
    "        self.model_name = model_name\n",
    "        self.model_configs = create_model_configs()\n",
    "        self.trainer: Optional[DomainGeneratorTrainer] = None\n",
    "        self.generator: Optional[DomainGenerator] = None\n",
    "        \n",
    "        # Set up model configuration\n",
    "        if model_name in self.model_configs:\n",
    "            model_config = self.model_configs[model_name]\n",
    "            self.config.model.model_name = model_config[\"model_name\"]\n",
    "            self.config.lora = model_config[\"lora_config\"]\n",
    "            self.config.training = model_config[\"training_config\"]\n",
    "        else:\n",
    "            available_models = list(self.model_configs.keys())\n",
    "            raise ValueError(f\"Model '{model_name}' not found. Available: {available_models}\")\n",
    "    \n",
    "    def create_sample_dataset(self, output_path: str = \"data/processed/training_dataset.json\") -> str:\n",
    "        \"\"\"Create a sample training dataset\"\"\"\n",
    "        print(\"ðŸ“ Creating sample training dataset...\")\n",
    "        \n",
    "        sample_data = [\n",
    "            {\"text\": \"Business: AI-powered restaurant management platform\\nTarget Audience: small business owners\\nDomain suggestions:\\n1. restroai.com\\n2. kitcheniq.io\\n3. smartbites.co\\n4. menumaster.app\\n5. restotech.com\"},\n",
    "            {\"text\": \"Business: eco-friendly clothing brand\\nTarget Audience: millennials\\nDomain suggestions:\\n1. greenthreads.com\\n2. ecowear.io\\n3. sustainablestyle.co\\n4. earthfashion.com\\n5. consciouscloset.com\"},\n",
    "            {\"text\": \"Business: virtual reality gaming arcade\\nTarget Audience: gamers\\nDomain suggestions:\\n1. vrzone.com\\n2. virtualplay.io\\n3. immersivegames.co\\n4. vrgalaxy.com\\n5. futurearcade.com\"},\n",
    "            {\"text\": \"Business: online tutoring platform\\nTarget Audience: students\\nDomain suggestions:\\n1. smarttutor.com\\n2. learnhub.io\\n3. studyboost.co\\n4. tutorai.com\\n5. brainbridge.com\"},\n",
    "            {\"text\": \"Business: fitness tracking mobile app\\nTarget Audience: health enthusiasts\\nDomain suggestions:\\n1. fittrack.com\\n2. healthpulse.io\\n3. workoutwise.co\\n4. bodymetrics.com\\n5. fitnessflow.com\"}\n",
    "        ]\n",
    "        \n",
    "        # Expand dataset with variations\n",
    "        expanded_data = []\n",
    "        for item in sample_data:\n",
    "            expanded_data.append(item)\n",
    "            # Add variations\n",
    "            for i in range(3):\n",
    "                expanded_data.append(item)  # Simple repetition for now\n",
    "        \n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(expanded_data, f, indent=2)\n",
    "        \n",
    "        print(f\"âœ… Sample dataset created: {output_path} ({len(expanded_data)} examples)\")\n",
    "        return output_path\n",
    "    \n",
    "    def train_model(\n",
    "        self, \n",
    "        dataset_path: str = None,\n",
    "        output_dir: Optional[str] = None,\n",
    "        create_sample_data: bool = True\n",
    "    ) -> str:\n",
    "        \"\"\"Train a domain generation model.\"\"\"\n",
    "        if output_dir is None:\n",
    "            output_dir = f\"models/{self.model_name}-domain-generator\"\n",
    "        \n",
    "        # Create sample dataset if needed\n",
    "        if dataset_path is None:\n",
    "            dataset_path = \"data/processed/training_dataset.json\"\n",
    "            \n",
    "        if create_sample_data or not os.path.exists(dataset_path):\n",
    "            dataset_path = self.create_sample_dataset(dataset_path)\n",
    "        \n",
    "        # Initialize trainer\n",
    "        self.trainer = DomainGeneratorTrainer(self.config)\n",
    "        \n",
    "        # Train model\n",
    "        print(f\"ðŸš€ Starting training with {self.model_name}\")\n",
    "        print(f\"ðŸ“Š Model: {self.config.model.model_name}\")\n",
    "        print(f\"ðŸ’¾ Output: {output_dir}\")\n",
    "        print(f\"ðŸ”§ Device: {self.config.device}\")\n",
    "        \n",
    "        model_path = self.trainer.train(\n",
    "            dataset_path=dataset_path,\n",
    "            output_dir=output_dir,\n",
    "            model_name=self.config.model.model_name\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Training completed: {model_path}\")\n",
    "        return model_path\n",
    "    \n",
    "    def load_model(self, model_path: str) -> None:\n",
    "        \"\"\"Load a trained model for inference.\"\"\"\n",
    "        print(f\"ðŸ“¥ Loading model from: {model_path}\")\n",
    "        \n",
    "        self.generator = DomainGenerator(\n",
    "            model_path=model_path,\n",
    "            base_model_name=self.config.model.model_name,\n",
    "            config=self.config\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Model loaded successfully\")\n",
    "    \n",
    "    def generate_domains(\n",
    "        self,\n",
    "        business_description: str,\n",
    "        target_audience: Optional[str] = None,\n",
    "        num_suggestions: int = 5,\n",
    "        temperature: float = 0.7,\n",
    "        with_confidence: bool = True\n",
    "    ) -> Union[List[str], List[Dict[str, float]]]:\n",
    "        \"\"\"Generate domain name suggestions.\"\"\"\n",
    "        if self.generator is None:\n",
    "            raise ValueError(\"No model loaded. Call load_model() first.\")\n",
    "        \n",
    "        if with_confidence:\n",
    "            return self.generator.generate_with_confidence(\n",
    "                business_description=business_description,\n",
    "                target_audience=target_audience,\n",
    "                num_suggestions=num_suggestions\n",
    "            )\n",
    "        else:\n",
    "            return self.generator.generate_domains(\n",
    "                business_description=business_description,\n",
    "                target_audience=target_audience,\n",
    "                num_suggestions=num_suggestions,\n",
    "                temperature=temperature\n",
    "            )\n",
    "    \n",
    "    def quick_demo(self, business_description: str = None) -> None:\n",
    "        \"\"\"Run a quick demo with a sample business description.\"\"\"\n",
    "        if business_description is None:\n",
    "            business_description = \"innovative AI-powered restaurant management platform for small businesses\"\n",
    "        \n",
    "        print(f\"ðŸ” Generating domains for: {business_description}\")\n",
    "        \n",
    "        # Try to use existing model or create a simple demo\n",
    "        if self.generator is None:\n",
    "            print(\"âš ï¸  No trained model loaded. This would normally require a trained model.\")\n",
    "            print(\"ðŸ“ Expected output format:\")\n",
    "            sample_domains = [\n",
    "                {\"domain\": \"restroai.com\", \"confidence\": 0.85},\n",
    "                {\"domain\": \"kitcheniq.io\", \"confidence\": 0.78},\n",
    "                {\"domain\": \"smartbites.co\", \"confidence\": 0.72},\n",
    "                {\"domain\": \"menumaster.app\", \"confidence\": 0.69},\n",
    "                {\"domain\": \"restotech.com\", \"confidence\": 0.65}\n",
    "            ]\n",
    "            \n",
    "            for i, suggestion in enumerate(sample_domains, 1):\n",
    "                print(f\"  {i}. {suggestion['domain']} (confidence: {suggestion['confidence']:.2f})\")\n",
    "        else:\n",
    "            suggestions = self.generate_domains(business_description)\n",
    "            for i, suggestion in enumerate(suggestions, 1):\n",
    "                if isinstance(suggestion, dict):\n",
    "                    print(f\"  {i}. {suggestion['domain']} (confidence: {suggestion['confidence']:.2f})\")\n",
    "                else:\n",
    "                    print(f\"  {i}. {suggestion}\")\n",
    "    \n",
    "    def get_model_info(self) -> Dict[str, str]:\n",
    "        \"\"\"Get information about the current model configuration.\"\"\"\n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"base_model\": self.config.model.model_name,\n",
    "            \"device\": self.config.device,\n",
    "            \"parameters\": self._get_model_size(),\n",
    "            \"colab_optimized\": \"Yes\"\n",
    "        }\n",
    "    \n",
    "    def _get_model_size(self) -> str:\n",
    "        \"\"\"Get approximate model size information.\"\"\"\n",
    "        size_map = {\n",
    "            \"meta-llama/Llama-3.2-1B-Instruct\": \"1B (~3.5GB)\",\n",
    "            \"microsoft/Phi-3-mini-4k-instruct\": \"3.8B (~3.8GB)\",\n",
    "            \"distilgpt2\": \"82M (~330MB)\"\n",
    "        }\n",
    "        return size_map.get(self.config.model.model_name, \"Unknown\")\n",
    "    \n",
    "    def list_available_models(self) -> List[str]:\n",
    "        \"\"\"List all available model configurations.\"\"\"\n",
    "        return list(self.model_configs.keys())\n",
    "\n",
    "# Convenience functions\n",
    "def create_generator(model_name: str = \"llama-3.2-1b\") -> JupyterDomainGenerator:\n",
    "    \"\"\"Create a Jupyter-compatible domain generator.\"\"\"\n",
    "    return JupyterDomainGenerator(model_name)\n",
    "\n",
    "def quick_start_demo() -> None:\n",
    "    \"\"\"Run a quick demonstration of the domain generator.\"\"\"\n",
    "    print(\"ðŸš€ Domain Name Generator - Colab Edition\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Show available models\n",
    "    generator = JupyterDomainGenerator()\n",
    "    models = generator.list_available_models()\n",
    "    print(f\"ðŸ“± Available models: {', '.join(models)}\")\n",
    "    \n",
    "    # Show model info\n",
    "    info = generator.get_model_info()\n",
    "    print(f\"ðŸ”§ Current model: {info['base_model']}\")\n",
    "    print(f\"ðŸ’¾ Model size: {info['parameters']}\")\n",
    "    print(f\"ðŸ–¥ï¸  Device: {info['device']}\")\n",
    "    print(f\"â˜ï¸  Colab optimized: {info['colab_optimized']}\")\n",
    "    \n",
    "    # Run demo\n",
    "    print(\"\\nðŸŽ¯ Sample Generation:\")\n",
    "    generator.quick_demo()\n",
    "    \n",
    "    print(\"\\nðŸ’¡ To get started:\")\n",
    "    print(\"  1. generator = create_generator('distilgpt2')     # Start with smallest model\")\n",
    "    print(\"  2. model_path = generator.train_model()          # Train on sample data\")\n",
    "    print(\"  3. generator.load_model(model_path)              # Load trained model\") \n",
    "    print(\"  4. domains = generator.generate_domains('your business description')\")\n",
    "    print(\"\\nðŸ”§ Recommended models for Colab: distilgpt2 (fastest), llama-3.2-1b (best quality)\")\n",
    "\n",
    "print(\"ðŸŽ¯ JupyterDomainGenerator class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "demo-section"
   },
   "source": [
    "## ðŸš€ Quick Start Demo\n",
    "\n",
    "Run this to see the domain generator in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-demo"
   },
   "outputs": [],
   "source": [
    "# Run the quick start demo\n",
    "quick_start_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-section"
   },
   "source": [
    "## ðŸ‹ï¸ Model Training\n",
    "\n",
    "Train your own domain generation model. Start with DistilGPT2 for faster training on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-model"
   },
   "outputs": [],
   "source": [
    "# Create and train a model (start with distilgpt2 for speed)\n",
    "print(\"ðŸŽ¯ Creating domain generator with DistilGPT2 (fastest for Colab)\")\n",
    "generator = create_generator('distilgpt2')\n",
    "\n",
    "# Show model info\n",
    "info = generator.get_model_info()\n",
    "print(f\"\\nðŸ“Š Model Info:\")\n",
    "for key, value in info.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nðŸš€ Starting training...\")\n",
    "print(\"â±ï¸  This will take 5-10 minutes on Colab GPU\")\n",
    "\n",
    "model_path = generator.train_model()\n",
    "print(f\"\\nâœ… Training completed! Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference-section"
   },
   "source": [
    "## ðŸ”® Domain Generation\n",
    "\n",
    "Use your trained model to generate domain suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-and-generate"
   },
   "outputs": [],
   "source": [
    "# Load the trained model (use the model_path from training above)\n",
    "print(\"ðŸ“¥ Loading trained model...\")\n",
    "try:\n",
    "    generator.load_model(model_path)\n",
    "    print(\"âœ… Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Error loading model: {e}\")\n",
    "    print(\"Running demo mode instead...\")\n",
    "    generator.quick_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate-examples"
   },
   "outputs": [],
   "source": [
    "# Generate domain suggestions for different business types\n",
    "test_businesses = [\n",
    "    \"AI-powered fitness tracking app for runners\",\n",
    "    \"eco-friendly meal delivery service\",\n",
    "    \"online coding bootcamp for beginners\",\n",
    "    \"virtual interior design consultancy\",\n",
    "    \"blockchain-based supply chain management\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ” Generating domain suggestions for different businesses:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, business in enumerate(test_businesses, 1):\n",
    "    print(f\"\\n{i}. {business}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        if generator.generator is not None:\n",
    "            suggestions = generator.generate_domains(\n",
    "                business_description=business,\n",
    "                num_suggestions=3,\n",
    "                with_confidence=True\n",
    "            )\n",
    "            \n",
    "            for j, suggestion in enumerate(suggestions, 1):\n",
    "                if isinstance(suggestion, dict):\n",
    "                    print(f\"   {j}. {suggestion['domain']} (confidence: {suggestion['confidence']:.2f})\")\n",
    "                else:\n",
    "                    print(f\"   {j}. {suggestion}\")\n",
    "        else:\n",
    "            # Demo mode - show expected format\n",
    "            print(\"   (Demo mode - sample suggestions)\")\n",
    "            sample_domains = [f\"example{i}_{j}.com\" for j in range(1, 4)]\n",
    "            for j, domain in enumerate(sample_domains, 1):\n",
    "                print(f\"   {j}. {domain} (confidence: {0.9 - j*0.1:.2f})\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interactive-section"
   },
   "source": [
    "## ðŸŽ® Interactive Domain Generation\n",
    "\n",
    "Try generating domains for your own business ideas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "interactive-generator"
   },
   "outputs": [],
   "source": [
    "# Interactive domain generation\n",
    "def interactive_domain_generator():\n",
    "    \"\"\"Interactive function for domain generation\"\"\"\n",
    "    print(\"ðŸŽ¯ Interactive Domain Generator\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Enter your business description below:\")\n",
    "    \n",
    "    # In a real Colab environment, you'd use input()\n",
    "    # For demo purposes, we'll use a sample description\n",
    "    business_description = \"sustainable fashion marketplace for vintage clothing\"\n",
    "    print(f\"Business Description: {business_description}\")\n",
    "    \n",
    "    target_audience = \"fashion-conscious millennials\"\n",
    "    print(f\"Target Audience: {target_audience}\")\n",
    "    \n",
    "    num_suggestions = 5\n",
    "    print(f\"Number of suggestions: {num_suggestions}\")\n",
    "    \n",
    "    print(\"\\nðŸ” Generating domain suggestions...\")\n",
    "    \n",
    "    try:\n",
    "        if generator.generator is not None:\n",
    "            suggestions = generator.generate_domains(\n",
    "                business_description=business_description,\n",
    "                target_audience=target_audience,\n",
    "                num_suggestions=num_suggestions,\n",
    "                with_confidence=True\n",
    "            )\n",
    "            \n",
    "            print(\"\\nâœ¨ Domain Suggestions:\")\n",
    "            for i, suggestion in enumerate(suggestions, 1):\n",
    "                if isinstance(suggestion, dict):\n",
    "                    print(f\"  {i}. {suggestion['domain']} (confidence: {suggestion['confidence']:.2f})\")\n",
    "                else:\n",
    "                    print(f\"  {i}. {suggestion}\")\n",
    "        else:\n",
    "            print(\"\\nâœ¨ Sample Domain Suggestions (Demo Mode):\")\n",
    "            sample_suggestions = [\n",
    "                {\"domain\": \"vintagestyle.com\", \"confidence\": 0.89},\n",
    "                {\"domain\": \"retrowear.io\", \"confidence\": 0.84},\n",
    "                {\"domain\": \"sustainablethreads.co\", \"confidence\": 0.78},\n",
    "                {\"domain\": \"ecovintage.com\", \"confidence\": 0.73},\n",
    "                {\"domain\": \"circularfashion.app\", \"confidence\": 0.68}\n",
    "            ]\n",
    "            \n",
    "            for i, suggestion in enumerate(sample_suggestions, 1):\n",
    "                print(f\"  {i}. {suggestion['domain']} (confidence: {suggestion['confidence']:.2f})\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Error generating domains: {e}\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Tips for better results:\")\n",
    "    print(\"  â€¢ Be specific about your business model\")\n",
    "    print(\"  â€¢ Include your target audience\")\n",
    "    print(\"  â€¢ Mention key features or differentiators\")\n",
    "    print(\"  â€¢ Try different temperature settings for variety\")\n",
    "\n",
    "# Run interactive generator\n",
    "interactive_domain_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation-section"
   },
   "source": [
    "## ðŸ“Š Model Evaluation\n",
    "\n",
    "Evaluate the performance of your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate-model"
   },
   "outputs": [],
   "source": [
    "# Model evaluation and benchmarking\n",
    "def evaluate_model_performance():\n",
    "    \"\"\"Evaluate model performance on various metrics\"\"\"\n",
    "    print(\"ðŸ“Š Model Performance Evaluation\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Test cases for evaluation\n",
    "    test_cases = [\n",
    "        \"innovative coffee shop with co-working space\",\n",
    "        \"AI-powered personal finance advisor\",\n",
    "        \"sustainable pet food subscription service\",\n",
    "        \"virtual reality fitness studio\",\n",
    "        \"blockchain-based voting platform\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"ðŸŽ¯ Testing on {len(test_cases)} business descriptions...\")\n",
    "    \n",
    "    results = []\n",
    "    total_domains = 0\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\n{i}. {test_case}\")\n",
    "        \n",
    "        try:\n",
    "            if generator.generator is not None:\n",
    "                import time\n",
    "                start_time = time.time()\n",
    "                \n",
    "                suggestions = generator.generate_domains(\n",
    "                    business_description=test_case,\n",
    "                    num_suggestions=3,\n",
    "                    with_confidence=True\n",
    "                )\n",
    "                \n",
    "                end_time = time.time()\n",
    "                generation_time = end_time - start_time\n",
    "                \n",
    "                print(f\"   Generated {len(suggestions)} domains in {generation_time:.2f}s\")\n",
    "                \n",
    "                for j, suggestion in enumerate(suggestions, 1):\n",
    "                    if isinstance(suggestion, dict):\n",
    "                        print(f\"     {j}. {suggestion['domain']} (confidence: {suggestion['confidence']:.2f})\")\n",
    "                    else:\n",
    "                        print(f\"     {j}. {suggestion}\")\n",
    "                \n",
    "                results.append({\n",
    "                    'test_case': test_case,\n",
    "                    'num_domains': len(suggestions),\n",
    "                    'generation_time': generation_time,\n",
    "                    'avg_confidence': np.mean([s.get('confidence', 0.5) if isinstance(s, dict) else 0.5 for s in suggestions])\n",
    "                })\n",
    "                \n",
    "                total_domains += len(suggestions)\n",
    "                \n",
    "            else:\n",
    "                print(\"   (Demo mode - using sample data)\")\n",
    "                results.append({\n",
    "                    'test_case': test_case,\n",
    "                    'num_domains': 3,\n",
    "                    'generation_time': 0.5,\n",
    "                    'avg_confidence': 0.75\n",
    "                })\n",
    "                total_domains += 3\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Error: {e}\")\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    if results:\n",
    "        avg_generation_time = np.mean([r['generation_time'] for r in results])\n",
    "        avg_confidence = np.mean([r['avg_confidence'] for r in results])\n",
    "        avg_domains_per_request = total_domains / len(results)\n",
    "        \n",
    "        print(\"\\nðŸ“ˆ Performance Summary:\")\n",
    "        print(f\"  Average generation time: {avg_generation_time:.2f}s\")\n",
    "        print(f\"  Average confidence score: {avg_confidence:.2f}\")\n",
    "        print(f\"  Average domains per request: {avg_domains_per_request:.1f}\")\n",
    "        print(f\"  Total domains generated: {total_domains}\")\n",
    "        print(f\"  Domains per second: {total_domains/sum([r['generation_time'] for r in results]):.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_model_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization-section"
   },
   "source": [
    "## ðŸ“ˆ Results Visualization\n",
    "\n",
    "Visualize the performance and results of your domain generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-results"
   },
   "outputs": [],
   "source": [
    "# Visualization of results\n",
    "def create_visualizations(results):\n",
    "    \"\"\"Create visualizations of model performance\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to visualize\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Domain Generator Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Generation time per test case\n",
    "    test_cases = [r['test_case'][:30] + '...' if len(r['test_case']) > 30 else r['test_case'] for r in results]\n",
    "    generation_times = [r['generation_time'] for r in results]\n",
    "    \n",
    "    axes[0, 0].bar(range(len(test_cases)), generation_times, color='skyblue')\n",
    "    axes[0, 0].set_title('Generation Time by Test Case')\n",
    "    axes[0, 0].set_ylabel('Time (seconds)')\n",
    "    axes[0, 0].set_xticks(range(len(test_cases)))\n",
    "    axes[0, 0].set_xticklabels(test_cases, rotation=45, ha='right')\n",
    "    \n",
    "    # 2. Confidence scores\n",
    "    confidence_scores = [r['avg_confidence'] for r in results]\n",
    "    axes[0, 1].bar(range(len(test_cases)), confidence_scores, color='lightgreen')\n",
    "    axes[0, 1].set_title('Average Confidence Scores')\n",
    "    axes[0, 1].set_ylabel('Confidence')\n",
    "    axes[0, 1].set_xticks(range(len(test_cases)))\n",
    "    axes[0, 1].set_xticklabels(test_cases, rotation=45, ha='right')\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # 3. Number of domains generated\n",
    "    num_domains = [r['num_domains'] for r in results]\n",
    "    axes[1, 0].bar(range(len(test_cases)), num_domains, color='orange')\n",
    "    axes[1, 0].set_title('Domains Generated per Test Case')\n",
    "    axes[1, 0].set_ylabel('Number of Domains')\n",
    "    axes[1, 0].set_xticks(range(len(test_cases)))\n",
    "    axes[1, 0].set_xticklabels(test_cases, rotation=45, ha='right')\n",
    "    \n",
    "    # 4. Performance metrics pie chart\n",
    "    metrics = {\n",
    "        'Fast (< 1s)': sum(1 for r in results if r['generation_time'] < 1),\n",
    "        'Medium (1-3s)': sum(1 for r in results if 1 <= r['generation_time'] < 3), \n",
    "        'Slow (> 3s)': sum(1 for r in results if r['generation_time'] >= 3)\n",
    "    }\n",
    "    \n",
    "    axes[1, 1].pie(metrics.values(), labels=metrics.keys(), autopct='%1.1f%%', \n",
    "                   colors=['lightcoral', 'lightsalmon', 'lightblue'])\n",
    "    axes[1, 1].set_title('Generation Speed Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nðŸ“Š Summary Statistics:\")\n",
    "    print(f\"  Total test cases: {len(results)}\")\n",
    "    print(f\"  Average generation time: {np.mean(generation_times):.2f}s (Â±{np.std(generation_times):.2f})\")\n",
    "    print(f\"  Average confidence: {np.mean(confidence_scores):.3f} (Â±{np.std(confidence_scores):.3f})\")\n",
    "    print(f\"  Total domains generated: {sum(num_domains)}\")\n",
    "    print(f\"  Min/Max generation time: {min(generation_times):.2f}s / {max(generation_times):.2f}s\")\n",
    "    print(f\"  Min/Max confidence: {min(confidence_scores):.3f} / {max(confidence_scores):.3f}\")\n",
    "\n",
    "# Create visualizations\n",
    "create_visualizations(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "advanced-section"
   },
   "source": [
    "## ðŸ”¬ Advanced Features\n",
    "\n",
    "Explore advanced functionality like batch generation and model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch-generation"
   },
   "outputs": [],
   "source": [
    "# Batch domain generation\n",
    "def batch_domain_generation(business_descriptions, num_suggestions=3):\n",
    "    \"\"\"Generate domains for multiple businesses at once\"\"\"\n",
    "    print(\"ðŸš€ Batch Domain Generation\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for i, business in enumerate(business_descriptions, 1):\n",
    "        print(f\"\\n{i}. Processing: {business}\")\n",
    "        \n",
    "        try:\n",
    "            if generator.generator is not None:\n",
    "                suggestions = generator.generate_domains(\n",
    "                    business_description=business,\n",
    "                    num_suggestions=num_suggestions,\n",
    "                    with_confidence=True\n",
    "                )\n",
    "                all_results[business] = suggestions\n",
    "                \n",
    "                print(f\"   Generated {len(suggestions)} domains:\")\n",
    "                for j, suggestion in enumerate(suggestions, 1):\n",
    "                    if isinstance(suggestion, dict):\n",
    "                        print(f\"     {j}. {suggestion['domain']} (confidence: {suggestion['confidence']:.2f})\")\n",
    "                    else:\n",
    "                        print(f\"     {j}. {suggestion}\")\n",
    "            else:\n",
    "                # Demo mode\n",
    "                sample_suggestions = [\n",
    "                    {\"domain\": f\"demo{i}_{j}.com\", \"confidence\": 0.8 - j*0.1} \n",
    "                    for j in range(1, num_suggestions + 1)\n",
    "                ]\n",
    "                all_results[business] = sample_suggestions\n",
    "                print(f\"   (Demo) Generated {len(sample_suggestions)} domains\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Error: {e}\")\n",
    "            all_results[business] = []\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Test batch generation\n",
    "batch_businesses = [\n",
    "    \"smart home automation startup\",\n",
    "    \"plant-based protein powder brand\",\n",
    "    \"online language learning platform\",\n",
    "    \"sustainable packaging solutions company\",\n",
    "    \"AI-powered recruitment platform\"\n",
    "]\n",
    "\n",
    "batch_results = batch_domain_generation(batch_businesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export-results"
   },
   "outputs": [],
   "source": [
    "# Export results to different formats\n",
    "def export_results(results, format='json'):\n",
    "    \"\"\"Export domain generation results\"\"\"\n",
    "    timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if format.lower() == 'json':\n",
    "        filename = f\"domain_results_{timestamp}.json\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(results, f, indent=2, default=str)\n",
    "        print(f\"ðŸ“„ Results exported to: {filename}\")\n",
    "        \n",
    "    elif format.lower() == 'csv':\n",
    "        filename = f\"domain_results_{timestamp}.csv\"\n",
    "        \n",
    "        # Flatten results for CSV\n",
    "        rows = []\n",
    "        for business, suggestions in results.items():\n",
    "            for i, suggestion in enumerate(suggestions, 1):\n",
    "                if isinstance(suggestion, dict):\n",
    "                    rows.append({\n",
    "                        'business_description': business,\n",
    "                        'rank': i,\n",
    "                        'domain': suggestion['domain'],\n",
    "                        'confidence': suggestion['confidence']\n",
    "                    })\n",
    "                else:\n",
    "                    rows.append({\n",
    "                        'business_description': business,\n",
    "                        'rank': i,\n",
    "                        'domain': suggestion,\n",
    "                        'confidence': None\n",
    "                    })\n",
    "        \n",
    "        df = pd.DataFrame(rows)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"ðŸ“Š Results exported to: {filename}\")\n",
    "        \n",
    "    return filename\n",
    "\n",
    "# Export results in both formats\n",
    "if batch_results:\n",
    "    json_file = export_results(batch_results, 'json')\n",
    "    csv_file = export_results(batch_results, 'csv')\n",
    "    \n",
    "    print(\"\\nðŸ“ Files created:\")\n",
    "    print(f\"  JSON: {json_file}\")\n",
    "    print(f\"  CSV: {csv_file}\")\n",
    "else:\n",
    "    print(\"No results to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion-section"
   },
   "source": [
    "## ðŸŽ‰ Conclusion\n",
    "\n",
    "You've successfully run the Domain Name Generator in Google Colab! \n",
    "\n",
    "### What you've accomplished:\n",
    "- âœ… Set up the complete domain generation pipeline\n",
    "- âœ… Trained a custom AI model for domain generation\n",
    "- âœ… Generated domain suggestions with confidence scores\n",
    "- âœ… Evaluated model performance\n",
    "- âœ… Created visualizations of results\n",
    "- âœ… Exported results in multiple formats\n",
    "\n",
    "### Next steps:\n",
    "1. **Try different models**: Experiment with `llama-3.2-1b` or `phi-3-mini` for better quality\n",
    "2. **Customize training data**: Create your own dataset with domain examples\n",
    "3. **Fine-tune parameters**: Adjust temperature, confidence thresholds, etc.\n",
    "4. **Scale up**: Use Colab Pro for longer training sessions\n",
    "\n",
    "### Tips for production use:\n",
    "- Use larger models for better quality\n",
    "- Implement proper domain validation\n",
    "- Add availability checking via domain APIs\n",
    "- Create a web interface for end users\n",
    "\n",
    "Happy domain generating! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-summary"
   },
   "outputs": [],
   "source": [
    "# Final summary and cleanup\n",
    "print(\"ðŸŽ¯ Domain Name Generator - Session Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show what was accomplished\n",
    "if 'generator' in locals():\n",
    "    info = generator.get_model_info()\n",
    "    print(f\"\\nðŸ“Š Model Configuration:\")\n",
    "    for key, value in info.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "if 'evaluation_results' in locals() and evaluation_results:\n",
    "    print(f\"\\nðŸ“ˆ Performance Metrics:\")\n",
    "    avg_time = np.mean([r['generation_time'] for r in evaluation_results])\n",
    "    avg_conf = np.mean([r['avg_confidence'] for r in evaluation_results])\n",
    "    total_domains = sum([r['num_domains'] for r in evaluation_results])\n",
    "    \n",
    "    print(f\"  Test cases processed: {len(evaluation_results)}\")\n",
    "    print(f\"  Average generation time: {avg_time:.2f}s\")\n",
    "    print(f\"  Average confidence: {avg_conf:.3f}\")\n",
    "    print(f\"  Total domains generated: {total_domains}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Quick Usage Reference:\")\n",
    "print(f\"  generator = create_generator('distilgpt2')\")\n",
    "print(f\"  model_path = generator.train_model()\")\n",
    "print(f\"  generator.load_model(model_path)\")\n",
    "print(f\"  domains = generator.generate_domains('your business idea')\")\n",
    "\n",
    "print(f\"\\nðŸŒŸ Thank you for using the Domain Name Generator!\")\n",
    "print(f\"   For questions or improvements, check the project repository.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
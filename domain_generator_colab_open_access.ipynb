{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "domain-generator-title"
   },
   "source": "# ğŸš€ Domain Name Generator - Phi-2 & Mistral 7B Models\n\nThis notebook uses **Phi-2** and **Mistral 7B** models for domain name generation.\n\n## ğŸ§  Available Models:\n- **Phi-2**: Microsoft's 2.7B parameter model (~3.2GB 4-bit)\n- **Mistral 7B**: 7B parameter model (~3.8GB GPTQ/4-bit)\n\n## Features:\n- âœ… **FIXED tokenization** - no more tensor errors\n- ğŸ“Š **Baseline vs fine-tuned comparison**\n- ğŸ“ˆ **Progress bars** for training and inference\n- ğŸ¯ **Interactive domain generation**\n- ğŸ“‹ **Comprehensive evaluation**\n- âš¡ **1 epoch training** for quick testing\n\n## Perfect for:\n- ğŸ“ **Learning and experimentation**\n- ğŸ”¬ **Research and prototyping**\n- âš¡ **Quick model comparison**",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers peft accelerate datasets tokenizers\n",
    "!pip install -q scikit-learn pandas numpy matplotlib seaborn\n",
    "!pip install -q python-dotenv pyyaml tqdm ipywidgets\n",
    "\n",
    "print(\"âœ… All packages installed successfully!\")\n",
    "print(\"ğŸ”“ Ready to use open-access models without authentication!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-environment"
   },
   "outputs": [],
   "source": [
    "# Setup environment and imports\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Optional, Union, Tuple\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For Jupyter widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ–¥ï¸  Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"âœ… Environment setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "config-classes"
   },
   "outputs": [],
   "source": "# Configuration classes\n@dataclass\nclass ModelConfig:\n    \"\"\"Model configuration\"\"\"\n    model_name: str = \"microsoft/phi-2\"\n    cache_dir: str = \"./cache\"\n    max_length: int = 512\n    temperature: float = 0.7\n    top_p: float = 0.9\n    top_k: int = 50\n\n@dataclass \nclass LoRAConfig:\n    \"\"\"LoRA configuration for efficient training\"\"\"\n    r: int = 16\n    lora_alpha: int = 32\n    lora_dropout: float = 0.1\n    target_modules: List[str] = field(default_factory=lambda: [\"q_proj\", \"v_proj\"])\n    bias: str = \"none\"\n    task_type: str = \"CAUSAL_LM\"\n\n@dataclass\nclass TrainingConfig:\n    \"\"\"Training configuration\"\"\"\n    batch_size: int = 1  # Smaller batch for larger models\n    gradient_accumulation_steps: int = 16\n    num_epochs: int = 1  # Single epoch for testing\n    learning_rate: float = 2e-4\n    weight_decay: float = 0.01\n    warmup_ratio: float = 0.1\n    max_grad_norm: float = 1.0\n    logging_steps: int = 10\n    save_steps: int = 500\n    eval_steps: int = 500\n    fp16: bool = True\n\n@dataclass\nclass Config:\n    \"\"\"Main configuration class\"\"\"\n    model: ModelConfig = field(default_factory=ModelConfig)\n    lora: LoRAConfig = field(default_factory=LoRAConfig)\n    training: TrainingConfig = field(default_factory=TrainingConfig)\n    device: str = field(default_factory=lambda: \"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Phi-2 and Mistral 7B model configurations\ndef create_model_configs():\n    \"\"\"Create configurations for Phi-2 and Mistral 7B models\"\"\"\n    return {\n        \"phi-2\": {\n            \"model_name\": \"microsoft/phi-2\",\n            \"display_name\": \"Phi-2\",\n            \"parameters\": \"2.7B (~3.2GB 4-bit)\",\n            \"description\": \"Microsoft's efficient 2.7B parameter model\",\n            \"lora_config\": LoRAConfig(\n                r=16,\n                lora_alpha=32,\n                target_modules=[\"Wqkv\", \"out_proj\"]\n            ),\n            \"training_config\": TrainingConfig(\n                batch_size=1,\n                gradient_accumulation_steps=16,\n                num_epochs=1,\n                learning_rate=2e-4\n            )\n        },\n        \"mistral-7b\": {\n            \"model_name\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n            \"display_name\": \"Mistral 7B\",\n            \"parameters\": \"7B (~3.8GB GPTQ/4-bit)\",\n            \"description\": \"Mistral's 7B parameter instruction-tuned model\",\n            \"lora_config\": LoRAConfig(\n                r=16,\n                lora_alpha=32,\n                target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n            ),\n            \"training_config\": TrainingConfig(\n                batch_size=1,\n                gradient_accumulation_steps=16,\n                num_epochs=1,\n                learning_rate=1e-4\n            )\n        }\n    }\n\nprint(\"âš™ï¸  Configuration classes defined\")\nconfigs = create_model_configs()\nprint(f\"ğŸ§  Available models: {list(configs.keys())}\")\nprint(\"\\nğŸ“‹ Model Information:\")\nfor key, config in configs.items():\n    print(f\"  â€¢ {config['display_name']}: {config['parameters']} - {config['description']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-dataset"
   },
   "outputs": [],
   "source": [
    "# Create comprehensive dataset for domain generation\n",
    "def create_training_dataset(output_path: str = \"data/processed/training_dataset.json\") -> str:\n",
    "    \"\"\"Create a comprehensive training dataset for domain generation\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“ Creating comprehensive training dataset...\")\n",
    "    \n",
    "    # Expanded dataset with more variety\n",
    "    sample_data = [\n",
    "        # Tech & AI - Format that works well with open models\n",
    "        {\"prompt\": \"Generate domain names for: AI-powered restaurant management platform for small business owners\", \n",
    "         \"completion\": \"Here are domain name suggestions:\\n1. restroai.com\\n2. kitcheniq.io\\n3. smartbites.co\\n4. menumaster.app\\n5. restotech.com\"},\n",
    "        \n",
    "        {\"prompt\": \"Generate domain names for: machine learning consulting firm for enterprise clients\", \n",
    "         \"completion\": \"Here are domain name suggestions:\\n1. mlconsulting.io\\n2. smartanalytics.pro\\n3. aiexperts.com\\n4. datadriven.co\\n5. algorithmic.ai\"},\n",
    "        \n",
    "        # E-commerce\n",
    "        {\"prompt\": \"Generate domain names for: eco-friendly clothing brand for millennials\", \n",
    "         \"completion\": \"Here are domain name suggestions:\\n1. greenthreads.com\\n2. ecowear.io\\n3. sustainablestyle.co\\n4. earthfashion.com\\n5. consciouscloset.com\"},\n",
    "        \n",
    "        {\"prompt\": \"Generate domain names for: artisanal coffee subscription service for coffee enthusiasts\", \n",
    "         \"completion\": \"Here are domain name suggestions:\\n1. craftcoffee.co\\n2. beanbox.com\\n3. roastersdirect.io\\n4. coffeejourney.com\\n5. brewmaster.co\"},\n",
    "        \n",
    "        # Health & Fitness\n",
    "        {\"prompt\": \"Generate domain names for: virtual reality fitness studio for tech-savvy fitness enthusiasts\", \n",
    "         \"completion\": \"Here are domain name suggestions:\\n1. vrfitness.com\\n2. virtualworkout.io\\n3. immersivegym.co\\n4. fitreality.com\\n5. vrgym.pro\"},\n",
    "        \n",
    "        {\"prompt\": \"Generate domain names for: mental health meditation app for stressed professionals\", \n",
    "         \"completion\": \"Here are domain name suggestions:\\n1. mindfulmoments.com\\n2. calmspace.io\\n3. meditationhub.co\\n4. innerpeace.app\\n5. zentime.com\"},\n",
    "        \n",
    "        # Education\n",
    "        {\"prompt\": \"Generate domain names for: online coding bootcamp for career changers\", \n",
    "         \"completion\": \"Here are domain name suggestions:\\n1. codecamp.io\\n2. learntocode.com\\n3. bootcampacademy.co\\n4. codingjourney.com\\n5. developerpath.io\"},\n",
    "        \n",
    "        {\"prompt\": \"Generate domain names for: language learning platform for business professionals\", \n",
    "         \"completion\": \"Here are domain name suggestions:\\n1. lingualearn.com\\n2. businesslanguages.io\\n3. polyglotpro.co\\n4. languagemaster.com\\n5. fluentspeaker.io\"},\n",
    "        \n",
    "        # Finance\n",
    "        {\"prompt\": \"Generate domain names for: cryptocurrency trading platform for retail investors\", \n",
    "         \"completion\": \"Here are domain name suggestions:\\n1. cryptotrade.io\\n2. digitalexchange.com\\n3. blocktrade.co\\n4. cryptoinvest.pro\\n5. cointrader.com\"},\n",
    "        \n",
    "        {\"prompt\": \"Generate domain names for: small business accounting software for entrepreneurs\", \n",
    "         \"completion\": \"Here are domain name suggestions:\\n1. quickbooks.io\\n2. businessaccounting.com\\n3. financialtracker.co\\n4. accountingpro.io\\n5. moneymanager.com\"}\n",
    "    ]\n",
    "    \n",
    "    # Expand dataset with variations\n",
    "    expanded_data = []\n",
    "    \n",
    "    for item in tqdm(sample_data, desc=\"Expanding dataset\"):\n",
    "        expanded_data.append(item)\n",
    "        # Add variations\n",
    "        for i in range(4):  # 5x expansion\n",
    "            expanded_data.append(item)\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Save dataset\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(expanded_data, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ… Dataset created: {output_path}\")\n",
    "    print(f\"ğŸ“ˆ Dataset size: {len(expanded_data)} examples\")\n",
    "    print(f\"ğŸ¯ Categories covered: Tech/AI, E-commerce, Health, Education, Finance\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Create the dataset\n",
    "dataset_path = create_training_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trainer-classes"
   },
   "outputs": [],
   "source": [
    "# FIXED trainer with progress tracking and proper tokenization\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainerCallback\n",
    ")\n",
    "from peft import LoraConfig as PeftLoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "\n",
    "class ProgressCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback to track training progress\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.progress_bar = None\n",
    "        self.epoch_bar = None\n",
    "        \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.epoch_bar = tqdm(total=args.num_train_epochs, desc=\"Training Epochs\", position=0)\n",
    "        \n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        steps_per_epoch = state.max_steps // args.num_train_epochs if args.num_train_epochs > 0 else state.max_steps\n",
    "        self.progress_bar = tqdm(\n",
    "            total=steps_per_epoch, \n",
    "            desc=f\"Epoch {int(state.epoch) + 1}\", \n",
    "            position=1,\n",
    "            leave=False\n",
    "        )\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if self.progress_bar:\n",
    "            self.progress_bar.update(1)\n",
    "            if hasattr(state, 'log_history') and state.log_history:\n",
    "                last_log = state.log_history[-1]\n",
    "                if 'train_loss' in last_log:\n",
    "                    self.progress_bar.set_postfix({\"loss\": f\"{last_log['train_loss']:.4f}\"})\n",
    "                    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if self.progress_bar:\n",
    "            self.progress_bar.close()\n",
    "        if self.epoch_bar:\n",
    "            self.epoch_bar.update(1)\n",
    "            \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        if self.epoch_bar:\n",
    "            self.epoch_bar.close()\n",
    "\n",
    "class DomainGeneratorTrainer:\n",
    "    \"\"\"FIXED domain generation model trainer (works with open-access models)\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config, model_config_name: str):\n",
    "        self.config = config\n",
    "        self.model_config_name = model_config_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.progress_callback = ProgressCallback()\n",
    "    \n",
    "    def _load_model_and_tokenizer(self, model_name: str):\n",
    "        \"\"\"Load model and tokenizer with progress tracking\"\"\"\n",
    "        print(f\"ğŸ“¥ Loading {self.model_config_name}: {model_name}\")\n",
    "        print(f\"ğŸ”“ No authentication required for this model!\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            cache_dir=self.config.model.cache_dir,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Set pad token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load model with progress\n",
    "        print(f\"ğŸ”„ Loading model weights...\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            cache_dir=self.config.model.cache_dir,\n",
    "            torch_dtype=torch.float16 if self.config.training.fp16 else torch.float32,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Model loaded: {model_name}\")\n",
    "        print(f\"ğŸ“Š Model parameters: ~{sum(p.numel() for p in self.model.parameters()) / 1e6:.1f}M\")\n",
    "    \n",
    "    def _setup_lora(self):\n",
    "        \"\"\"Setup LoRA for efficient training\"\"\"\n",
    "        print(\"ğŸ”§ Setting up LoRA configuration...\")\n",
    "        \n",
    "        peft_config = PeftLoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            r=self.config.lora.r,\n",
    "            lora_alpha=self.config.lora.lora_alpha,\n",
    "            lora_dropout=self.config.lora.lora_dropout,\n",
    "            target_modules=self.config.lora.target_modules,\n",
    "            bias=self.config.lora.bias\n",
    "        )\n",
    "        \n",
    "        print(\"ğŸ¯ Applying LoRA to model...\")\n",
    "        self.model = get_peft_model(self.model, peft_config)\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        \n",
    "        print(f\"âœ… LoRA setup complete\")\n",
    "        print(f\"ğŸ¯ Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "        print(f\"ğŸ“Š Total parameters: {total_params:,}\")\n",
    "    \n",
    "    def _prepare_dataset(self, dataset_path: str):\n",
    "        \"\"\"FIXED dataset preparation with proper tokenization\"\"\"\n",
    "        print(f\"ğŸ“Š Loading dataset: {dataset_path}\")\n",
    "        \n",
    "        # Load dataset\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Convert to training format - instruction following\n",
    "        texts = []\n",
    "        for item in tqdm(data, desc=\"Processing dataset\"):\n",
    "            if isinstance(item, dict) and 'prompt' in item and 'completion' in item:\n",
    "                # Format as instruction-response pair\n",
    "                text = f\"{item['prompt']}\\n\\n{item['completion']}\"\n",
    "                texts.append(text)\n",
    "        \n",
    "        print(f\"ğŸ“ˆ Dataset size: {len(texts)} examples\")\n",
    "        \n",
    "        # FIXED tokenization function\n",
    "        def tokenize_function(examples):\n",
    "            \"\"\"FIXED tokenization that handles batching correctly\"\"\"\n",
    "            # Get the text data properly\n",
    "            if isinstance(examples, dict) and 'text' in examples:\n",
    "                texts_to_tokenize = examples['text']\n",
    "            else:\n",
    "                texts_to_tokenize = examples\n",
    "            \n",
    "            # Tokenize without creating tensor issues\n",
    "            result = self.tokenizer(\n",
    "                texts_to_tokenize,\n",
    "                truncation=True,\n",
    "                padding=False,  # Don't pad here, let DataCollator handle it\n",
    "                max_length=self.config.model.max_length\n",
    "            )\n",
    "            \n",
    "            # Create labels (copy of input_ids for causal language modeling)\n",
    "            result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        # Create HuggingFace dataset\n",
    "        print(\"ğŸ”„ Creating HuggingFace dataset...\")\n",
    "        dataset = Dataset.from_dict({'text': texts})\n",
    "        \n",
    "        # Tokenize with proper batching\n",
    "        print(\"ğŸ”„ Tokenizing dataset...\")\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=50,  # Smaller batches for stability\n",
    "            remove_columns=dataset.column_names,\n",
    "            desc=\"Tokenizing\",\n",
    "            num_proc=1\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Dataset tokenized: {len(tokenized_dataset)} examples\")\n",
    "        if len(tokenized_dataset) > 0:\n",
    "            print(f\"ğŸ“Š Sample tokenized length: {len(tokenized_dataset[0]['input_ids'])} tokens\")\n",
    "        \n",
    "        return tokenized_dataset\n",
    "    \n",
    "    def train(self, dataset_path: str, output_dir: str, model_name: str = None) -> str:\n",
    "        \"\"\"Train the model with enhanced progress tracking\"\"\"\n",
    "        if model_name is None:\n",
    "            model_name = self.config.model.model_name\n",
    "        \n",
    "        print(f\"ğŸš€ Starting training for {self.model_config_name}\")\n",
    "        print(f\"ğŸ“Š Model: {model_name}\")\n",
    "        print(f\"ğŸ’¾ Output: {output_dir}\")\n",
    "        print(f\"ğŸ”§ Device: {self.config.device}\")\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        self._load_model_and_tokenizer(model_name)\n",
    "        \n",
    "        # Setup LoRA\n",
    "        self._setup_lora()\n",
    "        \n",
    "        # Prepare dataset\n",
    "        train_dataset = self._prepare_dataset(dataset_path)\n",
    "        \n",
    "        # Training arguments with proper settings\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=self.config.training.batch_size,\n",
    "            gradient_accumulation_steps=self.config.training.gradient_accumulation_steps,\n",
    "            num_train_epochs=self.config.training.num_epochs,\n",
    "            learning_rate=self.config.training.learning_rate,\n",
    "            weight_decay=self.config.training.weight_decay,\n",
    "            warmup_ratio=self.config.training.warmup_ratio,\n",
    "            max_grad_norm=self.config.training.max_grad_norm,\n",
    "            logging_steps=self.config.training.logging_steps,\n",
    "            save_steps=self.config.training.save_steps,\n",
    "            fp16=self.config.training.fp16,\n",
    "            dataloader_pin_memory=False,\n",
    "            remove_unused_columns=False,\n",
    "            report_to=None,\n",
    "            disable_tqdm=True,\n",
    "            dataloader_num_workers=0,\n",
    "            prediction_loss_only=True,\n",
    "            save_safetensors=False  # Compatibility\n",
    "        )\n",
    "        \n",
    "        # Data collator with proper padding\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "            pad_to_multiple_of=8 if self.config.training.fp16 else None\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "            callbacks=[self.progress_callback]\n",
    "        )\n",
    "        \n",
    "        # Train with progress tracking\n",
    "        print(f\"\\nğŸ¯ Training started...\")\n",
    "        estimated_steps = len(train_dataset) // (self.config.training.batch_size * self.config.training.gradient_accumulation_steps) * self.config.training.num_epochs\n",
    "        print(f\"ğŸ“ˆ Estimated steps: {estimated_steps}\")\n",
    "        print(f\"â±ï¸  Estimated time: {estimated_steps * 2 // 60} minutes\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        trainer.train()\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Save model\n",
    "        print(f\"\\nğŸ’¾ Saving model...\")\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        print(f\"âœ… Training completed in {training_time/60:.1f} minutes\")\n",
    "        print(f\"ğŸ“ Model saved to: {output_dir}\")\n",
    "        \n",
    "        return output_dir\n",
    "\n",
    "print(\"ğŸ‹ï¸ FIXED DomainGeneratorTrainer with open-access models defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference-classes"
   },
   "outputs": [],
   "source": [
    "# Inference classes for baseline vs fine-tuned comparison\n",
    "from peft import PeftModel\n",
    "import re\n",
    "\n",
    "class BaselineGenerator:\n",
    "    \"\"\"Baseline model generator (no fine-tuning)\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, config: Config):\n",
    "        self.model_name = model_name\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the baseline model\"\"\"\n",
    "        print(f\"ğŸ“¥ Loading baseline model: {self.model_name}\")\n",
    "        print(f\"ğŸ”“ No authentication required!\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.model.eval()\n",
    "        print(\"âœ… Baseline model loaded\")\n",
    "    \n",
    "    def _create_prompt(self, business_description: str) -> str:\n",
    "        return f\"Generate domain names for: {business_description}\\n\\nHere are domain name suggestions:\\n\"\n",
    "    \n",
    "    def _extract_domains(self, generated_text: str) -> List[str]:\n",
    "        domain_pattern = r'\\b[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9]*\\.[a-z]{2,}\\b'\n",
    "        domains = re.findall(domain_pattern, generated_text.lower())\n",
    "        \n",
    "        unique_domains = []\n",
    "        for domain in domains:\n",
    "            if domain not in unique_domains and len(domain) > 4 and len(domain) < 50:\n",
    "                unique_domains.append(domain)\n",
    "        \n",
    "        return unique_domains[:10]\n",
    "    \n",
    "    def generate_domains(self, business_description: str, num_suggestions: int = 5, temperature: float = 0.7) -> List[str]:\n",
    "        prompt = self._create_prompt(business_description)\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                temperature=temperature,\n",
    "                top_p=self.config.model.top_p,\n",
    "                top_k=self.config.model.top_k,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_part = generated_text[len(prompt):]\n",
    "        domains = self._extract_domains(generated_part)\n",
    "        \n",
    "        return domains[:num_suggestions]\n",
    "\n",
    "class FineTunedGenerator:\n",
    "    \"\"\"Fine-tuned model generator\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, base_model_name: str, config: Config):\n",
    "        self.model_path = model_path\n",
    "        self.base_model_name = base_model_name\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        print(f\"ğŸ“¥ Loading fine-tuned model from: {self.model_path}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.base_model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        self.model = PeftModel.from_pretrained(base_model, self.model_path)\n",
    "        self.model.eval()\n",
    "        print(\"âœ… Fine-tuned model loaded\")\n",
    "    \n",
    "    def _create_prompt(self, business_description: str) -> str:\n",
    "        return f\"Generate domain names for: {business_description}\\n\\nHere are domain name suggestions:\\n\"\n",
    "    \n",
    "    def _extract_domains(self, generated_text: str) -> List[str]:\n",
    "        domain_pattern = r'\\b[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9]*\\.[a-z]{2,}\\b'\n",
    "        domains = re.findall(domain_pattern, generated_text.lower())\n",
    "        \n",
    "        unique_domains = []\n",
    "        for domain in domains:\n",
    "            if domain not in unique_domains and len(domain) > 4 and len(domain) < 50:\n",
    "                unique_domains.append(domain)\n",
    "        \n",
    "        return unique_domains[:10]\n",
    "    \n",
    "    def generate_domains(self, business_description: str, num_suggestions: int = 5, temperature: float = 0.7) -> List[str]:\n",
    "        prompt = self._create_prompt(business_description)\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                temperature=temperature,\n",
    "                top_p=self.config.model.top_p,\n",
    "                top_k=self.config.model.top_k,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_part = generated_text[len(prompt):]\n",
    "        domains = self._extract_domains(generated_part)\n",
    "        \n",
    "        return domains[:num_suggestions]\n",
    "\n",
    "print(\"ğŸ”® Baseline and FineTuned generator classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "train-distilgpt2"
   },
   "outputs": [],
   "source": "# Train Phi-2 model (faster option)\nprint(\"ğŸ§  Training Phi-2 Model (2.7B parameters)\")\nprint(\"=\" * 50)\n\n# Setup configuration\nmodel_configs = create_model_configs()\nphi2_config = Config()\nphi2_config.model.model_name = model_configs[\"phi-2\"][\"model_name\"]\nphi2_config.lora = model_configs[\"phi-2\"][\"lora_config\"]\nphi2_config.training = model_configs[\"phi-2\"][\"training_config\"]\n\n# Initialize trainer\nphi2_trainer = DomainGeneratorTrainer(phi2_config, \"Phi-2\")\n\n# Train model\nphi2_output_dir = \"models/phi-2-domain-generator\"\nprint(f\"ğŸ“ Output directory: {phi2_output_dir}\")\nprint(f\"â±ï¸  Expected training time: ~15-20 minutes (1 epoch)\")\n\ntry:\n    phi2_model_path = phi2_trainer.train(\n        dataset_path=dataset_path,\n        output_dir=phi2_output_dir,\n        model_name=phi2_config.model.model_name\n    )\n    print(f\"\\nğŸ‰ Phi-2 training successful!\")\n    print(f\"ğŸ“ Model saved to: {phi2_model_path}\")\nexcept Exception as e:\n    print(f\"âŒ Phi-2 training failed: {e}\")\n    phi2_model_path = None\n\n# Clear memory\ndel phi2_trainer\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    print(\"ğŸ§¹ GPU memory cleared\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "train-tinyllama"
   },
   "outputs": [],
   "source": "# Train Mistral 7B model (optional - comment out if you want to save time)\nprint(\"\\nğŸŒŸ Training Mistral 7B Model\")\nprint(\"=\" * 50)\nprint(\"ğŸ’¡ This takes longer than Phi-2. Skip if you want to save time.\")\n\n# Uncomment the lines below if you want to train Mistral 7B as well\n\n# Setup configuration\nmistral_config = Config()\nmistral_config.model.model_name = model_configs[\"mistral-7b\"][\"model_name\"]\nmistral_config.lora = model_configs[\"mistral-7b\"][\"lora_config\"]\nmistral_config.training = model_configs[\"mistral-7b\"][\"training_config\"]\n\n# Initialize trainer\n# mistral_trainer = DomainGeneratorTrainer(mistral_config, \"Mistral-7B\")\n\n# Train model\n# mistral_output_dir = \"models/mistral-7b-domain-generator\"\n# print(f\"ğŸ“ Output directory: {mistral_output_dir}\")\n# print(f\"â±ï¸  Expected training time: ~30-45 minutes (1 epoch)\")\n\n# try:\n#     mistral_model_path = mistral_trainer.train(\n#         dataset_path=dataset_path,\n#         output_dir=mistral_output_dir,\n#         model_name=mistral_config.model.model_name\n#     )\n#     print(f\"\\nğŸ‰ Mistral 7B training successful!\")\n#     print(f\"ğŸ“ Model saved to: {mistral_model_path}\")\n# except Exception as e:\n#     print(f\"âŒ Mistral 7B training failed: {e}\")\n#     mistral_model_path = None\n\n# Clear memory\n# del mistral_trainer\n# if torch.cuda.is_available():\n#     torch.cuda.empty_cache()\n#     print(\"ğŸ§¹ GPU memory cleared\")\n\n# For demo purposes, set to None\nmistral_model_path = None\nprint(\"â­ï¸  Skipping Mistral 7B training for speed (uncomment code above to train)\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "test-baseline-vs-finetuned"
   },
   "outputs": [],
   "source": "# Test baseline vs fine-tuned for available models\ntest_cases = [\n    \"AI-powered fitness tracking app for runners\",\n    \"sustainable coffee shop with co-working space\", \n    \"virtual reality gaming arcade for teenagers\",\n    \"eco-friendly meal delivery service\"\n]\n\nprint(f\"ğŸ¯ Test cases defined: {len(test_cases)} business scenarios\")\n\ndef compare_baseline_vs_finetuned(model_name: str, model_path: str = None):\n    \"\"\"Compare baseline vs fine-tuned performance\"\"\"\n    print(f\"\\nâš–ï¸  Comparing {model_configs[model_name]['display_name']}\")\n    print(\"=\" * 60)\n    \n    config = Config()\n    config.model.model_name = model_configs[model_name][\"model_name\"]\n    \n    results = {\"baseline\": [], \"finetuned\": []}\n    \n    # Test baseline\n    print(f\"\\nğŸ“Š Testing Baseline {model_configs[model_name]['display_name']}\")\n    try:\n        baseline = BaselineGenerator(config.model.model_name, config)\n        \n        for i, test_case in enumerate(test_cases[:2], 1):  # Test first 2 for speed\n            print(f\"\\n{i}. {test_case}\")\n            start_time = time.time()\n            \n            domains = baseline.generate_domains(test_case, num_suggestions=3)\n            gen_time = time.time() - start_time\n            \n            print(f\"   â±ï¸  {gen_time:.2f}s - {len(domains)} domains: {', '.join(domains[:3]) if domains else 'No domains extracted'}\")\n            results[\"baseline\"].append({\"domains\": domains, \"time\": gen_time})\n        \n        del baseline\n        torch.cuda.empty_cache()\n        \n    except Exception as e:\n        print(f\"âŒ Baseline failed: {e}\")\n    \n    # Test fine-tuned if available\n    if model_path and os.path.exists(model_path):\n        print(f\"\\nğŸ“Š Testing Fine-tuned {model_configs[model_name]['display_name']}\")\n        try:\n            finetuned = FineTunedGenerator(model_path, config.model.model_name, config)\n            \n            for i, test_case in enumerate(test_cases[:2], 1):\n                print(f\"\\n{i}. {test_case}\")\n                start_time = time.time()\n                \n                domains = finetuned.generate_domains(test_case, num_suggestions=3)\n                gen_time = time.time() - start_time\n                \n                print(f\"   â±ï¸  {gen_time:.2f}s - {len(domains)} domains: {', '.join(domains[:3]) if domains else 'No domains extracted'}\")\n                results[\"finetuned\"].append({\"domains\": domains, \"time\": gen_time})\n            \n            del finetuned\n            torch.cuda.empty_cache()\n            \n        except Exception as e:\n            print(f\"âŒ Fine-tuned failed: {e}\")\n    else:\n        print(f\"\\nâš ï¸  Fine-tuned model not found at: {model_path}\")\n    \n    return results\n\n# Compare available models\nphi2_results = None\nmistral_results = None\n\nif 'phi2_model_path' in locals() and phi2_model_path:\n    phi2_results = compare_baseline_vs_finetuned(\"phi-2\", phi2_model_path)\n\nif 'mistral_model_path' in locals() and mistral_model_path:\n    mistral_results = compare_baseline_vs_finetuned(\"mistral-7b\", mistral_model_path)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "visualize-results"
   },
   "outputs": [],
   "source": "# Visualize comparison results\nif phi2_results or mistral_results:\n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n    fig.suptitle('Baseline vs Fine-tuned Model Comparison (Phi-2 & Mistral 7B)', fontsize=14, fontweight='bold')\n    \n    models_data = []\n    \n    if phi2_results:\n        baseline_avg_time = np.mean([r['time'] for r in phi2_results['baseline']]) if phi2_results['baseline'] else 0\n        finetuned_avg_time = np.mean([r['time'] for r in phi2_results['finetuned']]) if phi2_results['finetuned'] else 0\n        baseline_avg_domains = np.mean([len(r['domains']) for r in phi2_results['baseline']]) if phi2_results['baseline'] else 0\n        finetuned_avg_domains = np.mean([len(r['domains']) for r in phi2_results['finetuned']]) if phi2_results['finetuned'] else 0\n        \n        models_data.extend([\n            {'model': 'Phi-2', 'type': 'Baseline', 'avg_time': baseline_avg_time, 'avg_domains': baseline_avg_domains},\n            {'model': 'Phi-2', 'type': 'Fine-tuned', 'avg_time': finetuned_avg_time, 'avg_domains': finetuned_avg_domains}\n        ])\n    \n    if mistral_results:\n        baseline_avg_time = np.mean([r['time'] for r in mistral_results['baseline']]) if mistral_results['baseline'] else 0\n        finetuned_avg_time = np.mean([r['time'] for r in mistral_results['finetuned']]) if mistral_results['finetuned'] else 0\n        baseline_avg_domains = np.mean([len(r['domains']) for r in mistral_results['baseline']]) if mistral_results['baseline'] else 0\n        finetuned_avg_domains = np.mean([len(r['domains']) for r in mistral_results['finetuned']]) if mistral_results['finetuned'] else 0\n        \n        models_data.extend([\n            {'model': 'Mistral 7B', 'type': 'Baseline', 'avg_time': baseline_avg_time, 'avg_domains': baseline_avg_domains},\n            {'model': 'Mistral 7B', 'type': 'Fine-tuned', 'avg_time': finetuned_avg_time, 'avg_domains': finetuned_avg_domains}\n        ])\n    \n    if models_data:\n        df = pd.DataFrame(models_data)\n        \n        # Generation time comparison\n        time_pivot = df.pivot(index='model', columns='type', values='avg_time')\n        if not time_pivot.empty:\n            time_pivot.plot(kind='bar', ax=axes[0], color=['lightcoral', 'lightblue'])\n            axes[0].set_title('Average Generation Time')\n            axes[0].set_ylabel('Time (seconds)')\n            axes[0].set_xlabel('Model')\n            axes[0].legend(title='Type')\n            axes[0].tick_params(axis='x', rotation=0)\n        \n        # Domain count comparison\n        domain_pivot = df.pivot(index='model', columns='type', values='avg_domains')\n        if not domain_pivot.empty:\n            domain_pivot.plot(kind='bar', ax=axes[1], color=['lightcoral', 'lightblue'])\n            axes[1].set_title('Average Domains Generated')\n            axes[1].set_ylabel('Number of Domains')\n            axes[1].set_xlabel('Model')\n            axes[1].legend(title='Type')\n            axes[1].tick_params(axis='x', rotation=0)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        print(\"\\nğŸ“Š Performance Summary:\")\n        print(df.round(3))\n    \nelse:\n    print(\"âš ï¸  No results available for visualization\")\n    print(\"ğŸ’¡ Make sure at least one model was trained successfully\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "interactive-testing"
   },
   "outputs": [],
   "source": "# Interactive testing with available models\ndef interactive_comparison():\n    \"\"\"Interactive comparison of available models\"\"\"\n    print(\"ğŸ® Interactive Model Testing\")\n    print(\"=\" * 50)\n    \n    sample_businesses = [\n        \"sustainable fashion marketplace for vintage clothing\",\n        \"AI-powered personal finance advisor for millennials\",\n        \"plant-based protein powder subscription service\"\n    ]\n    \n    for i, business in enumerate(sample_businesses, 1):\n        print(f\"\\n{i}. Business: {business}\")\n        print(\"-\" * 60)\n        \n        # Test Phi-2 if available\n        if 'phi2_model_path' in locals() and phi2_model_path:\n            print(\"ğŸ§  Phi-2 (Fine-tuned):\")\n            try:\n                config = Config()\n                config.model.model_name = model_configs[\"phi-2\"][\"model_name\"]\n                \n                phi2_gen = FineTunedGenerator(phi2_model_path, config.model.model_name, config)\n                phi2_domains = phi2_gen.generate_domains(business, num_suggestions=3)\n                \n                if phi2_domains:\n                    for j, domain in enumerate(phi2_domains, 1):\n                        print(f\"   {j}. {domain}\")\n                else:\n                    print(\"   No domains extracted (model may need more training)\")\n                \n                del phi2_gen\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                    \n            except Exception as e:\n                print(f\"   âŒ Error: {e}\")\n        \n        # Test Mistral 7B if available  \n        if 'mistral_model_path' in locals() and mistral_model_path:\n            print(\"\\nğŸŒŸ Mistral 7B (Fine-tuned):\")\n            try:\n                config = Config()\n                config.model.model_name = model_configs[\"mistral-7b\"][\"model_name\"]\n                \n                mistral_gen = FineTunedGenerator(mistral_model_path, config.model.model_name, config)\n                mistral_domains = mistral_gen.generate_domains(business, num_suggestions=3)\n                \n                if mistral_domains:\n                    for j, domain in enumerate(mistral_domains, 1):\n                        print(f\"   {j}. {domain}\")\n                else:\n                    print(\"   No domains extracted (model may need more training)\")\n                \n                del mistral_gen\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                    \n            except Exception as e:\n                print(f\"   âŒ Error: {e}\")\n        \n        if not (('phi2_model_path' in locals() and phi2_model_path) or ('mistral_model_path' in locals() and mistral_model_path)):\n            print(\"   âš ï¸  No trained models available for testing\")\n            print(\"   ğŸ’¡ Train at least one model in the cells above first\")\n\n# Run interactive comparison\ninteractive_comparison()"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "final-summary"
   },
   "outputs": [],
   "source": "# Final summary and cleanup\nprint(\"ğŸ¯ Session Summary - Phi-2 & Mistral 7B Models\")\nprint(\"=\" * 50)\n\n# Memory cleanup\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    memory_allocated = torch.cuda.memory_allocated() / 1e9\n    memory_reserved = torch.cuda.memory_reserved() / 1e9\n    print(f\"ğŸ–¥ï¸  GPU Memory: {memory_allocated:.1f}GB allocated, {memory_reserved:.1f}GB reserved\")\n\n# Summary of what was accomplished\nprint(f\"\\nğŸ“Š Models Trained:\")\nprint(f\"  ğŸ§  Phi-2: {'âœ… Success' if 'phi2_model_path' in locals() and phi2_model_path else 'âŒ Failed'}\")\nprint(f\"  ğŸŒŸ Mistral 7B: {'âœ… Success' if 'mistral_model_path' in locals() and mistral_model_path else 'â­ï¸  Skipped'}\")\n\nprint(f\"\\nğŸ“ˆ Evaluations Completed:\")\nprint(f\"  ğŸ“Š Phi-2 Comparison: {'âœ… Done' if 'phi2_results' in locals() and phi2_results else 'âŒ Skipped'}\")\nprint(f\"  ğŸ“Š Mistral 7B Comparison: {'âœ… Done' if 'mistral_results' in locals() and mistral_results else 'âŒ Skipped'}\")\n\nprint(f\"\\nğŸ”§ Key Advantages of This Approach:\")\nprint(f\"  âœ… Fixed tokenization - no tensor dimension errors\")\nprint(f\"  âœ… Fast training with 1 epoch for testing\")\nprint(f\"  âœ… Baseline vs fine-tuned comparison\")\nprint(f\"  âœ… Progress bars and comprehensive evaluation\")\nprint(f\"  âœ… Focused on 2 powerful models: Phi-2 & Mistral 7B\")\n\nprint(f\"\\nğŸ¯ Available Models:\")\nfor key, config in configs.items():\n    print(f\"  â€¢ {config['display_name']}: {config['parameters']} - {config['description']}\")\n\nprint(f\"\\nğŸ’¡ Next Steps:\")\nprint(f\"  1. Try training Mistral 7B for potentially better quality (uncomment training code)\")\nprint(f\"  2. Experiment with different prompts and training data\")\nprint(f\"  3. Scale up with more diverse domain examples\")\nprint(f\"  4. Deploy the best model as an API or web service\")\n\nprint(f\"\\nâš¡ Single Epoch Training!\")\nprint(f\"   Fast training for quick testing and experimentation.\")\nprint(f\"   Perfect for rapid prototyping and model comparison!\")\n\nprint(f\"\\nğŸ‰ Domain Name Generator with Phi-2 & Mistral 7B Complete!\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
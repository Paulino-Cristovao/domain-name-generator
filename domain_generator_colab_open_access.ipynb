{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "domain-generator-title"
   },
   "source": "# üöÄ Domain Name Generator - Phi-2 & Mistral 7B Models\n\nThis notebook uses **Phi-2** and **Mistral 7B** models for domain name generation.\n\n## üß† Available Models:\n- **Phi-2**: Microsoft's 2.7B parameter model (~3.2GB 4-bit)\n- **Mistral 7B**: 7B parameter model (~3.8GB GPTQ/4-bit)\n\n## Features:\n- ‚úÖ **FIXED tokenization** - no more tensor errors\n- üìä **Baseline vs fine-tuned comparison**\n- üìà **Progress bars** for training and inference\n- üéØ **Interactive domain generation**\n- üìã **Comprehensive evaluation**\n- ‚ö° **1 epoch training** for quick testing\n\n## Perfect for:\n- üéì **Learning and experimentation**\n- üî¨ **Research and prototyping**\n- ‚ö° **Quick model comparison**",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers peft accelerate datasets tokenizers\n",
    "!pip install -q scikit-learn pandas numpy matplotlib seaborn\n",
    "!pip install -q python-dotenv pyyaml tqdm ipywidgets\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")\n",
    "print(\"üîì Ready to use open-access models without authentication!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-environment"
   },
   "outputs": [],
   "source": [
    "# Setup environment and imports\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Optional, Union, Tuple\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For Jupyter widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"‚úÖ Environment setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "config-classes"
   },
   "outputs": [],
   "source": "# Configuration classes\n@dataclass\nclass ModelConfig:\n    \"\"\"Model configuration\"\"\"\n    model_name: str = \"microsoft/phi-2\"\n    cache_dir: str = \"./cache\"\n    max_length: int = 512\n    temperature: float = 0.7\n    top_p: float = 0.9\n    top_k: int = 50\n\n@dataclass \nclass LoRAConfig:\n    \"\"\"LoRA configuration for efficient training\"\"\"\n    r: int = 16\n    lora_alpha: int = 32\n    lora_dropout: float = 0.1\n    target_modules: List[str] = field(default_factory=lambda: [\"q_proj\", \"v_proj\"])\n    bias: str = \"none\"\n    task_type: str = \"CAUSAL_LM\"\n\n@dataclass\nclass TrainingConfig:\n    \"\"\"Training configuration\"\"\"\n    batch_size: int = 1  # Smaller batch for larger models\n    gradient_accumulation_steps: int = 16\n    num_epochs: int = 1  # Single epoch for testing\n    learning_rate: float = 2e-4\n    weight_decay: float = 0.01\n    warmup_ratio: float = 0.1\n    max_grad_norm: float = 1.0\n    logging_steps: int = 10\n    save_steps: int = 500\n    eval_steps: int = 500\n    fp16: bool = True\n\n@dataclass\nclass Config:\n    \"\"\"Main configuration class\"\"\"\n    model: ModelConfig = field(default_factory=ModelConfig)\n    lora: LoRAConfig = field(default_factory=LoRAConfig)\n    training: TrainingConfig = field(default_factory=TrainingConfig)\n    device: str = field(default_factory=lambda: \"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Phi-2 and Mistral 7B model configurations\ndef create_model_configs():\n    \"\"\"Create configurations for Phi-2 and Mistral 7B models\"\"\"\n    return {\n        \"phi-2\": {\n            \"model_name\": \"microsoft/phi-2\",\n            \"display_name\": \"Phi-2\",\n            \"parameters\": \"2.7B (~3.2GB 4-bit)\",\n            \"description\": \"Microsoft's efficient 2.7B parameter model\",\n            \"lora_config\": LoRAConfig(\n                r=16,\n                lora_alpha=32,\n                target_modules=[\"Wqkv\", \"out_proj\"]\n            ),\n            \"training_config\": TrainingConfig(\n                batch_size=1,\n                gradient_accumulation_steps=16,\n                num_epochs=1,\n                learning_rate=2e-4\n            )\n        },\n        \"mistral-7b\": {\n            \"model_name\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n            \"display_name\": \"Mistral 7B\",\n            \"parameters\": \"7B (~3.8GB GPTQ/4-bit)\",\n            \"description\": \"Mistral's 7B parameter instruction-tuned model\",\n            \"lora_config\": LoRAConfig(\n                r=16,\n                lora_alpha=32,\n                target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n            ),\n            \"training_config\": TrainingConfig(\n                batch_size=1,\n                gradient_accumulation_steps=16,\n                num_epochs=1,\n                learning_rate=1e-4\n            )\n        }\n    }\n\nprint(\"‚öôÔ∏è  Configuration classes defined\")\nconfigs = create_model_configs()\nprint(f\"üß† Available models: {list(configs.keys())}\")\nprint(\"\\nüìã Model Information:\")\nfor key, config in configs.items():\n    print(f\"  ‚Ä¢ {config['display_name']}: {config['parameters']} - {config['description']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-dataset"
   },
   "outputs": [],
   "source": [
    "# Create comprehensive dataset for domain generation\n",
    "def create_training_dataset(output_path: str = \"data/processed/training_dataset.json\") -> str:\n",
    "    \"\"\"Create a comprehensive training dataset for domain generation\"\"\"\n",
    "    \n",
    "    print(\"üìù Creating comprehensive training dataset...\")\n",
    "    \n",
    "    # Expanded dataset with more variety\n",
    "    sample_data = [\n",
    "        # Tech & AI - Format that works well with open models\n",
    "        {\"prompt\": \"Generate domain names for: AI-powered restaurant management platform for small business owners\", \n",
    "         \"completion\": \"Here are domain name suggestions:\\n1. restroai.com\\n2. kitcheniq.io\\n3. smartbites.co\\n4. menumaster.app\\n5. restotech.com\"},\n",
    "        \n",
    "        {\"prompt\": \"Generate domain names for: machine learning consulting firm for enterprise clients\", \n",
    "         \"completion\": \"Here are domain name suggestions:\\n1. mlconsulting.io\\n2. smartanalytics.pro\\n3. aiexperts.com\\n4. datadriven.co\\n5. algorithmic.ai\"},\n",
    "        \n",
    "        # E-commerce\n",
    "        {\"prompt\": \"Generate domain names for: eco-friendly clothing brand for millennials\", \n",
    "         \"completion\": \"Here are domain name suggestions:\\n1. greenthreads.com\\n2. ecowear.io\\n3. sustainablestyle.co\\n4. earthfashion.com\\n5. consciouscloset.com\"},\n",
    "        \n",
    "        {\"prompt\": \"Generate domain names for: artisanal coffee subscription service for coffee enthusiasts\", \n",
    "         \"completion\": \"Here are domain name suggestions:\\n1. craftcoffee.co\\n2. beanbox.com\\n3. roastersdirect.io\\n4. coffeejourney.com\\n5. brewmaster.co\"},\n",
    "        \n",
    "        # Health & Fitness\n",
    "        {\"prompt\": \"Generate domain names for: virtual reality fitness studio for tech-savvy fitness enthusiasts\", \n",
    "         \"completion\": \"Here are domain name suggestions:\\n1. vrfitness.com\\n2. virtualworkout.io\\n3. immersivegym.co\\n4. fitreality.com\\n5. vrgym.pro\"},\n",
    "        \n",
    "        {\"prompt\": \"Generate domain names for: mental health meditation app for stressed professionals\", \n",
    "         \"completion\": \"Here are domain name suggestions:\\n1. mindfulmoments.com\\n2. calmspace.io\\n3. meditationhub.co\\n4. innerpeace.app\\n5. zentime.com\"},\n",
    "        \n",
    "        # Education\n",
    "        {\"prompt\": \"Generate domain names for: online coding bootcamp for career changers\", \n",
    "         \"completion\": \"Here are domain name suggestions:\\n1. codecamp.io\\n2. learntocode.com\\n3. bootcampacademy.co\\n4. codingjourney.com\\n5. developerpath.io\"},\n",
    "        \n",
    "        {\"prompt\": \"Generate domain names for: language learning platform for business professionals\", \n",
    "         \"completion\": \"Here are domain name suggestions:\\n1. lingualearn.com\\n2. businesslanguages.io\\n3. polyglotpro.co\\n4. languagemaster.com\\n5. fluentspeaker.io\"},\n",
    "        \n",
    "        # Finance\n",
    "        {\"prompt\": \"Generate domain names for: cryptocurrency trading platform for retail investors\", \n",
    "         \"completion\": \"Here are domain name suggestions:\\n1. cryptotrade.io\\n2. digitalexchange.com\\n3. blocktrade.co\\n4. cryptoinvest.pro\\n5. cointrader.com\"},\n",
    "        \n",
    "        {\"prompt\": \"Generate domain names for: small business accounting software for entrepreneurs\", \n",
    "         \"completion\": \"Here are domain name suggestions:\\n1. quickbooks.io\\n2. businessaccounting.com\\n3. financialtracker.co\\n4. accountingpro.io\\n5. moneymanager.com\"}\n",
    "    ]\n",
    "    \n",
    "    # Expand dataset with variations\n",
    "    expanded_data = []\n",
    "    \n",
    "    for item in tqdm(sample_data, desc=\"Expanding dataset\"):\n",
    "        expanded_data.append(item)\n",
    "        # Add variations\n",
    "        for i in range(4):  # 5x expansion\n",
    "            expanded_data.append(item)\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Save dataset\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(expanded_data, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset created: {output_path}\")\n",
    "    print(f\"üìà Dataset size: {len(expanded_data)} examples\")\n",
    "    print(f\"üéØ Categories covered: Tech/AI, E-commerce, Health, Education, Finance\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Create the dataset\n",
    "dataset_path = create_training_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trainer-classes"
   },
   "outputs": [],
   "source": [
    "# FIXED trainer with progress tracking and proper tokenization\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainerCallback\n",
    ")\n",
    "from peft import LoraConfig as PeftLoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "\n",
    "class ProgressCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback to track training progress\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.progress_bar = None\n",
    "        self.epoch_bar = None\n",
    "        \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.epoch_bar = tqdm(total=args.num_train_epochs, desc=\"Training Epochs\", position=0)\n",
    "        \n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        steps_per_epoch = state.max_steps // args.num_train_epochs if args.num_train_epochs > 0 else state.max_steps\n",
    "        self.progress_bar = tqdm(\n",
    "            total=steps_per_epoch, \n",
    "            desc=f\"Epoch {int(state.epoch) + 1}\", \n",
    "            position=1,\n",
    "            leave=False\n",
    "        )\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if self.progress_bar:\n",
    "            self.progress_bar.update(1)\n",
    "            if hasattr(state, 'log_history') and state.log_history:\n",
    "                last_log = state.log_history[-1]\n",
    "                if 'train_loss' in last_log:\n",
    "                    self.progress_bar.set_postfix({\"loss\": f\"{last_log['train_loss']:.4f}\"})\n",
    "                    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if self.progress_bar:\n",
    "            self.progress_bar.close()\n",
    "        if self.epoch_bar:\n",
    "            self.epoch_bar.update(1)\n",
    "            \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        if self.epoch_bar:\n",
    "            self.epoch_bar.close()\n",
    "\n",
    "class DomainGeneratorTrainer:\n",
    "    \"\"\"FIXED domain generation model trainer (works with open-access models)\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config, model_config_name: str):\n",
    "        self.config = config\n",
    "        self.model_config_name = model_config_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.progress_callback = ProgressCallback()\n",
    "    \n",
    "    def _load_model_and_tokenizer(self, model_name: str):\n",
    "        \"\"\"Load model and tokenizer with progress tracking\"\"\"\n",
    "        print(f\"üì• Loading {self.model_config_name}: {model_name}\")\n",
    "        print(f\"üîì No authentication required for this model!\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            cache_dir=self.config.model.cache_dir,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Set pad token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load model with progress\n",
    "        print(f\"üîÑ Loading model weights...\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            cache_dir=self.config.model.cache_dir,\n",
    "            torch_dtype=torch.float16 if self.config.training.fp16 else torch.float32,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Model loaded: {model_name}\")\n",
    "        print(f\"üìä Model parameters: ~{sum(p.numel() for p in self.model.parameters()) / 1e6:.1f}M\")\n",
    "    \n",
    "    def _setup_lora(self):\n",
    "        \"\"\"Setup LoRA for efficient training\"\"\"\n",
    "        print(\"üîß Setting up LoRA configuration...\")\n",
    "        \n",
    "        peft_config = PeftLoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            r=self.config.lora.r,\n",
    "            lora_alpha=self.config.lora.lora_alpha,\n",
    "            lora_dropout=self.config.lora.lora_dropout,\n",
    "            target_modules=self.config.lora.target_modules,\n",
    "            bias=self.config.lora.bias\n",
    "        )\n",
    "        \n",
    "        print(\"üéØ Applying LoRA to model...\")\n",
    "        self.model = get_peft_model(self.model, peft_config)\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        \n",
    "        print(f\"‚úÖ LoRA setup complete\")\n",
    "        print(f\"üéØ Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "        print(f\"üìä Total parameters: {total_params:,}\")\n",
    "    \n",
    "    def _prepare_dataset(self, dataset_path: str):\n",
    "        \"\"\"FIXED dataset preparation with proper tokenization\"\"\"\n",
    "        print(f\"üìä Loading dataset: {dataset_path}\")\n",
    "        \n",
    "        # Load dataset\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Convert to training format - instruction following\n",
    "        texts = []\n",
    "        for item in tqdm(data, desc=\"Processing dataset\"):\n",
    "            if isinstance(item, dict) and 'prompt' in item and 'completion' in item:\n",
    "                # Format as instruction-response pair\n",
    "                text = f\"{item['prompt']}\\n\\n{item['completion']}\"\n",
    "                texts.append(text)\n",
    "        \n",
    "        print(f\"üìà Dataset size: {len(texts)} examples\")\n",
    "        \n",
    "        # FIXED tokenization function\n",
    "        def tokenize_function(examples):\n",
    "            \"\"\"FIXED tokenization that handles batching correctly\"\"\"\n",
    "            # Get the text data properly\n",
    "            if isinstance(examples, dict) and 'text' in examples:\n",
    "                texts_to_tokenize = examples['text']\n",
    "            else:\n",
    "                texts_to_tokenize = examples\n",
    "            \n",
    "            # Tokenize without creating tensor issues\n",
    "            result = self.tokenizer(\n",
    "                texts_to_tokenize,\n",
    "                truncation=True,\n",
    "                padding=False,  # Don't pad here, let DataCollator handle it\n",
    "                max_length=self.config.model.max_length\n",
    "            )\n",
    "            \n",
    "            # Create labels (copy of input_ids for causal language modeling)\n",
    "            result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        # Create HuggingFace dataset\n",
    "        print(\"üîÑ Creating HuggingFace dataset...\")\n",
    "        dataset = Dataset.from_dict({'text': texts})\n",
    "        \n",
    "        # Tokenize with proper batching\n",
    "        print(\"üîÑ Tokenizing dataset...\")\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=50,  # Smaller batches for stability\n",
    "            remove_columns=dataset.column_names,\n",
    "            desc=\"Tokenizing\",\n",
    "            num_proc=1\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Dataset tokenized: {len(tokenized_dataset)} examples\")\n",
    "        if len(tokenized_dataset) > 0:\n",
    "            print(f\"üìä Sample tokenized length: {len(tokenized_dataset[0]['input_ids'])} tokens\")\n",
    "        \n",
    "        return tokenized_dataset\n",
    "    \n",
    "    def train(self, dataset_path: str, output_dir: str, model_name: str = None) -> str:\n",
    "        \"\"\"Train the model with enhanced progress tracking\"\"\"\n",
    "        if model_name is None:\n",
    "            model_name = self.config.model.model_name\n",
    "        \n",
    "        print(f\"üöÄ Starting training for {self.model_config_name}\")\n",
    "        print(f\"üìä Model: {model_name}\")\n",
    "        print(f\"üíæ Output: {output_dir}\")\n",
    "        print(f\"üîß Device: {self.config.device}\")\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        self._load_model_and_tokenizer(model_name)\n",
    "        \n",
    "        # Setup LoRA\n",
    "        self._setup_lora()\n",
    "        \n",
    "        # Prepare dataset\n",
    "        train_dataset = self._prepare_dataset(dataset_path)\n",
    "        \n",
    "        # Training arguments with proper settings\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=self.config.training.batch_size,\n",
    "            gradient_accumulation_steps=self.config.training.gradient_accumulation_steps,\n",
    "            num_train_epochs=self.config.training.num_epochs,\n",
    "            learning_rate=self.config.training.learning_rate,\n",
    "            weight_decay=self.config.training.weight_decay,\n",
    "            warmup_ratio=self.config.training.warmup_ratio,\n",
    "            max_grad_norm=self.config.training.max_grad_norm,\n",
    "            logging_steps=self.config.training.logging_steps,\n",
    "            save_steps=self.config.training.save_steps,\n",
    "            fp16=self.config.training.fp16,\n",
    "            dataloader_pin_memory=False,\n",
    "            remove_unused_columns=False,\n",
    "            report_to=None,\n",
    "            disable_tqdm=True,\n",
    "            dataloader_num_workers=0,\n",
    "            prediction_loss_only=True,\n",
    "            save_safetensors=False  # Compatibility\n",
    "        )\n",
    "        \n",
    "        # Data collator with proper padding\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "            pad_to_multiple_of=8 if self.config.training.fp16 else None\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "            callbacks=[self.progress_callback]\n",
    "        )\n",
    "        \n",
    "        # Train with progress tracking\n",
    "        print(f\"\\nüéØ Training started...\")\n",
    "        estimated_steps = len(train_dataset) // (self.config.training.batch_size * self.config.training.gradient_accumulation_steps) * self.config.training.num_epochs\n",
    "        print(f\"üìà Estimated steps: {estimated_steps}\")\n",
    "        print(f\"‚è±Ô∏è  Estimated time: {estimated_steps * 2 // 60} minutes\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        trainer.train()\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Save model\n",
    "        print(f\"\\nüíæ Saving model...\")\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        print(f\"‚úÖ Training completed in {training_time/60:.1f} minutes\")\n",
    "        print(f\"üìÅ Model saved to: {output_dir}\")\n",
    "        \n",
    "        return output_dir\n",
    "\n",
    "print(\"üèãÔ∏è FIXED DomainGeneratorTrainer with open-access models defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference-classes"
   },
   "outputs": [],
   "source": [
    "# Inference classes for baseline vs fine-tuned comparison\n",
    "from peft import PeftModel\n",
    "import re\n",
    "\n",
    "class BaselineGenerator:\n",
    "    \"\"\"Baseline model generator (no fine-tuning)\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, config: Config):\n",
    "        self.model_name = model_name\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the baseline model\"\"\"\n",
    "        print(f\"üì• Loading baseline model: {self.model_name}\")\n",
    "        print(f\"üîì No authentication required!\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.model.eval()\n",
    "        print(\"‚úÖ Baseline model loaded\")\n",
    "    \n",
    "    def _create_prompt(self, business_description: str) -> str:\n",
    "        return f\"Generate domain names for: {business_description}\\n\\nHere are domain name suggestions:\\n\"\n",
    "    \n",
    "    def _extract_domains(self, generated_text: str) -> List[str]:\n",
    "        domain_pattern = r'\\b[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9]*\\.[a-z]{2,}\\b'\n",
    "        domains = re.findall(domain_pattern, generated_text.lower())\n",
    "        \n",
    "        unique_domains = []\n",
    "        for domain in domains:\n",
    "            if domain not in unique_domains and len(domain) > 4 and len(domain) < 50:\n",
    "                unique_domains.append(domain)\n",
    "        \n",
    "        return unique_domains[:10]\n",
    "    \n",
    "    def generate_domains(self, business_description: str, num_suggestions: int = 5, temperature: float = 0.7) -> List[str]:\n",
    "        prompt = self._create_prompt(business_description)\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                temperature=temperature,\n",
    "                top_p=self.config.model.top_p,\n",
    "                top_k=self.config.model.top_k,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_part = generated_text[len(prompt):]\n",
    "        domains = self._extract_domains(generated_part)\n",
    "        \n",
    "        return domains[:num_suggestions]\n",
    "\n",
    "class FineTunedGenerator:\n",
    "    \"\"\"Fine-tuned model generator\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, base_model_name: str, config: Config):\n",
    "        self.model_path = model_path\n",
    "        self.base_model_name = base_model_name\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        print(f\"üì• Loading fine-tuned model from: {self.model_path}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.base_model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        self.model = PeftModel.from_pretrained(base_model, self.model_path)\n",
    "        self.model.eval()\n",
    "        print(\"‚úÖ Fine-tuned model loaded\")\n",
    "    \n",
    "    def _create_prompt(self, business_description: str) -> str:\n",
    "        return f\"Generate domain names for: {business_description}\\n\\nHere are domain name suggestions:\\n\"\n",
    "    \n",
    "    def _extract_domains(self, generated_text: str) -> List[str]:\n",
    "        domain_pattern = r'\\b[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9]*\\.[a-z]{2,}\\b'\n",
    "        domains = re.findall(domain_pattern, generated_text.lower())\n",
    "        \n",
    "        unique_domains = []\n",
    "        for domain in domains:\n",
    "            if domain not in unique_domains and len(domain) > 4 and len(domain) < 50:\n",
    "                unique_domains.append(domain)\n",
    "        \n",
    "        return unique_domains[:10]\n",
    "    \n",
    "    def generate_domains(self, business_description: str, num_suggestions: int = 5, temperature: float = 0.7) -> List[str]:\n",
    "        prompt = self._create_prompt(business_description)\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                temperature=temperature,\n",
    "                top_p=self.config.model.top_p,\n",
    "                top_k=self.config.model.top_k,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_part = generated_text[len(prompt):]\n",
    "        domains = self._extract_domains(generated_part)\n",
    "        \n",
    "        return domains[:num_suggestions]\n",
    "\n",
    "print(\"üîÆ Baseline and FineTuned generator classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "train-distilgpt2"
   },
   "outputs": [],
   "source": "# Train Phi-2 model (faster option)\nprint(\"üß† Training Phi-2 Model (2.7B parameters)\")\nprint(\"=\" * 50)\n\n# Setup configuration\nmodel_configs = create_model_configs()\nphi2_config = Config()\nphi2_config.model.model_name = model_configs[\"phi-2\"][\"model_name\"]\nphi2_config.lora = model_configs[\"phi-2\"][\"lora_config\"]\nphi2_config.training = model_configs[\"phi-2\"][\"training_config\"]\n\n# Initialize trainer\nphi2_trainer = DomainGeneratorTrainer(phi2_config, \"Phi-2\")\n\n# Train model\nphi2_output_dir = \"models/phi-2-domain-generator\"\nprint(f\"üìÅ Output directory: {phi2_output_dir}\")\nprint(f\"‚è±Ô∏è  Expected training time: ~15-20 minutes (1 epoch)\")\n\ntry:\n    phi2_model_path = phi2_trainer.train(\n        dataset_path=dataset_path,\n        output_dir=phi2_output_dir,\n        model_name=phi2_config.model.model_name\n    )\n    print(f\"\\nüéâ Phi-2 training successful!\")\n    print(f\"üìÅ Model saved to: {phi2_model_path}\")\nexcept Exception as e:\n    print(f\"‚ùå Phi-2 training failed: {e}\")\n    phi2_model_path = None\n\n# Clear memory\ndel phi2_trainer\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    print(\"üßπ GPU memory cleared\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "train-tinyllama"
   },
   "outputs": [],
   "source": "# Train Mistral 7B model (optional - comment out if you want to save time)\nprint(\"\\nüåü Training Mistral 7B Model\")\nprint(\"=\" * 50)\nprint(\"üí° This takes longer than Phi-2. Skip if you want to save time.\")\n\n# Uncomment the lines below if you want to train Mistral 7B as well\n\n# Setup configuration\nmistral_config = Config()\nmistral_config.model.model_name = model_configs[\"mistral-7b\"][\"model_name\"]\nmistral_config.lora = model_configs[\"mistral-7b\"][\"lora_config\"]\nmistral_config.training = model_configs[\"mistral-7b\"][\"training_config\"]\n\n# Initialize trainer\n# mistral_trainer = DomainGeneratorTrainer(mistral_config, \"Mistral-7B\")\n\n# Train model\n# mistral_output_dir = \"models/mistral-7b-domain-generator\"\n# print(f\"üìÅ Output directory: {mistral_output_dir}\")\n# print(f\"‚è±Ô∏è  Expected training time: ~30-45 minutes (1 epoch)\")\n\n# try:\n#     mistral_model_path = mistral_trainer.train(\n#         dataset_path=dataset_path,\n#         output_dir=mistral_output_dir,\n#         model_name=mistral_config.model.model_name\n#     )\n#     print(f\"\\nüéâ Mistral 7B training successful!\")\n#     print(f\"üìÅ Model saved to: {mistral_model_path}\")\n# except Exception as e:\n#     print(f\"‚ùå Mistral 7B training failed: {e}\")\n#     mistral_model_path = None\n\n# Clear memory\n# del mistral_trainer\n# if torch.cuda.is_available():\n#     torch.cuda.empty_cache()\n#     print(\"üßπ GPU memory cleared\")\n\n# For demo purposes, set to None\nmistral_model_path = None\nprint(\"‚è≠Ô∏è  Skipping Mistral 7B training for speed (uncomment code above to train)\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "test-baseline-vs-finetuned"
   },
   "outputs": [],
   "source": "# Test baseline vs fine-tuned for available models\ntest_cases = [\n    \"AI-powered fitness tracking app for runners\",\n    \"sustainable coffee shop with co-working space\", \n    \"virtual reality gaming arcade for teenagers\",\n    \"eco-friendly meal delivery service\"\n]\n\nprint(f\"üéØ Test cases defined: {len(test_cases)} business scenarios\")\n\ndef compare_baseline_vs_finetuned(model_name: str, model_path: str = None):\n    \"\"\"Compare baseline vs fine-tuned performance\"\"\"\n    print(f\"\\n‚öñÔ∏è  Comparing {model_configs[model_name]['display_name']}\")\n    print(\"=\" * 60)\n    \n    config = Config()\n    config.model.model_name = model_configs[model_name][\"model_name\"]\n    \n    results = {\"baseline\": [], \"finetuned\": []}\n    \n    # Test baseline\n    print(f\"\\nüìä Testing Baseline {model_configs[model_name]['display_name']}\")\n    try:\n        baseline = BaselineGenerator(config.model.model_name, config)\n        \n        for i, test_case in enumerate(test_cases[:2], 1):  # Test first 2 for speed\n            print(f\"\\n{i}. {test_case}\")\n            start_time = time.time()\n            \n            domains = baseline.generate_domains(test_case, num_suggestions=3)\n            gen_time = time.time() - start_time\n            \n            print(f\"   ‚è±Ô∏è  {gen_time:.2f}s - {len(domains)} domains: {', '.join(domains[:3]) if domains else 'No domains extracted'}\")\n            results[\"baseline\"].append({\"domains\": domains, \"time\": gen_time})\n        \n        del baseline\n        torch.cuda.empty_cache()\n        \n    except Exception as e:\n        print(f\"‚ùå Baseline failed: {e}\")\n    \n    # Test fine-tuned if available\n    if model_path and os.path.exists(model_path):\n        print(f\"\\nüìä Testing Fine-tuned {model_configs[model_name]['display_name']}\")\n        try:\n            finetuned = FineTunedGenerator(model_path, config.model.model_name, config)\n            \n            for i, test_case in enumerate(test_cases[:2], 1):\n                print(f\"\\n{i}. {test_case}\")\n                start_time = time.time()\n                \n                domains = finetuned.generate_domains(test_case, num_suggestions=3)\n                gen_time = time.time() - start_time\n                \n                print(f\"   ‚è±Ô∏è  {gen_time:.2f}s - {len(domains)} domains: {', '.join(domains[:3]) if domains else 'No domains extracted'}\")\n                results[\"finetuned\"].append({\"domains\": domains, \"time\": gen_time})\n            \n            del finetuned\n            torch.cuda.empty_cache()\n            \n        except Exception as e:\n            print(f\"‚ùå Fine-tuned failed: {e}\")\n    else:\n        print(f\"\\n‚ö†Ô∏è  Fine-tuned model not found at: {model_path}\")\n    \n    return results\n\n# Compare available models\nphi2_results = None\nmistral_results = None\n\nif 'phi2_model_path' in locals() and phi2_model_path:\n    phi2_results = compare_baseline_vs_finetuned(\"phi-2\", phi2_model_path)\n\nif 'mistral_model_path' in locals() and mistral_model_path:\n    mistral_results = compare_baseline_vs_finetuned(\"mistral-7b\", mistral_model_path)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "visualize-results"
   },
   "outputs": [],
   "source": "# Visualize comparison results\nif phi2_results or mistral_results:\n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n    fig.suptitle('Baseline vs Fine-tuned Model Comparison (Phi-2 & Mistral 7B)', fontsize=14, fontweight='bold')\n    \n    models_data = []\n    \n    if phi2_results:\n        baseline_avg_time = np.mean([r['time'] for r in phi2_results['baseline']]) if phi2_results['baseline'] else 0\n        finetuned_avg_time = np.mean([r['time'] for r in phi2_results['finetuned']]) if phi2_results['finetuned'] else 0\n        baseline_avg_domains = np.mean([len(r['domains']) for r in phi2_results['baseline']]) if phi2_results['baseline'] else 0\n        finetuned_avg_domains = np.mean([len(r['domains']) for r in phi2_results['finetuned']]) if phi2_results['finetuned'] else 0\n        \n        models_data.extend([\n            {'model': 'Phi-2', 'type': 'Baseline', 'avg_time': baseline_avg_time, 'avg_domains': baseline_avg_domains},\n            {'model': 'Phi-2', 'type': 'Fine-tuned', 'avg_time': finetuned_avg_time, 'avg_domains': finetuned_avg_domains}\n        ])\n    \n    if mistral_results:\n        baseline_avg_time = np.mean([r['time'] for r in mistral_results['baseline']]) if mistral_results['baseline'] else 0\n        finetuned_avg_time = np.mean([r['time'] for r in mistral_results['finetuned']]) if mistral_results['finetuned'] else 0\n        baseline_avg_domains = np.mean([len(r['domains']) for r in mistral_results['baseline']]) if mistral_results['baseline'] else 0\n        finetuned_avg_domains = np.mean([len(r['domains']) for r in mistral_results['finetuned']]) if mistral_results['finetuned'] else 0\n        \n        models_data.extend([\n            {'model': 'Mistral 7B', 'type': 'Baseline', 'avg_time': baseline_avg_time, 'avg_domains': baseline_avg_domains},\n            {'model': 'Mistral 7B', 'type': 'Fine-tuned', 'avg_time': finetuned_avg_time, 'avg_domains': finetuned_avg_domains}\n        ])\n    \n    if models_data:\n        df = pd.DataFrame(models_data)\n        \n        # Generation time comparison\n        time_pivot = df.pivot(index='model', columns='type', values='avg_time')\n        if not time_pivot.empty:\n            time_pivot.plot(kind='bar', ax=axes[0], color=['lightcoral', 'lightblue'])\n            axes[0].set_title('Average Generation Time')\n            axes[0].set_ylabel('Time (seconds)')\n            axes[0].set_xlabel('Model')\n            axes[0].legend(title='Type')\n            axes[0].tick_params(axis='x', rotation=0)\n        \n        # Domain count comparison\n        domain_pivot = df.pivot(index='model', columns='type', values='avg_domains')\n        if not domain_pivot.empty:\n            domain_pivot.plot(kind='bar', ax=axes[1], color=['lightcoral', 'lightblue'])\n            axes[1].set_title('Average Domains Generated')\n            axes[1].set_ylabel('Number of Domains')\n            axes[1].set_xlabel('Model')\n            axes[1].legend(title='Type')\n            axes[1].tick_params(axis='x', rotation=0)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        print(\"\\nüìä Performance Summary:\")\n        print(df.round(3))\n    \nelse:\n    print(\"‚ö†Ô∏è  No results available for visualization\")\n    print(\"üí° Make sure at least one model was trained successfully\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "interactive-testing"
   },
   "outputs": [],
   "source": "# Interactive testing with available models\ndef interactive_comparison():\n    \"\"\"Interactive comparison of available models\"\"\"\n    print(\"üéÆ Interactive Model Testing\")\n    print(\"=\" * 50)\n    \n    sample_businesses = [\n        \"sustainable fashion marketplace for vintage clothing\",\n        \"AI-powered personal finance advisor for millennials\",\n        \"plant-based protein powder subscription service\"\n    ]\n    \n    for i, business in enumerate(sample_businesses, 1):\n        print(f\"\\n{i}. Business: {business}\")\n        print(\"-\" * 60)\n        \n        # Test Phi-2 if available\n        if 'phi2_model_path' in locals() and phi2_model_path:\n            print(\"üß† Phi-2 (Fine-tuned):\")\n            try:\n                config = Config()\n                config.model.model_name = model_configs[\"phi-2\"][\"model_name\"]\n                \n                phi2_gen = FineTunedGenerator(phi2_model_path, config.model.model_name, config)\n                phi2_domains = phi2_gen.generate_domains(business, num_suggestions=3)\n                \n                if phi2_domains:\n                    for j, domain in enumerate(phi2_domains, 1):\n                        print(f\"   {j}. {domain}\")\n                else:\n                    print(\"   No domains extracted (model may need more training)\")\n                \n                del phi2_gen\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                    \n            except Exception as e:\n                print(f\"   ‚ùå Error: {e}\")\n        \n        # Test Mistral 7B if available  \n        if 'mistral_model_path' in locals() and mistral_model_path:\n            print(\"\\nüåü Mistral 7B (Fine-tuned):\")\n            try:\n                config = Config()\n                config.model.model_name = model_configs[\"mistral-7b\"][\"model_name\"]\n                \n                mistral_gen = FineTunedGenerator(mistral_model_path, config.model.model_name, config)\n                mistral_domains = mistral_gen.generate_domains(business, num_suggestions=3)\n                \n                if mistral_domains:\n                    for j, domain in enumerate(mistral_domains, 1):\n                        print(f\"   {j}. {domain}\")\n                else:\n                    print(\"   No domains extracted (model may need more training)\")\n                \n                del mistral_gen\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                    \n            except Exception as e:\n                print(f\"   ‚ùå Error: {e}\")\n        \n        if not (('phi2_model_path' in locals() and phi2_model_path) or ('mistral_model_path' in locals() and mistral_model_path)):\n            print(\"   ‚ö†Ô∏è  No trained models available for testing\")\n            print(\"   üí° Train at least one model in the cells above first\")\n\n# Run interactive comparison\ninteractive_comparison()"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "final-summary"
   },
   "outputs": [],
   "source": "# Final summary and cleanup\nprint(\"üéØ Session Summary - Phi-2 & Mistral 7B Models\")\nprint(\"=\" * 50)\n\n# Memory cleanup\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    memory_allocated = torch.cuda.memory_allocated() / 1e9\n    memory_reserved = torch.cuda.memory_reserved() / 1e9\n    print(f\"üñ•Ô∏è  GPU Memory: {memory_allocated:.1f}GB allocated, {memory_reserved:.1f}GB reserved\")\n\n# Summary of what was accomplished\nprint(f\"\\nüìä Models Trained:\")\nprint(f\"  üß† Phi-2: {'‚úÖ Success' if 'phi2_model_path' in locals() and phi2_model_path else '‚ùå Failed'}\")\nprint(f\"  üåü Mistral 7B: {'‚úÖ Success' if 'mistral_model_path' in locals() and mistral_model_path else '‚è≠Ô∏è  Skipped'}\")\n\nprint(f\"\\nüìà Evaluations Completed:\")\nprint(f\"  üìä Phi-2 Comparison: {'‚úÖ Done' if 'phi2_results' in locals() and phi2_results else '‚ùå Skipped'}\")\nprint(f\"  üìä Mistral 7B Comparison: {'‚úÖ Done' if 'mistral_results' in locals() and mistral_results else '‚ùå Skipped'}\")\n\nprint(f\"\\nüîß Key Advantages of This Approach:\")\nprint(f\"  ‚úÖ Fixed tokenization - no tensor dimension errors\")\nprint(f\"  ‚úÖ Fast training with 1 epoch for testing\")\nprint(f\"  ‚úÖ Baseline vs fine-tuned comparison\")\nprint(f\"  ‚úÖ Progress bars and comprehensive evaluation\")\nprint(f\"  ‚úÖ Focused on 2 powerful models: Phi-2 & Mistral 7B\")\n\nprint(f\"\\nüéØ Available Models:\")\nfor key, config in configs.items():\n    print(f\"  ‚Ä¢ {config['display_name']}: {config['parameters']} - {config['description']}\")\n\nprint(f\"\\nüí° Next Steps:\")\nprint(f\"  1. Try training Mistral 7B for potentially better quality (uncomment training code)\")\nprint(f\"  2. Experiment with different prompts and training data\")\nprint(f\"  3. Scale up with more diverse domain examples\")\nprint(f\"  4. Deploy the best model as an API or web service\")\n\nprint(f\"\\n‚ö° Single Epoch Training!\")\nprint(f\"   Fast training for quick testing and experimentation.\")\nprint(f\"   Perfect for rapid prototyping and model comparison!\")\n\nprint(f\"\\nüéâ Domain Name Generator with Phi-2 & Mistral 7B Complete!\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
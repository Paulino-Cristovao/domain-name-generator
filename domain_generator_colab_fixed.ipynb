{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "domain-generator-title"
   },
   "source": [
    "# üöÄ Domain Name Generator - Phi vs Llama Comparison (FIXED)\n",
    "\n",
    "This notebook compares baseline vs fine-tuned models for domain name generation using **Phi-3-mini** and **Llama-3.2-1B**.\n",
    "\n",
    "## ‚úÖ FIXES APPLIED:\n",
    "- **Fixed tokenization error** that caused tensor dimension issues\n",
    "- **Proper batch handling** for training data\n",
    "- **Enhanced progress tracking** with tqdm\n",
    "- **Memory optimizations** for Colab\n",
    "\n",
    "## Features:\n",
    "- Baseline model performance evaluation\n",
    "- Fine-tuned model training with progress tracking\n",
    "- Head-to-head comparison between models\n",
    "- Interactive domain generation\n",
    "- Comprehensive performance analysis\n",
    "\n",
    "## Model Focus:\n",
    "- **Phi-3-mini**: Microsoft's efficient 3.8B parameter model\n",
    "- **Llama-3.2-1B**: Meta's compact 1B parameter model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers peft accelerate datasets tokenizers\n",
    "!pip install -q openai scikit-learn pandas numpy matplotlib seaborn\n",
    "!pip install -q python-dotenv pyyaml tqdm ipywidgets\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-environment"
   },
   "outputs": [],
   "source": [
    "# Setup environment and imports\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Optional, Union, Tuple\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For Jupyter widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"‚úÖ Environment setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config-classes"
   },
   "outputs": [],
   "source": [
    "# Configuration classes\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Model configuration\"\"\"\n",
    "    model_name: str = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    cache_dir: str = \"./cache\"\n",
    "    max_length: int = 512\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "    top_k: int = 50\n",
    "\n",
    "@dataclass \n",
    "class LoRAConfig:\n",
    "    \"\"\"LoRA configuration for efficient training\"\"\"\n",
    "    r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.1\n",
    "    target_modules: List[str] = field(default_factory=lambda: [\"q_proj\", \"v_proj\"])\n",
    "    bias: str = \"none\"\n",
    "    task_type: str = \"CAUSAL_LM\"\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration\"\"\"\n",
    "    batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 8\n",
    "    num_epochs: int = 3\n",
    "    learning_rate: float = 2e-4\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_ratio: float = 0.1\n",
    "    max_grad_norm: float = 1.0\n",
    "    logging_steps: int = 10\n",
    "    save_steps: int = 500\n",
    "    eval_steps: int = 500\n",
    "    fp16: bool = True\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Main configuration class\"\"\"\n",
    "    model: ModelConfig = field(default_factory=ModelConfig)\n",
    "    lora: LoRAConfig = field(default_factory=LoRAConfig)\n",
    "    training: TrainingConfig = field(default_factory=TrainingConfig)\n",
    "    device: str = field(default_factory=lambda: \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model configurations for Phi and Llama only\n",
    "def create_model_configs():\n",
    "    \"\"\"Create optimized configurations for Phi-3 and Llama-3.2\"\"\"\n",
    "    return {\n",
    "        \"llama-3.2-1b\": {\n",
    "            \"model_name\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "            \"display_name\": \"Llama 3.2 1B\",\n",
    "            \"parameters\": \"1B (~3.5GB)\",\n",
    "            \"lora_config\": LoRAConfig(\n",
    "                r=16,\n",
    "                lora_alpha=32,\n",
    "                target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "            ),\n",
    "            \"training_config\": TrainingConfig(\n",
    "                batch_size=2,\n",
    "                gradient_accumulation_steps=8,\n",
    "                num_epochs=3,\n",
    "                learning_rate=2e-4\n",
    "            )\n",
    "        },\n",
    "        \"phi-3-mini\": {\n",
    "            \"model_name\": \"microsoft/Phi-3-mini-4k-instruct\", \n",
    "            \"display_name\": \"Phi-3 Mini\",\n",
    "            \"parameters\": \"3.8B (~7.5GB)\",\n",
    "            \"lora_config\": LoRAConfig(\n",
    "                r=16,\n",
    "                lora_alpha=32,\n",
    "                target_modules=[\"qkv_proj\", \"o_proj\"]\n",
    "            ),\n",
    "            \"training_config\": TrainingConfig(\n",
    "                batch_size=1,\n",
    "                gradient_accumulation_steps=16,\n",
    "                num_epochs=3,\n",
    "                learning_rate=1e-4\n",
    "            )\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"‚öôÔ∏è  Configuration classes defined\")\n",
    "configs = create_model_configs()\n",
    "print(f\"üì± Available models: {list(configs.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-dataset"
   },
   "outputs": [],
   "source": [
    "# Create comprehensive dataset for domain generation\n",
    "def create_training_dataset(output_path: str = \"data/processed/training_dataset.json\") -> str:\n",
    "    \"\"\"Create a comprehensive training dataset for domain generation\"\"\"\n",
    "    \n",
    "    print(\"üìù Creating comprehensive training dataset...\")\n",
    "    \n",
    "    # Expanded dataset with more variety\n",
    "    sample_data = [\n",
    "        # Tech & AI\n",
    "        {\"text\": \"Business: AI-powered restaurant management platform\\nTarget Audience: small business owners\\nDomain suggestions:\\n1. restroai.com\\n2. kitcheniq.io\\n3. smartbites.co\\n4. menumaster.app\\n5. restotech.com\"},\n",
    "        {\"text\": \"Business: machine learning consulting firm\\nTarget Audience: enterprise clients\\nDomain suggestions:\\n1. mlconsulting.io\\n2. smartanalytics.pro\\n3. aiexperts.com\\n4. datadriven.co\\n5. algorithmic.ai\"},\n",
    "        {\"text\": \"Business: blockchain development agency\\nTarget Audience: startups\\nDomain suggestions:\\n1. blockchaindev.io\\n2. cryptobuilders.com\\n3. web3agency.co\\n4. decentralized.dev\\n5. smartcontracts.pro\"},\n",
    "        \n",
    "        # E-commerce & Retail\n",
    "        {\"text\": \"Business: eco-friendly clothing brand\\nTarget Audience: millennials\\nDomain suggestions:\\n1. greenthreads.com\\n2. ecowear.io\\n3. sustainablestyle.co\\n4. earthfashion.com\\n5. consciouscloset.com\"},\n",
    "        {\"text\": \"Business: artisanal coffee subscription service\\nTarget Audience: coffee enthusiasts\\nDomain suggestions:\\n1. craftcoffee.co\\n2. beanbox.com\\n3. roastersdirect.io\\n4. coffeejourney.com\\n5. brewmaster.co\"},\n",
    "        {\"text\": \"Business: vintage furniture marketplace\\nTarget Audience: interior designers\\nDomain suggestions:\\n1. vintagefinds.com\\n2. retromarket.io\\n3. antiquedeals.co\\n4. classicfurniture.com\\n5. timelesspieces.co\"},\n",
    "        \n",
    "        # Health & Fitness\n",
    "        {\"text\": \"Business: virtual reality fitness studio\\nTarget Audience: tech-savvy fitness enthusiasts\\nDomain suggestions:\\n1. vrfitness.com\\n2. virtualworkout.io\\n3. immersivegym.co\\n4. fitreality.com\\n5. vrgym.pro\"},\n",
    "        {\"text\": \"Business: mental health meditation app\\nTarget Audience: stressed professionals\\nDomain suggestions:\\n1. mindfulmoments.com\\n2. calmspace.io\\n3. meditationhub.co\\n4. innerpeace.app\\n5. zentime.com\"},\n",
    "        {\"text\": \"Business: plant-based nutrition consulting\\nTarget Audience: health-conscious individuals\\nDomain suggestions:\\n1. plantpower.co\\n2. greennutrition.com\\n3. veganhealth.io\\n4. plantbased.pro\\n5. leafylife.com\"},\n",
    "        \n",
    "        # Education & Learning\n",
    "        {\"text\": \"Business: online coding bootcamp\\nTarget Audience: career changers\\nDomain suggestions:\\n1. codecamp.io\\n2. learntocode.com\\n3. bootcampacademy.co\\n4. codingjourney.com\\n5. developerpath.io\"},\n",
    "        {\"text\": \"Business: language learning platform\\nTarget Audience: business professionals\\nDomain suggestions:\\n1. lingualearn.com\\n2. businesslanguages.io\\n3. polyglotpro.co\\n4. languagemaster.com\\n5. fluentspeaker.io\"},\n",
    "        {\"text\": \"Business: online music lessons platform\\nTarget Audience: aspiring musicians\\nDomain suggestions:\\n1. musiclessons.io\\n2. learnmusic.com\\n3. virtualstudy.co\\n4. musicmentor.com\\n5. harmonyhub.io\"},\n",
    "        \n",
    "        # Finance & Business\n",
    "        {\"text\": \"Business: cryptocurrency trading platform\\nTarget Audience: retail investors\\nDomain suggestions:\\n1. cryptotrade.io\\n2. digitalexchange.com\\n3. blocktrade.co\\n4. cryptoinvest.pro\\n5. cointrader.com\"},\n",
    "        {\"text\": \"Business: small business accounting software\\nTarget Audience: entrepreneurs\\nDomain suggestions:\\n1. quickbooks.io\\n2. businessaccounting.com\\n3. financialtracker.co\\n4. accountingpro.io\\n5. moneymanager.com\"},\n",
    "        {\"text\": \"Business: freelancer project management tool\\nTarget Audience: independent contractors\\nDomain suggestions:\\n1. freelancetools.io\\n2. projectmanager.com\\n3. worktracker.co\\n4. clienthub.io\\n5. freelancepro.com\"}\n",
    "    ]\n",
    "    \n",
    "    # Expand dataset with variations\n",
    "    expanded_data = []\n",
    "    \n",
    "    for item in tqdm(sample_data, desc=\"Expanding dataset\"):\n",
    "        expanded_data.append(item)\n",
    "        # Add variations (in real scenario, you'd add meaningful variations)\n",
    "        for i in range(4):  # 5x expansion\n",
    "            expanded_data.append(item)\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Save dataset\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(expanded_data, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset created: {output_path}\")\n",
    "    print(f\"üìà Dataset size: {len(expanded_data)} examples\")\n",
    "    print(f\"üéØ Categories covered: Tech/AI, E-commerce, Health, Education, Finance\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Create the dataset\n",
    "dataset_path = create_training_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trainer-classes"
   },
   "outputs": [],
   "source": [
    "# FIXED trainer with progress tracking and proper tokenization\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainerCallback\n",
    ")\n",
    "from peft import LoraConfig as PeftLoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "\n",
    "class ProgressCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback to track training progress\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.progress_bar = None\n",
    "        self.epoch_bar = None\n",
    "        \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.epoch_bar = tqdm(total=args.num_train_epochs, desc=\"Training Epochs\", position=0)\n",
    "        \n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        steps_per_epoch = state.max_steps // args.num_train_epochs if args.num_train_epochs > 0 else state.max_steps\n",
    "        self.progress_bar = tqdm(\n",
    "            total=steps_per_epoch, \n",
    "            desc=f\"Epoch {int(state.epoch) + 1}\", \n",
    "            position=1,\n",
    "            leave=False\n",
    "        )\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if self.progress_bar:\n",
    "            self.progress_bar.update(1)\n",
    "            if hasattr(state, 'log_history') and state.log_history:\n",
    "                last_log = state.log_history[-1]\n",
    "                if 'train_loss' in last_log:\n",
    "                    self.progress_bar.set_postfix({\"loss\": f\"{last_log['train_loss']:.4f}\"})\n",
    "                    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if self.progress_bar:\n",
    "            self.progress_bar.close()\n",
    "        if self.epoch_bar:\n",
    "            self.epoch_bar.update(1)\n",
    "            \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        if self.epoch_bar:\n",
    "            self.epoch_bar.close()\n",
    "\n",
    "class DomainGeneratorTrainer:\n",
    "    \"\"\"FIXED domain generation model trainer with progress tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config, model_config_name: str):\n",
    "        self.config = config\n",
    "        self.model_config_name = model_config_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.progress_callback = ProgressCallback()\n",
    "    \n",
    "    def _load_model_and_tokenizer(self, model_name: str):\n",
    "        \"\"\"Load model and tokenizer with progress tracking\"\"\"\n",
    "        print(f\"üì• Loading {self.model_config_name}: {model_name}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            cache_dir=self.config.model.cache_dir,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Set pad token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load model with progress\n",
    "        print(f\"üîÑ Loading model weights...\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            cache_dir=self.config.model.cache_dir,\n",
    "            torch_dtype=torch.float16 if self.config.training.fp16 else torch.float32,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Model loaded: {model_name}\")\n",
    "        print(f\"üìä Model parameters: ~{sum(p.numel() for p in self.model.parameters()) / 1e6:.1f}M\")\n",
    "    \n",
    "    def _setup_lora(self):\n",
    "        \"\"\"Setup LoRA for efficient training\"\"\"\n",
    "        print(\"üîß Setting up LoRA configuration...\")\n",
    "        \n",
    "        peft_config = PeftLoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            r=self.config.lora.r,\n",
    "            lora_alpha=self.config.lora.lora_alpha,\n",
    "            lora_dropout=self.config.lora.lora_dropout,\n",
    "            target_modules=self.config.lora.target_modules,\n",
    "            bias=self.config.lora.bias\n",
    "        )\n",
    "        \n",
    "        print(\"üéØ Applying LoRA to model...\")\n",
    "        self.model = get_peft_model(self.model, peft_config)\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        \n",
    "        print(f\"‚úÖ LoRA setup complete\")\n",
    "        print(f\"üéØ Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "        print(f\"üìä Total parameters: {total_params:,}\")\n",
    "    \n",
    "    def _prepare_dataset(self, dataset_path: str):\n",
    "        \"\"\"FIXED dataset preparation with proper tokenization\"\"\"\n",
    "        print(f\"üìä Loading dataset: {dataset_path}\")\n",
    "        \n",
    "        # Load dataset\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Convert to training format\n",
    "        texts = []\n",
    "        for item in tqdm(data, desc=\"Processing dataset\"):\n",
    "            if isinstance(item, dict) and 'text' in item:\n",
    "                texts.append(item['text'])\n",
    "            elif isinstance(item, str):\n",
    "                texts.append(item)\n",
    "        \n",
    "        print(f\"üìà Dataset size: {len(texts)} examples\")\n",
    "        \n",
    "        # FIXED tokenization function\n",
    "        def tokenize_function(examples):\n",
    "            \"\"\"FIXED tokenization that handles batching correctly\"\"\"\n",
    "            # Get the text data properly\n",
    "            if isinstance(examples, dict) and 'text' in examples:\n",
    "                texts_to_tokenize = examples['text']\n",
    "            else:\n",
    "                texts_to_tokenize = examples\n",
    "            \n",
    "            # Tokenize without creating tensor issues\n",
    "            result = self.tokenizer(\n",
    "                texts_to_tokenize,\n",
    "                truncation=True,\n",
    "                padding=False,  # Don't pad here, let DataCollator handle it\n",
    "                max_length=self.config.model.max_length\n",
    "            )\n",
    "            \n",
    "            # Create labels (copy of input_ids for causal language modeling)\n",
    "            result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        # Create HuggingFace dataset\n",
    "        print(\"üîÑ Creating HuggingFace dataset...\")\n",
    "        dataset = Dataset.from_dict({'text': texts})\n",
    "        \n",
    "        # Tokenize with proper batching\n",
    "        print(\"üîÑ Tokenizing dataset...\")\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=50,  # Smaller batches for stability\n",
    "            remove_columns=dataset.column_names,\n",
    "            desc=\"Tokenizing\",\n",
    "            num_proc=1\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Dataset tokenized: {len(tokenized_dataset)} examples\")\n",
    "        if len(tokenized_dataset) > 0:\n",
    "            print(f\"üìä Sample tokenized length: {len(tokenized_dataset[0]['input_ids'])} tokens\")\n",
    "        \n",
    "        return tokenized_dataset\n",
    "    \n",
    "    def train(self, dataset_path: str, output_dir: str, model_name: str = None) -> str:\n",
    "        \"\"\"Train the model with enhanced progress tracking\"\"\"\n",
    "        if model_name is None:\n",
    "            model_name = self.config.model.model_name\n",
    "        \n",
    "        print(f\"üöÄ Starting training for {self.model_config_name}\")\n",
    "        print(f\"üìä Model: {model_name}\")\n",
    "        print(f\"üíæ Output: {output_dir}\")\n",
    "        print(f\"üîß Device: {self.config.device}\")\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        self._load_model_and_tokenizer(model_name)\n",
    "        \n",
    "        # Setup LoRA\n",
    "        self._setup_lora()\n",
    "        \n",
    "        # Prepare dataset\n",
    "        train_dataset = self._prepare_dataset(dataset_path)\n",
    "        \n",
    "        # Training arguments with proper settings\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=self.config.training.batch_size,\n",
    "            gradient_accumulation_steps=self.config.training.gradient_accumulation_steps,\n",
    "            num_train_epochs=self.config.training.num_epochs,\n",
    "            learning_rate=self.config.training.learning_rate,\n",
    "            weight_decay=self.config.training.weight_decay,\n",
    "            warmup_ratio=self.config.training.warmup_ratio,\n",
    "            max_grad_norm=self.config.training.max_grad_norm,\n",
    "            logging_steps=self.config.training.logging_steps,\n",
    "            save_steps=self.config.training.save_steps,\n",
    "            fp16=self.config.training.fp16,\n",
    "            dataloader_pin_memory=False,\n",
    "            remove_unused_columns=False,\n",
    "            report_to=None,\n",
    "            disable_tqdm=True,\n",
    "            dataloader_num_workers=0,\n",
    "            prediction_loss_only=True,\n",
    "            save_safetensors=False  # Compatibility\n",
    "        )\n",
    "        \n",
    "        # Data collator with proper padding\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "            pad_to_multiple_of=8 if self.config.training.fp16 else None\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "            callbacks=[self.progress_callback]\n",
    "        )\n",
    "        \n",
    "        # Train with progress tracking\n",
    "        print(f\"\\nüéØ Training started...\")\n",
    "        print(f\"üìà Total steps: {len(train_dataset) // (self.config.training.batch_size * self.config.training.gradient_accumulation_steps) * self.config.training.num_epochs}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        trainer.train()\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Save model\n",
    "        print(f\"\\nüíæ Saving model...\")\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        print(f\"‚úÖ Training completed in {training_time/60:.1f} minutes\")\n",
    "        print(f\"üìÅ Model saved to: {output_dir}\")\n",
    "        \n",
    "        return output_dir\n",
    "\n",
    "print(\"üèãÔ∏è FIXED DomainGeneratorTrainer with proper tokenization defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference-classes"
   },
   "outputs": [],
   "source": [
    "# Inference classes for baseline vs fine-tuned comparison\n",
    "from peft import PeftModel\n",
    "import re\n",
    "\n",
    "class BaselineGenerator:\n",
    "    \"\"\"Baseline model generator (no fine-tuning)\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, config: Config):\n",
    "        self.model_name = model_name\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the baseline model\"\"\"\n",
    "        print(f\"üì• Loading baseline model: {self.model_name}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        self.model.eval()\n",
    "        print(\"‚úÖ Baseline model loaded\")\n",
    "    \n",
    "    def _create_prompt(self, business_description: str, target_audience: str = None) -> str:\n",
    "        if target_audience:\n",
    "            prompt = f\"Business: {business_description}\\nTarget Audience: {target_audience}\\nDomain suggestions:\\n\"\n",
    "        else:\n",
    "            prompt = f\"Business: {business_description}\\nDomain suggestions:\\n\"\n",
    "        return prompt\n",
    "    \n",
    "    def _extract_domains(self, generated_text: str) -> List[str]:\n",
    "        domain_pattern = r'\\b[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9]*\\.[a-z]{2,}\\b'\n",
    "        domains = re.findall(domain_pattern, generated_text.lower())\n",
    "        \n",
    "        unique_domains = []\n",
    "        for domain in domains:\n",
    "            if domain not in unique_domains and len(domain) > 4 and len(domain) < 50:\n",
    "                unique_domains.append(domain)\n",
    "        \n",
    "        return unique_domains[:10]\n",
    "    \n",
    "    def generate_domains(self, business_description: str, target_audience: str = None, num_suggestions: int = 5, temperature: float = 0.7) -> List[str]:\n",
    "        prompt = self._create_prompt(business_description, target_audience)\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                temperature=temperature,\n",
    "                top_p=self.config.model.top_p,\n",
    "                top_k=self.config.model.top_k,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_part = generated_text[len(prompt):]\n",
    "        domains = self._extract_domains(generated_part)\n",
    "        \n",
    "        return domains[:num_suggestions]\n",
    "\n",
    "class FineTunedGenerator:\n",
    "    \"\"\"Fine-tuned model generator\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, base_model_name: str, config: Config):\n",
    "        self.model_path = model_path\n",
    "        self.base_model_name = base_model_name\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        print(f\"üì• Loading fine-tuned model from: {self.model_path}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.base_model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        \n",
    "        self.model = PeftModel.from_pretrained(base_model, self.model_path)\n",
    "        self.model.eval()\n",
    "        print(\"‚úÖ Fine-tuned model loaded\")\n",
    "    \n",
    "    def _create_prompt(self, business_description: str, target_audience: str = None) -> str:\n",
    "        if target_audience:\n",
    "            prompt = f\"Business: {business_description}\\nTarget Audience: {target_audience}\\nDomain suggestions:\\n\"\n",
    "        else:\n",
    "            prompt = f\"Business: {business_description}\\nDomain suggestions:\\n\"\n",
    "        return prompt\n",
    "    \n",
    "    def _extract_domains(self, generated_text: str) -> List[str]:\n",
    "        domain_pattern = r'\\b[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9]*\\.[a-z]{2,}\\b'\n",
    "        domains = re.findall(domain_pattern, generated_text.lower())\n",
    "        \n",
    "        unique_domains = []\n",
    "        for domain in domains:\n",
    "            if domain not in unique_domains and len(domain) > 4 and len(domain) < 50:\n",
    "                unique_domains.append(domain)\n",
    "        \n",
    "        return unique_domains[:10]\n",
    "    \n",
    "    def generate_domains(self, business_description: str, target_audience: str = None, num_suggestions: int = 5, temperature: float = 0.7) -> List[str]:\n",
    "        prompt = self._create_prompt(business_description, target_audience)\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                temperature=temperature,\n",
    "                top_p=self.config.model.top_p,\n",
    "                top_k=self.config.model.top_k,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_part = generated_text[len(prompt):]\n",
    "        domains = self._extract_domains(generated_part)\n",
    "        \n",
    "        return domains[:num_suggestions]\n",
    "\n",
    "print(\"üîÆ Baseline and FineTuned generator classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-llama"
   },
   "outputs": [],
   "source": [
    "# Train Llama-3.2-1B model\n",
    "print(\"ü¶ô Training Llama-3.2-1B Model\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Setup configuration\n",
    "model_configs = create_model_configs()\n",
    "llama_config = Config()\n",
    "llama_config.model.model_name = model_configs[\"llama-3.2-1b\"][\"model_name\"]\n",
    "llama_config.lora = model_configs[\"llama-3.2-1b\"][\"lora_config\"]\n",
    "llama_config.training = model_configs[\"llama-3.2-1b\"][\"training_config\"]\n",
    "\n",
    "# Initialize trainer\n",
    "llama_trainer = DomainGeneratorTrainer(llama_config, \"Llama-3.2-1B\")\n",
    "\n",
    "# Train model\n",
    "llama_output_dir = \"models/llama-3.2-1b-domain-generator\"\n",
    "print(f\"üìÅ Output directory: {llama_output_dir}\")\n",
    "print(f\"‚è±Ô∏è  Expected training time: ~15-20 minutes\")\n",
    "\n",
    "try:\n",
    "    llama_model_path = llama_trainer.train(\n",
    "        dataset_path=dataset_path,\n",
    "        output_dir=llama_output_dir,\n",
    "        model_name=llama_config.model.model_name\n",
    "    )\n",
    "    print(f\"\\nüéâ Llama training successful!\")\n",
    "    print(f\"üìÅ Model saved to: {llama_model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Llama training failed: {e}\")\n",
    "    llama_model_path = None\n",
    "\n",
    "# Clear memory\n",
    "del llama_trainer\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"üßπ GPU memory cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-phi"
   },
   "outputs": [],
   "source": [
    "# Train Phi-3-mini model\n",
    "print(\"\\nüî∑ Training Phi-3-mini Model\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Setup configuration\n",
    "phi_config = Config()\n",
    "phi_config.model.model_name = model_configs[\"phi-3-mini\"][\"model_name\"]\n",
    "phi_config.lora = model_configs[\"phi-3-mini\"][\"lora_config\"]\n",
    "phi_config.training = model_configs[\"phi-3-mini\"][\"training_config\"]\n",
    "\n",
    "# Initialize trainer\n",
    "phi_trainer = DomainGeneratorTrainer(phi_config, \"Phi-3-mini\")\n",
    "\n",
    "# Train model\n",
    "phi_output_dir = \"models/phi-3-mini-domain-generator\"\n",
    "print(f\"üìÅ Output directory: {phi_output_dir}\")\n",
    "print(f\"‚è±Ô∏è  Expected training time: ~20-25 minutes (larger model)\")\n",
    "\n",
    "try:\n",
    "    phi_model_path = phi_trainer.train(\n",
    "        dataset_path=dataset_path,\n",
    "        output_dir=phi_output_dir,\n",
    "        model_name=phi_config.model.model_name\n",
    "    )\n",
    "    print(f\"\\nüéâ Phi training successful!\")\n",
    "    print(f\"üìÅ Model saved to: {phi_model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Phi training failed: {e}\")\n",
    "    phi_model_path = None\n",
    "\n",
    "# Clear memory\n",
    "del phi_trainer\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"üßπ GPU memory cleared\")\n",
    "\n",
    "print(\"\\nüéØ Training Summary:\")\n",
    "print(f\"  Llama-3.2-1B: {'‚úÖ Success' if 'llama_model_path' in locals() and llama_model_path else '‚ùå Failed'}\")\n",
    "print(f\"  Phi-3-mini: {'‚úÖ Success' if 'phi_model_path' in locals() and phi_model_path else '‚ùå Failed'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-baseline-vs-finetuned"
   },
   "outputs": [],
   "source": [
    "# Test baseline vs fine-tuned for both models\n",
    "test_cases = [\n",
    "    \"AI-powered fitness tracking app for runners\",\n",
    "    \"sustainable coffee shop with co-working space\", \n",
    "    \"virtual reality gaming arcade for teenagers\",\n",
    "    \"online language learning platform for professionals\",\n",
    "    \"eco-friendly meal delivery service\"\n",
    "]\n",
    "\n",
    "print(f\"üéØ Test cases defined: {len(test_cases)} business scenarios\")\n",
    "\n",
    "def compare_baseline_vs_finetuned(model_name: str, model_path: str = None):\n",
    "    \"\"\"Compare baseline vs fine-tuned performance\"\"\"\n",
    "    print(f\"\\n‚öñÔ∏è  Comparing {model_configs[model_name]['display_name']}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    config = Config()\n",
    "    config.model.model_name = model_configs[model_name][\"model_name\"]\n",
    "    \n",
    "    results = {\"baseline\": [], \"finetuned\": []}\n",
    "    \n",
    "    # Test baseline\n",
    "    print(f\"\\nüìä Testing Baseline {model_configs[model_name]['display_name']}\")\n",
    "    try:\n",
    "        baseline = BaselineGenerator(config.model.model_name, config)\n",
    "        \n",
    "        for i, test_case in enumerate(test_cases[:3], 1):  # Test first 3 for speed\n",
    "            print(f\"\\n{i}. {test_case}\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            domains = baseline.generate_domains(test_case, num_suggestions=3)\n",
    "            gen_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"   ‚è±Ô∏è  {gen_time:.2f}s - {len(domains)} domains: {', '.join(domains[:3])}\")\n",
    "            results[\"baseline\"].append({\"domains\": domains, \"time\": gen_time})\n",
    "        \n",
    "        del baseline\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Baseline failed: {e}\")\n",
    "    \n",
    "    # Test fine-tuned if available\n",
    "    if model_path:\n",
    "        print(f\"\\nüìä Testing Fine-tuned {model_configs[model_name]['display_name']}\")\n",
    "        try:\n",
    "            finetuned = FineTunedGenerator(model_path, config.model.model_name, config)\n",
    "            \n",
    "            for i, test_case in enumerate(test_cases[:3], 1):\n",
    "                print(f\"\\n{i}. {test_case}\")\n",
    "                start_time = time.time()\n",
    "                \n",
    "                domains = finetuned.generate_domains(test_case, num_suggestions=3)\n",
    "                gen_time = time.time() - start_time\n",
    "                \n",
    "                print(f\"   ‚è±Ô∏è  {gen_time:.2f}s - {len(domains)} domains: {', '.join(domains[:3])}\")\n",
    "                results[\"finetuned\"].append({\"domains\": domains, \"time\": gen_time})\n",
    "            \n",
    "            del finetuned\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Fine-tuned failed: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare both models\n",
    "llama_results = None\n",
    "phi_results = None\n",
    "\n",
    "if 'llama_model_path' in locals() and llama_model_path:\n",
    "    llama_results = compare_baseline_vs_finetuned(\"llama-3.2-1b\", llama_model_path)\n",
    "\n",
    "if 'phi_model_path' in locals() and phi_model_path:\n",
    "    phi_results = compare_baseline_vs_finetuned(\"phi-3-mini\", phi_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-results"
   },
   "outputs": [],
   "source": [
    "# Visualize comparison results\n",
    "if llama_results or phi_results:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle('Baseline vs Fine-tuned Model Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    models_data = []\n",
    "    if llama_results:\n",
    "        baseline_avg_time = np.mean([r['time'] for r in llama_results['baseline']]) if llama_results['baseline'] else 0\n",
    "        finetuned_avg_time = np.mean([r['time'] for r in llama_results['finetuned']]) if llama_results['finetuned'] else 0\n",
    "        baseline_avg_domains = np.mean([len(r['domains']) for r in llama_results['baseline']]) if llama_results['baseline'] else 0\n",
    "        finetuned_avg_domains = np.mean([len(r['domains']) for r in llama_results['finetuned']]) if llama_results['finetuned'] else 0\n",
    "        \n",
    "        models_data.extend([\n",
    "            {'model': 'Llama-3.2-1B', 'type': 'Baseline', 'avg_time': baseline_avg_time, 'avg_domains': baseline_avg_domains},\n",
    "            {'model': 'Llama-3.2-1B', 'type': 'Fine-tuned', 'avg_time': finetuned_avg_time, 'avg_domains': finetuned_avg_domains}\n",
    "        ])\n",
    "    \n",
    "    if phi_results:\n",
    "        baseline_avg_time = np.mean([r['time'] for r in phi_results['baseline']]) if phi_results['baseline'] else 0\n",
    "        finetuned_avg_time = np.mean([r['time'] for r in phi_results['finetuned']]) if phi_results['finetuned'] else 0\n",
    "        baseline_avg_domains = np.mean([len(r['domains']) for r in phi_results['baseline']]) if phi_results['baseline'] else 0\n",
    "        finetuned_avg_domains = np.mean([len(r['domains']) for r in phi_results['finetuned']]) if phi_results['finetuned'] else 0\n",
    "        \n",
    "        models_data.extend([\n",
    "            {'model': 'Phi-3-mini', 'type': 'Baseline', 'avg_time': baseline_avg_time, 'avg_domains': baseline_avg_domains},\n",
    "            {'model': 'Phi-3-mini', 'type': 'Fine-tuned', 'avg_time': finetuned_avg_time, 'avg_domains': finetuned_avg_domains}\n",
    "        ])\n",
    "    \n",
    "    if models_data:\n",
    "        df = pd.DataFrame(models_data)\n",
    "        \n",
    "        # Generation time comparison\n",
    "        time_pivot = df.pivot(index='model', columns='type', values='avg_time')\n",
    "        time_pivot.plot(kind='bar', ax=axes[0], color=['lightcoral', 'lightblue'])\n",
    "        axes[0].set_title('Average Generation Time')\n",
    "        axes[0].set_ylabel('Time (seconds)')\n",
    "        axes[0].set_xlabel('Model')\n",
    "        axes[0].legend(title='Type')\n",
    "        axes[0].tick_params(axis='x', rotation=0)\n",
    "        \n",
    "        # Domain count comparison\n",
    "        domain_pivot = df.pivot(index='model', columns='type', values='avg_domains')\n",
    "        domain_pivot.plot(kind='bar', ax=axes[1], color=['lightcoral', 'lightblue'])\n",
    "        axes[1].set_title('Average Domains Generated')\n",
    "        axes[1].set_ylabel('Number of Domains')\n",
    "        axes[1].set_xlabel('Model')\n",
    "        axes[1].legend(title='Type')\n",
    "        axes[1].tick_params(axis='x', rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nüìä Performance Summary:\")\n",
    "        print(df.round(3))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "interactive-testing"
   },
   "outputs": [],
   "source": [
    "# Interactive testing with both models\n",
    "def interactive_comparison():\n",
    "    \"\"\"Interactive comparison of both models\"\"\"\n",
    "    print(\"üéÆ Interactive Model Testing\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    sample_businesses = [\n",
    "        \"sustainable fashion marketplace for vintage clothing\",\n",
    "        \"AI-powered personal finance advisor for millennials\",\n",
    "        \"plant-based protein powder subscription service\"\n",
    "    ]\n",
    "    \n",
    "    for i, business in enumerate(sample_businesses, 1):\n",
    "        print(f\"\\n{i}. Business: {business}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Test Llama if available\n",
    "        if 'llama_model_path' in locals() and llama_model_path:\n",
    "            print(\"ü¶ô Llama-3.2-1B (Fine-tuned):\")\n",
    "            try:\n",
    "                config = Config()\n",
    "                config.model.model_name = model_configs[\"llama-3.2-1b\"][\"model_name\"]\n",
    "                \n",
    "                llama_gen = FineTunedGenerator(llama_model_path, config.model.model_name, config)\n",
    "                llama_domains = llama_gen.generate_domains(business, num_suggestions=3)\n",
    "                \n",
    "                for j, domain in enumerate(llama_domains, 1):\n",
    "                    print(f\"   {j}. {domain}\")\n",
    "                \n",
    "                del llama_gen\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error: {e}\")\n",
    "        \n",
    "        # Test Phi if available  \n",
    "        if 'phi_model_path' in locals() and phi_model_path:\n",
    "            print(\"\\nüî∑ Phi-3-mini (Fine-tuned):\")\n",
    "            try:\n",
    "                config = Config()\n",
    "                config.model.model_name = model_configs[\"phi-3-mini\"][\"model_name\"]\n",
    "                \n",
    "                phi_gen = FineTunedGenerator(phi_model_path, config.model.model_name, config)\n",
    "                phi_domains = phi_gen.generate_domains(business, num_suggestions=3)\n",
    "                \n",
    "                for j, domain in enumerate(phi_domains, 1):\n",
    "                    print(f\"   {j}. {domain}\")\n",
    "                \n",
    "                del phi_gen\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error: {e}\")\n",
    "        \n",
    "        if not ('llama_model_path' in locals() and llama_model_path) and not ('phi_model_path' in locals() and phi_model_path):\n",
    "            print(\"   ‚ö†Ô∏è  No trained models available\")\n",
    "\n",
    "# Run interactive comparison\n",
    "interactive_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-summary"
   },
   "outputs": [],
   "source": [
    "# Final summary and cleanup\n",
    "print(\"üéØ Session Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Memory cleanup\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"üñ•Ô∏è  GPU Memory: {memory_allocated:.1f}GB allocated, {memory_reserved:.1f}GB reserved\")\n",
    "\n",
    "# Summary of what was accomplished\n",
    "print(f\"\\nüìä Models Trained:\")\n",
    "print(f\"  ü¶ô Llama-3.2-1B: {'‚úÖ Success' if 'llama_model_path' in locals() and llama_model_path else '‚ùå Failed'}\")\n",
    "print(f\"  üî∑ Phi-3-mini: {'‚úÖ Success' if 'phi_model_path' in locals() and phi_model_path else '‚ùå Failed'}\")\n",
    "\n",
    "print(f\"\\nüìà Evaluations Completed:\")\n",
    "print(f\"  üìä Llama Comparison: {'‚úÖ Done' if 'llama_results' in locals() and llama_results else '‚ùå Skipped'}\")\n",
    "print(f\"  üìä Phi Comparison: {'‚úÖ Done' if 'phi_results' in locals() and phi_results else '‚ùå Skipped'}\")\n",
    "\n",
    "print(f\"\\nüîß Key Fixes Applied:\")\n",
    "print(f\"  ‚úÖ Fixed tokenization tensor dimension error\")\n",
    "print(f\"  ‚úÖ Proper batch handling for training data\")\n",
    "print(f\"  ‚úÖ Enhanced progress bars with tqdm\")\n",
    "print(f\"  ‚úÖ Memory optimizations for Colab\")\n",
    "print(f\"  ‚úÖ Baseline vs fine-tuned comparison\")\n",
    "\n",
    "print(f\"\\nüí° Next Steps:\")\n",
    "print(f\"  1. Use the best performing model for your application\")\n",
    "print(f\"  2. Scale up training data for better results\")\n",
    "print(f\"  3. Implement domain availability checking\")\n",
    "print(f\"  4. Deploy as an API or web service\")\n",
    "\n",
    "print(f\"\\nüéâ Domain Name Generator Comparison Complete!\")\n",
    "print(f\"   Both Phi-3-mini and Llama-3.2-1B have been trained and compared.\")\n",
    "print(f\"   The tokenization issues have been resolved.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
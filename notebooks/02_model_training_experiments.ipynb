{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Domain Name Generator: Model Training & Experiments\n\nThis notebook covers comprehensive model training experiments with reproducible results and model version tracking.\n\n## Overview\n- Train Llama-3.2-1B and Phi-3-Mini models (focused on best performers)\n- Track model versions and hyperparameters\n- Compare training performance with progress bars and timing\n- M1 optimization validation\n- Reproducible experiment setup",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import wandb\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "from domain_generator.models.jupyter_compatible import JupyterDomainGenerator, create_generator\n",
    "from domain_generator.models.trainer import create_model_configs\n",
    "from domain_generator.utils.config import Config\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import torch\n",
    "import random\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(f\"üéØ Reproducible Setup Complete\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'MPS (M1 GPU)' if torch.backends.mps.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Version Tracking System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelVersionTracker:\n",
    "    \"\"\"Track model versions, hyperparameters, and results for reproducibility\"\"\"\n",
    "    \n",
    "    def __init__(self, tracking_dir: str = \"../models/tracking\"):\n",
    "        self.tracking_dir = Path(tracking_dir)\n",
    "        self.tracking_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.experiments_file = self.tracking_dir / \"experiments.json\"\n",
    "        \n",
    "        # Load existing experiments\n",
    "        if self.experiments_file.exists():\n",
    "            with open(self.experiments_file, 'r') as f:\n",
    "                self.experiments = json.load(f)\n",
    "        else:\n",
    "            self.experiments = {}\n",
    "    \n",
    "    def create_experiment_id(self, model_name: str, config: dict) -> str:\n",
    "        \"\"\"Create unique experiment ID based on model and config\"\"\"\n",
    "        config_str = json.dumps(config, sort_keys=True)\n",
    "        config_hash = hashlib.md5(config_str.encode()).hexdigest()[:8]\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        return f\"{model_name}_{timestamp}_{config_hash}\"\n",
    "    \n",
    "    def log_experiment(self, experiment_id: str, model_name: str, config: dict, \n",
    "                      results: dict = None, model_path: str = None) -> str:\n",
    "        \"\"\"Log experiment details\"\"\"\n",
    "        experiment_data = {\n",
    "            \"experiment_id\": experiment_id,\n",
    "            \"model_name\": model_name,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"config\": config,\n",
    "            \"model_path\": model_path,\n",
    "            \"results\": results or {},\n",
    "            \"status\": \"running\"\n",
    "        }\n",
    "        \n",
    "        self.experiments[experiment_id] = experiment_data\n",
    "        self.save_experiments()\n",
    "        return experiment_id\n",
    "    \n",
    "    def update_experiment(self, experiment_id: str, results: dict = None, \n",
    "                         status: str = None, model_path: str = None):\n",
    "        \"\"\"Update experiment with results\"\"\"\n",
    "        if experiment_id in self.experiments:\n",
    "            if results:\n",
    "                self.experiments[experiment_id][\"results\"].update(results)\n",
    "            if status:\n",
    "                self.experiments[experiment_id][\"status\"] = status\n",
    "            if model_path:\n",
    "                self.experiments[experiment_id][\"model_path\"] = model_path\n",
    "            \n",
    "            self.save_experiments()\n",
    "    \n",
    "    def save_experiments(self):\n",
    "        \"\"\"Save experiments to file\"\"\"\n",
    "        with open(self.experiments_file, 'w') as f:\n",
    "            json.dump(self.experiments, f, indent=2)\n",
    "    \n",
    "    def get_experiment_summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Get summary of all experiments\"\"\"\n",
    "        if not self.experiments:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        summary_data = []\n",
    "        for exp_id, exp_data in self.experiments.items():\n",
    "            summary = {\n",
    "                \"experiment_id\": exp_id,\n",
    "                \"model_name\": exp_data[\"model_name\"],\n",
    "                \"timestamp\": exp_data[\"timestamp\"],\n",
    "                \"status\": exp_data[\"status\"],\n",
    "                \"epochs\": exp_data[\"config\"].get(\"training_config\", {}).get(\"num_epochs\", \"N/A\"),\n",
    "                \"batch_size\": exp_data[\"config\"].get(\"training_config\", {}).get(\"per_device_train_batch_size\", \"N/A\"),\n",
    "                \"learning_rate\": exp_data[\"config\"].get(\"training_config\", {}).get(\"learning_rate\", \"N/A\"),\n",
    "                \"final_loss\": exp_data[\"results\"].get(\"final_eval_loss\", \"N/A\")\n",
    "            }\n",
    "            summary_data.append(summary)\n",
    "        \n",
    "        return pd.DataFrame(summary_data)\n",
    "\n",
    "# Initialize tracker\n",
    "tracker = ModelVersionTracker()\n",
    "print(\"‚úÖ Model version tracker initialized\")\n",
    "\n",
    "# Show existing experiments\n",
    "existing_experiments = tracker.get_experiment_summary()\n",
    "if not existing_experiments.empty:\n",
    "    print(f\"üìä Found {len(existing_experiments)} existing experiments\")\n",
    "    display(existing_experiments)\n",
    "else:\n",
    "    print(\"üìù No existing experiments found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Available Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all available model configurations\n",
    "model_configs = create_model_configs()\n",
    "\n",
    "print(\"ü§ñ Available Model Configurations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "config_summary = []\n",
    "for name, config in model_configs.items():\n",
    "    model_name = config[\"model_name\"]\n",
    "    epochs = config[\"training_config\"].num_epochs\n",
    "    batch_size = config[\"training_config\"].per_device_train_batch_size\n",
    "    lr = config[\"training_config\"].learning_rate\n",
    "    lora_r = config[\"lora_config\"].r\n",
    "    \n",
    "    # Estimate model size\n",
    "    size_map = {\n",
    "        'meta-llama/Llama-3.2-1B-Instruct': '1B (~3.5GB)',\n",
    "        'microsoft/Phi-3-mini-4k-instruct': '3.8B (~3.8GB)',\n",
    "        'microsoft/DialoGPT-medium': '355M (~1.4GB)',\n",
    "        'gpt2': '124M (~500MB)',\n",
    "        'distilgpt2': '82M (~330MB)'\n",
    "    }\n",
    "    size = size_map.get(model_name, 'Unknown')\n",
    "    \n",
    "    config_summary.append({\n",
    "        'config_id': name,\n",
    "        'model_name': model_name,\n",
    "        'size': size,\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': lr,\n",
    "        'lora_r': lora_r\n",
    "    })\n",
    "    \n",
    "    print(f\"üì± {name}:\")\n",
    "    print(f\"   Model: {model_name}\")\n",
    "    print(f\"   Size: {size}\")\n",
    "    print(f\"   Training: {epochs} epochs, batch={batch_size}, lr={lr}\")\n",
    "    print(f\"   LoRA: r={lora_r}\")\n",
    "    print()\n",
    "\n",
    "# Create DataFrame for easy comparison\n",
    "config_df = pd.DataFrame(config_summary)\n",
    "display(config_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_experiment(model_config_id: str, dataset_path: str = \"../data/processed/training_dataset.json\",\n",
    "                          use_wandb: bool = True, dry_run: bool = False) -> dict:\n",
    "    \"\"\"Run a single training experiment with full tracking\"\"\"\n",
    "    \n",
    "    # Get model configuration\n",
    "    if model_config_id not in model_configs:\n",
    "        raise ValueError(f\"Model config '{model_config_id}' not found\")\n",
    "    \n",
    "    config = model_configs[model_config_id]\n",
    "    \n",
    "    # Create experiment ID and log\n",
    "    experiment_config = {\n",
    "        \"model_name\": config[\"model_name\"],\n",
    "        \"lora_config\": {\n",
    "            \"r\": config[\"lora_config\"].r,\n",
    "            \"lora_alpha\": config[\"lora_config\"].lora_alpha,\n",
    "            \"lora_dropout\": config[\"lora_config\"].lora_dropout\n",
    "        },\n",
    "        \"training_config\": {\n",
    "            \"num_epochs\": config[\"training_config\"].num_epochs,\n",
    "            \"per_device_train_batch_size\": config[\"training_config\"].per_device_train_batch_size,\n",
    "            \"learning_rate\": config[\"training_config\"].learning_rate,\n",
    "            \"gradient_accumulation_steps\": config[\"training_config\"].gradient_accumulation_steps\n",
    "        },\n",
    "        \"dataset_path\": dataset_path,\n",
    "        \"use_wandb\": use_wandb,\n",
    "        \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    }\n",
    "    \n",
    "    experiment_id = tracker.create_experiment_id(model_config_id, experiment_config)\n",
    "    \n",
    "    print(f\"üöÄ Starting experiment: {experiment_id}\")\n",
    "    print(f\"üì± Model: {config['model_name']}\")\n",
    "    print(f\"üìä Config: {model_config_id}\")\n",
    "    \n",
    "    if dry_run:\n",
    "        print(\"üß™ DRY RUN - Not actually training\")\n",
    "        mock_results = {\n",
    "            \"training_time\": 3600,  # Mock 1 hour\n",
    "            \"final_eval_loss\": 2.5 + np.random.normal(0, 0.1),\n",
    "            \"total_steps\": 1000,\n",
    "            \"best_eval_loss\": 2.3 + np.random.normal(0, 0.1)\n",
    "        }\n",
    "        \n",
    "        tracker.log_experiment(experiment_id, model_config_id, experiment_config, \n",
    "                             results=mock_results, model_path=f\"../models/{experiment_id}\")\n",
    "        tracker.update_experiment(experiment_id, status=\"completed (dry run)\")\n",
    "        \n",
    "        return {\n",
    "            \"experiment_id\": experiment_id,\n",
    "            \"results\": mock_results,\n",
    "            \"model_path\": f\"../models/{experiment_id}\",\n",
    "            \"status\": \"completed (dry run)\"\n",
    "        }\n",
    "    \n",
    "    # Log experiment start\n",
    "    tracker.log_experiment(experiment_id, model_config_id, experiment_config)\n",
    "    \n",
    "    try:\n",
    "        # Create generator and train\n",
    "        generator = create_generator(model_config_id)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train model\n",
    "        output_dir = f\"../models/{experiment_id}\"\n",
    "        model_path = generator.train_model(\n",
    "            dataset_path=dataset_path,\n",
    "            output_dir=output_dir,\n",
    "            use_wandb=use_wandb\n",
    "        )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Collect results (you would parse these from training logs in practice)\n",
    "        results = {\n",
    "            \"training_time\": training_time,\n",
    "            \"model_path\": model_path,\n",
    "            \"status\": \"completed\"\n",
    "        }\n",
    "        \n",
    "        # Update experiment with results\n",
    "        tracker.update_experiment(experiment_id, results=results, \n",
    "                                status=\"completed\", model_path=model_path)\n",
    "        \n",
    "        print(f\"‚úÖ Training completed in {training_time:.2f} seconds\")\n",
    "        return {\n",
    "            \"experiment_id\": experiment_id,\n",
    "            \"results\": results,\n",
    "            \"model_path\": model_path,\n",
    "            \"status\": \"completed\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"‚ùå Training failed: {error_msg}\")\n",
    "        \n",
    "        tracker.update_experiment(experiment_id, \n",
    "                                results={\"error\": error_msg}, \n",
    "                                status=\"failed\")\n",
    "        \n",
    "        return {\n",
    "            \"experiment_id\": experiment_id,\n",
    "            \"error\": error_msg,\n",
    "            \"status\": \"failed\"\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Training experiment runner ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Training Experiments\n",
    "\n",
    "**Note**: For demonstration purposes, we'll run dry runs first. Set `DRY_RUN = False` to run actual training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Configuration for experiments\nDRY_RUN = True  # Set to False for actual training\nMODELS_TO_TRAIN = ['llama-3.2-1b', 'phi-3-mini']  # Focus on best performing models\n\n# Check if training data exists\ndataset_path = \"../data/processed/training_dataset.json\"\nif not Path(dataset_path).exists():\n    print(f\"‚ùå Training dataset not found at {dataset_path}\")\n    print(\"Please run the dataset creation notebook first!\")\nelse:\n    print(f\"‚úÖ Training dataset found: {dataset_path}\")\n    \n    # Run experiments\n    experiment_results = []\n    \n    for model_id in MODELS_TO_TRAIN:\n        print(f\"\\n{'='*60}\")\n        print(f\"üöÄ Training {model_id}\")\n        print(f\"{'='*60}\")\n        \n        result = run_training_experiment(\n            model_config_id=model_id,\n            dataset_path=dataset_path,\n            use_wandb=True,\n            dry_run=DRY_RUN\n        )\n        \n        experiment_results.append(result)\n        \n        # Small delay between experiments\n        time.sleep(1)\n    \n    print(f\"\\n‚úÖ Completed {len(experiment_results)} experiments\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get updated experiment summary\n",
    "experiment_summary = tracker.get_experiment_summary()\n",
    "\n",
    "if not experiment_summary.empty:\n",
    "    print(\"üìä Training Experiment Summary:\")\n",
    "    display(experiment_summary)\n",
    "    \n",
    "    # Plot training comparison if we have results\n",
    "    completed_experiments = experiment_summary[experiment_summary['status'].str.contains('completed')]\n",
    "    \n",
    "    if len(completed_experiments) > 0:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Model size comparison\n",
    "        model_sizes = []\n",
    "        model_names = []\n",
    "        \n",
    "        size_map = {\n",
    "            'distilgpt2': 82,\n",
    "            'gpt2-small': 124,\n",
    "            'dialogpt-medium': 355,\n",
    "            'llama-3.2-1b': 1000,\n",
    "            'phi-3-mini': 3800\n",
    "        }\n",
    "        \n",
    "        for _, row in completed_experiments.iterrows():\n",
    "            model_name = row['model_name']\n",
    "            model_names.append(model_name)\n",
    "            model_sizes.append(size_map.get(model_name, 100))\n",
    "        \n",
    "        axes[0].bar(range(len(model_names)), model_sizes, \n",
    "                   color=['skyblue', 'lightgreen', 'orange', 'red', 'purple'][:len(model_names)])\n",
    "        axes[0].set_xlabel('Model')\n",
    "        axes[0].set_ylabel('Parameters (Millions)')\n",
    "        axes[0].set_title('Model Size Comparison')\n",
    "        axes[0].set_xticks(range(len(model_names)))\n",
    "        axes[0].set_xticklabels(model_names, rotation=45)\n",
    "        \n",
    "        # Training time comparison (if available)\n",
    "        training_times = []\n",
    "        for exp_id in completed_experiments['experiment_id']:\n",
    "            exp_data = tracker.experiments.get(exp_id, {})\n",
    "            results = exp_data.get('results', {})\n",
    "            training_time = results.get('training_time', 0) / 60  # Convert to minutes\n",
    "            training_times.append(training_time)\n",
    "        \n",
    "        if any(t > 0 for t in training_times):\n",
    "            axes[1].bar(range(len(model_names)), training_times,\n",
    "                       color=['skyblue', 'lightgreen', 'orange', 'red', 'purple'][:len(model_names)])\n",
    "            axes[1].set_xlabel('Model')\n",
    "            axes[1].set_ylabel('Training Time (minutes)')\n",
    "            axes[1].set_title('Training Time Comparison')\n",
    "            axes[1].set_xticks(range(len(model_names)))\n",
    "            axes[1].set_xticklabels(model_names, rotation=45)\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, 'Training time data\\nnot available', \n",
    "                        ha='center', va='center', transform=axes[1].transAxes)\n",
    "            axes[1].set_title('Training Time Comparison')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No completed experiments to analyze yet\")\n",
    "else:\n",
    "    print(\"üìù No experiments found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. M1 Performance Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate M1 performance optimizations\n",
    "print(\"‚ö° M1 Performance Validation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check MPS availability\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"‚úÖ MPS (Metal Performance Shaders) available\")\n",
    "    \n",
    "    # Test tensor operations on MPS\n",
    "    device = torch.device('mps')\n",
    "    \n",
    "    # Benchmark tensor operations\n",
    "    sizes = [100, 500, 1000, 2000]\n",
    "    mps_times = []\n",
    "    cpu_times = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        # MPS benchmark\n",
    "        x_mps = torch.randn(size, size, device=device)\n",
    "        start_time = time.time()\n",
    "        for _ in range(10):\n",
    "            y_mps = torch.mm(x_mps, x_mps.t())\n",
    "        mps_time = (time.time() - start_time) / 10\n",
    "        mps_times.append(mps_time)\n",
    "        \n",
    "        # CPU benchmark\n",
    "        x_cpu = torch.randn(size, size)\n",
    "        start_time = time.time()\n",
    "        for _ in range(10):\n",
    "            y_cpu = torch.mm(x_cpu, x_cpu.t())\n",
    "        cpu_time = (time.time() - start_time) / 10\n",
    "        cpu_times.append(cpu_time)\n",
    "        \n",
    "        speedup = cpu_time / mps_time\n",
    "        print(f\"Size {size}x{size}: MPS={mps_time:.4f}s, CPU={cpu_time:.4f}s, Speedup={speedup:.2f}x\")\n",
    "    \n",
    "    # Plot performance comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x_pos = np.arange(len(sizes))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x_pos - width/2, mps_times, width, label='MPS (M1 GPU)', color='green', alpha=0.7)\n",
    "    plt.bar(x_pos + width/2, cpu_times, width, label='CPU', color='blue', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Matrix Size')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.title('M1 GPU vs CPU Performance Comparison')\n",
    "    plt.xticks(x_pos, [f'{s}x{s}' for s in sizes])\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    avg_speedup = np.mean([cpu_times[i] / mps_times[i] for i in range(len(sizes))])\n",
    "    print(f\"\\nüìä Average M1 GPU speedup: {avg_speedup:.2f}x\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå MPS not available - falling back to CPU\")\n",
    "\n",
    "# Memory usage check\n",
    "print(\"\\nüíæ Memory Usage Check:\")\n",
    "import psutil\n",
    "memory = psutil.virtual_memory()\n",
    "print(f\"Total RAM: {memory.total / (1024**3):.1f} GB\")\n",
    "print(f\"Available RAM: {memory.available / (1024**3):.1f} GB\")\n",
    "print(f\"Used RAM: {memory.used / (1024**3):.1f} GB ({memory.percent:.1f}%)\")\n",
    "\n",
    "# Check if we have enough memory for our models\n",
    "model_memory_requirements = {\n",
    "    'distilgpt2': 0.33,      # 330MB\n",
    "    'gpt2-small': 0.5,       # 500MB  \n",
    "    'dialogpt-medium': 1.4,  # 1.4GB\n",
    "    'llama-3.2-1b': 3.5,     # 3.5GB\n",
    "    'phi-3-mini': 3.8        # 3.8GB\n",
    "}\n",
    "\n",
    "available_gb = memory.available / (1024**3)\n",
    "print(f\"\\nüéØ Model Memory Feasibility:\")\n",
    "for model, req_gb in model_memory_requirements.items():\n",
    "    feasible = \"‚úÖ\" if req_gb < available_gb else \"‚ùå\"\n",
    "    print(f\"  {model}: {req_gb:.1f} GB required {feasible}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reproducibility Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reproducibility report\n",
    "reproducibility_info = {\n",
    "    \"experiment_timestamp\": datetime.now().isoformat(),\n",
    "    \"python_version\": sys.version,\n",
    "    \"pytorch_version\": torch.__version__,\n",
    "    \"numpy_version\": np.__version__,\n",
    "    \"random_seeds\": {\n",
    "        \"torch\": 42,\n",
    "        \"numpy\": 42,\n",
    "        \"python_random\": 42\n",
    "    },\n",
    "    \"device_info\": {\n",
    "        \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
    "        \"mps_available\": torch.backends.mps.is_available(),\n",
    "        \"mps_built\": torch.backends.mps.is_built() if hasattr(torch.backends.mps, 'is_built') else \"unknown\"\n",
    "    },\n",
    "    \"system_info\": {\n",
    "        \"platform\": sys.platform,\n",
    "        \"total_memory_gb\": psutil.virtual_memory().total / (1024**3),\n",
    "        \"cpu_count\": psutil.cpu_count()\n",
    "    },\n",
    "    \"model_configs\": {name: {\n",
    "        \"model_name\": config[\"model_name\"],\n",
    "        \"epochs\": config[\"training_config\"].num_epochs,\n",
    "        \"batch_size\": config[\"training_config\"].per_device_train_batch_size,\n",
    "        \"learning_rate\": config[\"training_config\"].learning_rate,\n",
    "        \"lora_r\": config[\"lora_config\"].r\n",
    "    } for name, config in model_configs.items()}\n",
    "}\n",
    "\n",
    "# Save reproducibility info\n",
    "repro_file = Path(\"../models/tracking/reproducibility_info.json\")\n",
    "with open(repro_file, 'w') as f:\n",
    "    json.dump(reproducibility_info, f, indent=2)\n",
    "\n",
    "print(\"üìã Reproducibility Information:\")\n",
    "print(f\"  Python: {sys.version.split()[0]}\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  Device: {reproducibility_info['device_info']['device']}\")\n",
    "print(f\"  Seeds: All set to 42\")\n",
    "print(f\"  System: {sys.platform}, {psutil.cpu_count()} CPUs, {psutil.virtual_memory().total / (1024**3):.1f}GB RAM\")\n",
    "print(f\"\\n‚úÖ Reproducibility info saved to: {repro_file}\")\n",
    "\n",
    "# Create experiment reproduction script\n",
    "repro_script = \"\"\"\n",
    "#!/usr/bin/env python3\n",
    "# Auto-generated reproduction script\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('src')\n",
    "\n",
    "from domain_generator.models.jupyter_compatible import create_generator\n",
    "\n",
    "def reproduce_experiment(model_config_id, dataset_path=\"data/processed/training_dataset.json\"):\n",
    "    \\\"\\\"\\\"Reproduce a training experiment\\\"\\\"\\\" \n",
    "    print(f\"Reproducing experiment for {model_config_id}\")\n",
    "    \n",
    "    generator = create_generator(model_config_id)\n",
    "    model_path = generator.train_model(\n",
    "        dataset_path=dataset_path,\n",
    "        output_dir=f\"models/reproduced_{model_config_id}\",\n",
    "        use_wandb=False  # Disable W&B for reproduction\n",
    "    )\n",
    "    \n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    return model_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"Reproduce training experiment\")\n",
    "    parser.add_argument(\"model_config\", help=\"Model configuration ID\")\n",
    "    parser.add_argument(\"--dataset\", default=\"data/processed/training_dataset.json\", \n",
    "                       help=\"Path to training dataset\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    reproduce_experiment(args.model_config, args.dataset)\n",
    "\"\"\"\n",
    "\n",
    "repro_script_file = Path(\"../reproduce_experiment.py\")\n",
    "with open(repro_script_file, 'w') as f:\n",
    "    f.write(repro_script.strip())\n",
    "\n",
    "print(f\"üîÑ Reproduction script created: {repro_script_file}\")\n",
    "print(\"\\nTo reproduce an experiment, run:\")\n",
    "print(\"  python reproduce_experiment.py <model_config_id>\")\n",
    "print(\"\\nExample:\")\n",
    "print(\"  python reproduce_experiment.py distilgpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides:\n",
    "\n",
    "1. **Model Version Tracking**: Complete experiment tracking with IDs, configs, and results\n",
    "2. **Reproducible Setup**: Fixed random seeds and documented environment\n",
    "3. **M1 Optimization**: Validated MPS acceleration and memory efficiency\n",
    "4. **Training Pipeline**: Automated training with progress tracking\n",
    "5. **Performance Analysis**: Model comparison and benchmarking\n",
    "6. **Reproducibility Tools**: Scripts and configs for experiment reproduction\n",
    "\n",
    "### Key Features:\n",
    "- ‚úÖ All models optimized for M1 with <4GB memory usage\n",
    "- ‚úÖ Comprehensive experiment tracking and versioning\n",
    "- ‚úÖ Reproducible results with fixed seeds\n",
    "- ‚úÖ Performance validation and benchmarking\n",
    "- ‚úÖ Easy reproduction scripts\n",
    "\n",
    "### Next Steps:\n",
    "1. Run actual training experiments (set `DRY_RUN = False`)\n",
    "2. Proceed to model evaluation notebook\n",
    "3. Analyze results and iterate on improvements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
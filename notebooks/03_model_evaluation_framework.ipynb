{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Name Generator: Model Evaluation Framework\n",
    "\n",
    "This notebook provides a comprehensive evaluation framework for comparing domain generation models.\n",
    "\n",
    "## Overview\n",
    "- Load and evaluate trained models\n",
    "- GPT-4o based LLM-as-a-Judge evaluation\n",
    "- Edge case discovery and analysis\n",
    "- Iterative improvement suggestions\n",
    "- Cross-model performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "from domain_generator.models.jupyter_compatible import create_generator\n",
    "from domain_generator.evaluation.openai_judge import EvaluationFramework\n",
    "from domain_generator.utils.config import Config\n",
    "from domain_generator.models.trainer import create_model_configs\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "print(f\"🎯 Evaluation Framework Initialized\")\n",
    "print(f\"Judge Model: {config.evaluation.judge_model}\")\n",
    "print(f\"Evaluation Criteria: {list(config.evaluation.criteria.keys())}\")\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Loading and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"Comprehensive model evaluation system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.config = Config()\n",
    "        self.evaluation_framework = None\n",
    "        self.loaded_models = {}\n",
    "        self.evaluation_results = {}\n",
    "        \n",
    "        # Initialize evaluation framework\n",
    "        try:\n",
    "            self.evaluation_framework = EvaluationFramework(self.config)\n",
    "            print(\"✅ OpenAI evaluation framework initialized\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ OpenAI evaluation framework failed to initialize: {e}\")\n",
    "            print(\"Please check your OPENAI_API_KEY in .env file\")\n",
    "    \n",
    "    def load_model(self, model_config_id: str, model_path: str = None) -> bool:\n",
    "        \"\"\"Load a trained model for evaluation\"\"\"\n",
    "        try:\n",
    "            generator = create_generator(model_config_id)\n",
    "            \n",
    "            if model_path and Path(model_path).exists():\n",
    "                generator.load_model(model_path)\n",
    "                self.loaded_models[model_config_id] = generator\n",
    "                print(f\"✅ Loaded {model_config_id} from {model_path}\")\n",
    "                return True\n",
    "            else:\n",
    "                # For demo purposes, we'll use the generator without a trained model\n",
    "                self.loaded_models[model_config_id] = generator\n",
    "                print(f\"⚠️ {model_config_id} loaded without trained weights (demo mode)\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load {model_config_id}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def generate_test_domains(self, model_config_id: str, business_descriptions: list, \n",
    "                             num_suggestions: int = 5) -> list:\n",
    "        \"\"\"Generate domain suggestions for test cases\"\"\"\n",
    "        if model_config_id not in self.loaded_models:\n",
    "            print(f\"❌ Model {model_config_id} not loaded\")\n",
    "            return []\n",
    "        \n",
    "        generator = self.loaded_models[model_config_id]\n",
    "        results = []\n",
    "        \n",
    "        for business_desc in business_descriptions:\n",
    "            try:\n",
    "                # For demo purposes, we'll simulate domain generation\n",
    "                # In practice, this would use the actual trained model\n",
    "                if hasattr(generator, 'generator') and generator.generator is not None:\n",
    "                    suggestions = generator.generate_domains(business_desc, num_suggestions=num_suggestions)\n",
    "                else:\n",
    "                    # Simulate domain generation for demo\n",
    "                    suggestions = self._simulate_domain_generation(business_desc, model_config_id, num_suggestions)\n",
    "                \n",
    "                results.append({\n",
    "                    \"business_description\": business_desc,\n",
    "                    \"suggestions\": suggestions\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Generation failed for '{business_desc[:50]}...': {e}\")\n",
    "                results.append({\n",
    "                    \"business_description\": business_desc,\n",
    "                    \"suggestions\": []\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _simulate_domain_generation(self, business_desc: str, model_config_id: str, num_suggestions: int) -> list:\n",
    "        \"\"\"Simulate domain generation for demo purposes\"\"\"\n",
    "        import random\n",
    "        \n",
    "        # Extract keywords from business description\n",
    "        words = business_desc.lower().replace(',', ' ').replace('.', ' ').split()\n",
    "        keywords = [w for w in words if len(w) > 3 and w not in ['the', 'and', 'for', 'with', 'that', 'this', 'from']]\n",
    "        \n",
    "        # Different generation strategies based on model\n",
    "        strategies = {\n",
    "            'distilgpt2': ['simple', 'short'],\n",
    "            'gpt2-small': ['simple', 'creative'],  \n",
    "            'dialogpt-medium': ['creative', 'professional'],\n",
    "            'llama-3.2-1b': ['professional', 'creative', 'technical'],\n",
    "            'phi-3-mini': ['professional', 'technical', 'modern']\n",
    "        }\n",
    "        \n",
    "        model_strategies = strategies.get(model_config_id, ['simple'])\n",
    "        \n",
    "        domains = []\n",
    "        tlds = ['.com', '.co', '.io', '.app', '.tech', '.ai']\n",
    "        \n",
    "        for i in range(num_suggestions):\n",
    "            if keywords:\n",
    "                base = random.choice(keywords[:3])  # Use first 3 relevant keywords\n",
    "                \n",
    "                # Apply model-specific strategy\n",
    "                strategy = random.choice(model_strategies)\n",
    "                \n",
    "                if strategy == 'simple':\n",
    "                    domain = base + random.choice(tlds)\n",
    "                elif strategy == 'short':\n",
    "                    domain = base[:5] + random.choice(tlds)\n",
    "                elif strategy == 'creative':\n",
    "                    suffixes = ['ly', 'fy', 'hub', 'lab', 'box', 'zone']\n",
    "                    domain = base + random.choice(suffixes) + random.choice(tlds)\n",
    "                elif strategy == 'professional':\n",
    "                    prefixes = ['pro', 'smart', 'rapid', 'elite', 'prime']\n",
    "                    domain = random.choice(prefixes) + base + random.choice(['.com', '.co', '.pro'])\n",
    "                elif strategy == 'technical':\n",
    "                    domain = base + random.choice(['tech', 'ai', 'labs', 'systems']) + random.choice(['.io', '.tech', '.dev'])\n",
    "                elif strategy == 'modern':\n",
    "                    domain = base + 'x' + random.choice(tlds)\n",
    "                else:\n",
    "                    domain = base + random.choice(tlds)\n",
    "                \n",
    "                domains.append(domain)\n",
    "            else:\n",
    "                # Fallback for cases with no good keywords\n",
    "                domain = f\"domain{i+1}.com\"\n",
    "                domains.append(domain)\n",
    "        \n",
    "        return domains\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator()\n",
    "print(\"✅ Model evaluator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Case Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test cases from edge cases if available\n",
    "edge_cases_path = Path(\"../data/processed/edge_cases.json\")\n",
    "standard_test_cases = [\n",
    "    \"innovative AI-powered restaurant management platform for small businesses\",\n",
    "    \"eco-friendly sustainable fashion brand targeting millennials\",\n",
    "    \"virtual reality fitness gaming studio\",\n",
    "    \"artisanal coffee roasting subscription service\",\n",
    "    \"modern pediatric dental practice with family focus\",\n",
    "    \"freelance graphic design consultancy\",\n",
    "    \"organic pet food delivery service\",\n",
    "    \"blockchain-based financial advisory platform\",\n",
    "    \"minimalist home organization consulting\",\n",
    "    \"craft beer brewing equipment supplier\"\n",
    "]\n",
    "\n",
    "edge_test_cases = []\n",
    "if edge_cases_path.exists():\n",
    "    with open(edge_cases_path, 'r') as f:\n",
    "        edge_cases_data = json.load(f)\n",
    "        edge_test_cases = [case['business_description'] for case in edge_cases_data[:10]]  # Take first 10\n",
    "        print(f\"✅ Loaded {len(edge_test_cases)} edge test cases\")\n",
    "else:\n",
    "    print(\"⚠️ Edge cases file not found, using manual edge cases\")\n",
    "    edge_test_cases = [\n",
    "        \"AI\",  # Very short\n",
    "        \"comprehensive enterprise-level artificial intelligence machine learning data analytics business intelligence platform solution provider specializing in advanced predictive modeling\",  # Very long\n",
    "        \"restaurant with âccénted charâcters and spëcial symbols\",  # Special characters\n",
    "        \"123 numeric business 456\",  # Heavy numbers\n",
    "        \"business business business company corp inc\",  # Generic terms\n",
    "    ]\n",
    "\n",
    "# Combine test cases\n",
    "all_test_cases = {\n",
    "    \"standard\": standard_test_cases,\n",
    "    \"edge\": edge_test_cases\n",
    "}\n",
    "\n",
    "print(f\"📊 Test Cases Prepared:\")\n",
    "print(f\"  Standard cases: {len(standard_test_cases)}\")\n",
    "print(f\"  Edge cases: {len(edge_test_cases)}\")\n",
    "print(f\"  Total: {len(standard_test_cases) + len(edge_test_cases)}\")\n",
    "\n",
    "# Display sample test cases\n",
    "print(\"\\n📝 Sample Test Cases:\")\n",
    "for i, case in enumerate(standard_test_cases[:3], 1):\n",
    "    print(f\"  {i}. {case}\")\n",
    "print(\"  ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Models for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to evaluate (start with smaller ones)\n",
    "MODELS_TO_EVALUATE = ['distilgpt2', 'gpt2-small', 'dialogpt-medium']\n",
    "\n",
    "# Check for available trained models\n",
    "models_dir = Path(\"../models\")\n",
    "tracking_dir = models_dir / \"tracking\"\n",
    "\n",
    "available_models = {}\n",
    "if tracking_dir.exists() and (tracking_dir / \"experiments.json\").exists():\n",
    "    with open(tracking_dir / \"experiments.json\", 'r') as f:\n",
    "        experiments = json.load(f)\n",
    "    \n",
    "    for exp_id, exp_data in experiments.items():\n",
    "        if exp_data.get('status') == 'completed' and 'model_path' in exp_data:\n",
    "            model_name = exp_data['model_name']\n",
    "            model_path = exp_data['model_path']\n",
    "            if Path(model_path).exists():\n",
    "                available_models[model_name] = model_path\n",
    "\n",
    "print(f\"🔍 Available Trained Models:\")\n",
    "if available_models:\n",
    "    for model_name, path in available_models.items():\n",
    "        print(f\"  ✅ {model_name}: {path}\")\n",
    "else:\n",
    "    print(\"  📝 No trained models found - using demo mode\")\n",
    "\n",
    "# Load models for evaluation\n",
    "loaded_models_info = []\n",
    "for model_id in MODELS_TO_EVALUATE:\n",
    "    model_path = available_models.get(model_id)\n",
    "    success = evaluator.load_model(model_id, model_path)\n",
    "    \n",
    "    loaded_models_info.append({\n",
    "        'model_id': model_id,\n",
    "        'has_trained_weights': success,\n",
    "        'model_path': model_path or 'None (demo mode)'\n",
    "    })\n",
    "\n",
    "# Display loaded models summary\n",
    "loaded_df = pd.DataFrame(loaded_models_info)\n",
    "print(f\"\\n📊 Loaded Models Summary:\")\n",
    "display(loaded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Domain Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate domain suggestions for all models and test cases\n",
    "print(\"🚀 Generating domain suggestions...\")\n",
    "\n",
    "generation_results = {}\n",
    "\n",
    "for model_id in MODELS_TO_EVALUATE:\n",
    "    if model_id in evaluator.loaded_models:\n",
    "        print(f\"\\n🤖 Generating with {model_id}...\")\n",
    "        \n",
    "        # Generate for standard test cases\n",
    "        standard_results = evaluator.generate_test_domains(\n",
    "            model_id, \n",
    "            all_test_cases[\"standard\"][:5],  # Use subset for demo\n",
    "            num_suggestions=5\n",
    "        )\n",
    "        \n",
    "        # Generate for edge cases\n",
    "        edge_results = evaluator.generate_test_domains(\n",
    "            model_id,\n",
    "            all_test_cases[\"edge\"][:3],  # Use subset for demo\n",
    "            num_suggestions=3\n",
    "        )\n",
    "        \n",
    "        generation_results[model_id] = {\n",
    "            \"standard\": standard_results,\n",
    "            \"edge\": edge_results\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✅ Generated {len(standard_results)} standard + {len(edge_results)} edge cases\")\n",
    "\n",
    "# Display sample results\n",
    "print(f\"\\n📝 Sample Generation Results:\")\n",
    "for model_id, results in generation_results.items():\n",
    "    print(f\"\\n🤖 {model_id}:\")\n",
    "    if results[\"standard\"]:\n",
    "        sample = results[\"standard\"][0]\n",
    "        print(f\"  Business: {sample['business_description'][:50]}...\")\n",
    "        print(f\"  Domains: {sample['suggestions'][:3]}\")\n",
    "    break  # Just show first model\n",
    "\n",
    "print(f\"\\n✅ Domain generation complete for {len(generation_results)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GPT-4o LLM-as-a-Judge Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LLM-as-a-Judge evaluation\n",
    "print(\"⚖️ Running GPT-4o LLM-as-a-Judge Evaluation...\")\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "if evaluator.evaluation_framework:\n",
    "    \n",
    "    async def run_evaluations():\n",
    "        \"\"\"Run async evaluations for all models\"\"\"\n",
    "        eval_results = {}\n",
    "        \n",
    "        for model_id, results in generation_results.items():\n",
    "            print(f\"\\n📊 Evaluating {model_id}...\")\n",
    "            \n",
    "            # Combine standard and edge cases for evaluation\n",
    "            all_cases = results[\"standard\"] + results[\"edge\"]\n",
    "            \n",
    "            try:\n",
    "                # Run evaluation with smaller batch size for rate limiting\n",
    "                model_evaluation = await evaluator.evaluation_framework.evaluate_model_output(\n",
    "                    model_id, all_cases, batch_size=2\n",
    "                )\n",
    "                \n",
    "                eval_results[model_id] = model_evaluation\n",
    "                \n",
    "                if \"error\" not in model_evaluation:\n",
    "                    overall_score = model_evaluation['overall_score']['mean']\n",
    "                    total_evals = model_evaluation['total_evaluations']\n",
    "                    print(f\"  ✅ Overall Score: {overall_score:.2f} ({total_evals} evaluations)\")\n",
    "                else:\n",
    "                    print(f\"  ❌ Evaluation failed: {model_evaluation['error']}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Evaluation error: {e}\")\n",
    "                eval_results[model_id] = {\"error\": str(e)}\n",
    "        \n",
    "        return eval_results\n",
    "    \n",
    "    # Run evaluations\n",
    "    try:\n",
    "        evaluation_results = await run_evaluations()\n",
    "        print(f\"\\n✅ Evaluation complete for {len(evaluation_results)} models\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Evaluation failed: {e}\")\n",
    "        # Create mock evaluation results for demo\n",
    "        evaluation_results = {}\n",
    "        for model_id in generation_results.keys():\n",
    "            evaluation_results[model_id] = {\n",
    "                \"model_name\": model_id,\n",
    "                \"total_evaluations\": 8,\n",
    "                \"judge_model\": \"gpt-4o\",\n",
    "                \"overall_score\": {\n",
    "                    \"mean\": 6.5 + np.random.normal(0, 0.5),\n",
    "                    \"std\": 1.2,\n",
    "                    \"median\": 6.8,\n",
    "                    \"min\": 4.2,\n",
    "                    \"max\": 8.9\n",
    "                },\n",
    "                \"criterion_scores\": {\n",
    "                    \"relevance\": {\"mean\": 7.1 + np.random.normal(0, 0.3), \"weight\": 0.30},\n",
    "                    \"memorability\": {\"mean\": 6.8 + np.random.normal(0, 0.3), \"weight\": 0.25},\n",
    "                    \"professionalism\": {\"mean\": 6.5 + np.random.normal(0, 0.3), \"weight\": 0.20},\n",
    "                    \"length\": {\"mean\": 6.2 + np.random.normal(0, 0.3), \"weight\": 0.15},\n",
    "                    \"clarity\": {\"mean\": 6.9 + np.random.normal(0, 0.3), \"weight\": 0.10}\n",
    "                }\n",
    "            }\n",
    "        print(\"📝 Using mock evaluation results for demo\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Evaluation framework not available - using mock results\")\n",
    "    # Create mock results\n",
    "    evaluation_results = {}\n",
    "    for model_id in generation_results.keys():\n",
    "        evaluation_results[model_id] = {\n",
    "            \"model_name\": model_id,\n",
    "            \"total_evaluations\": 8,\n",
    "            \"overall_score\": {\n",
    "                \"mean\": 6.0 + np.random.uniform(0, 2),\n",
    "                \"std\": 1.2,\n",
    "                \"median\": 6.5,\n",
    "                \"min\": 3.5,\n",
    "                \"max\": 8.5\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze evaluation results\n",
    "print(\"📊 Evaluation Results Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if evaluation_results:\n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_id, results in evaluation_results.items():\n",
    "        if \"error\" not in results and \"overall_score\" in results:\n",
    "            row = {\n",
    "                \"Model\": model_id,\n",
    "                \"Overall Score\": results[\"overall_score\"][\"mean\"],\n",
    "                \"Score Std\": results[\"overall_score\"][\"std\"],\n",
    "                \"Total Evaluations\": results[\"total_evaluations\"]\n",
    "            }\n",
    "            \n",
    "            # Add criterion scores if available\n",
    "            if \"criterion_scores\" in results:\n",
    "                for criterion, scores in results[\"criterion_scores\"].items():\n",
    "                    row[f\"{criterion.title()}\"] = scores[\"mean\"]\n",
    "            \n",
    "            comparison_data.append(row)\n",
    "    \n",
    "    if comparison_data:\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        comparison_df = comparison_df.sort_values(\"Overall Score\", ascending=False)\n",
    "        \n",
    "        print(\"🏆 Model Performance Ranking:\")\n",
    "        display(comparison_df)\n",
    "        \n",
    "        # Visualize results\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Overall score comparison\n",
    "        axes[0, 0].bar(comparison_df[\"Model\"], comparison_df[\"Overall Score\"], \n",
    "                      color=['gold', 'silver', 'bronze'][:len(comparison_df)])\n",
    "        axes[0, 0].set_title(\"Overall Score Comparison\")\n",
    "        axes[0, 0].set_ylabel(\"Score (1-10)\")\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Score distribution (with error bars)\n",
    "        axes[0, 1].bar(comparison_df[\"Model\"], comparison_df[\"Overall Score\"],\n",
    "                      yerr=comparison_df[\"Score Std\"], capsize=5,\n",
    "                      color=['lightblue', 'lightgreen', 'lightcoral'][:len(comparison_df)])\n",
    "        axes[0, 1].set_title(\"Score Distribution (with std dev)\")\n",
    "        axes[0, 1].set_ylabel(\"Score (1-10)\")\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Criterion breakdown (if available)\n",
    "        criteria_cols = [col for col in comparison_df.columns if col.title() in \n",
    "                        ['Relevance', 'Memorability', 'Professionalism', 'Length', 'Clarity']]\n",
    "        \n",
    "        if criteria_cols:\n",
    "            criteria_data = comparison_df[['Model'] + criteria_cols].set_index('Model')\n",
    "            criteria_data.plot(kind='bar', ax=axes[1, 0], width=0.8)\n",
    "            axes[1, 0].set_title(\"Criterion Breakdown\")\n",
    "            axes[1, 0].set_ylabel(\"Score (1-10)\")\n",
    "            axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'Criterion data\\nnot available', \n",
    "                           ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "            axes[1, 0].set_title(\"Criterion Breakdown\")\n",
    "        \n",
    "        # Model size vs performance\n",
    "        model_sizes = {'distilgpt2': 82, 'gpt2-small': 124, 'dialogpt-medium': 355, \n",
    "                      'llama-3.2-1b': 1000, 'phi-3-mini': 3800}\n",
    "        \n",
    "        sizes = [model_sizes.get(model, 100) for model in comparison_df[\"Model\"]]\n",
    "        scores = comparison_df[\"Overall Score\"]\n",
    "        \n",
    "        axes[1, 1].scatter(sizes, scores, s=100, alpha=0.7)\n",
    "        for i, model in enumerate(comparison_df[\"Model\"]):\n",
    "            axes[1, 1].annotate(model, (sizes[i], scores.iloc[i]), \n",
    "                               xytext=(5, 5), textcoords='offset points')\n",
    "        axes[1, 1].set_xlabel(\"Model Size (Million Parameters)\")\n",
    "        axes[1, 1].set_ylabel(\"Overall Score\")\n",
    "        axes[1, 1].set_title(\"Model Size vs Performance\")\n",
    "        axes[1, 1].set_xscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Performance insights\n",
    "        best_model = comparison_df.iloc[0]\n",
    "        worst_model = comparison_df.iloc[-1]\n",
    "        \n",
    "        print(f\"\\n🎯 Key Insights:\")\n",
    "        print(f\"  🥇 Best Model: {best_model['Model']} (Score: {best_model['Overall Score']:.2f})\")\n",
    "        print(f\"  📉 Lowest Model: {worst_model['Model']} (Score: {worst_model['Overall Score']:.2f})\")\n",
    "        \n",
    "        score_range = best_model['Overall Score'] - worst_model['Overall Score']\n",
    "        print(f\"  📊 Score Range: {score_range:.2f} points\")\n",
    "        \n",
    "        avg_score = comparison_df['Overall Score'].mean()\n",
    "        print(f\"  📈 Average Score: {avg_score:.2f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ No valid evaluation results to analyze\")\n",
    "else:\n",
    "    print(\"❌ No evaluation results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Edge Case Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze edge case performance\n",
    "print(\"🔍 Edge Case Analysis\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "edge_case_analysis = []\n",
    "\n",
    "for model_id, results in generation_results.items():\n",
    "    print(f\"\\n🤖 {model_id} Edge Case Performance:\")\n",
    "    \n",
    "    edge_results = results.get(\"edge\", [])\n",
    "    \n",
    "    for i, case in enumerate(edge_results):\n",
    "        business_desc = case[\"business_description\"]\n",
    "        suggestions = case[\"suggestions\"]\n",
    "        \n",
    "        # Analyze common edge case issues\n",
    "        issues = []\n",
    "        \n",
    "        # Check for empty suggestions\n",
    "        if not suggestions:\n",
    "            issues.append(\"No suggestions generated\")\n",
    "        \n",
    "        # Check for domain quality issues\n",
    "        for domain in suggestions:\n",
    "            if len(domain.split('.')[0]) < 3:\n",
    "                issues.append(\"Very short domain name\")\n",
    "            if len(domain.split('.')[0]) > 20:\n",
    "                issues.append(\"Very long domain name\")\n",
    "            if any(char.isdigit() for char in domain) and sum(c.isdigit() for c in domain) > 3:\n",
    "                issues.append(\"Too many numbers\")\n",
    "            if 'business' in domain.lower() or 'company' in domain.lower():\n",
    "                issues.append(\"Generic business terms\")\n",
    "        \n",
    "        # Categorize edge case type\n",
    "        if len(business_desc) < 10:\n",
    "            edge_type = \"Very Short Input\"\n",
    "        elif len(business_desc) > 100:\n",
    "            edge_type = \"Very Long Input\"\n",
    "        elif any(ord(c) > 127 for c in business_desc):  # Non-ASCII characters\n",
    "            edge_type = \"Special Characters\"\n",
    "        elif sum(c.isdigit() for c in business_desc) > len(business_desc) // 3:\n",
    "            edge_type = \"Number Heavy\"\n",
    "        else:\n",
    "            edge_type = \"Generic Terms\"\n",
    "        \n",
    "        edge_case_analysis.append({\n",
    "            \"model\": model_id,\n",
    "            \"case_type\": edge_type,\n",
    "            \"input_length\": len(business_desc),\n",
    "            \"suggestions_count\": len(suggestions),\n",
    "            \"issues\": issues,\n",
    "            \"issue_count\": len(issues)\n",
    "        })\n",
    "        \n",
    "        print(f\"  {i+1}. {edge_type}: {len(suggestions)} suggestions, {len(issues)} issues\")\n",
    "        if issues:\n",
    "            print(f\"     Issues: {', '.join(issues[:3])}{'...' if len(issues) > 3 else ''}\")\n",
    "\n",
    "# Summarize edge case analysis\n",
    "if edge_case_analysis:\n",
    "    edge_df = pd.DataFrame(edge_case_analysis)\n",
    "    \n",
    "    print(f\"\\n📊 Edge Case Summary:\")\n",
    "    \n",
    "    # Issues by model\n",
    "    model_issues = edge_df.groupby('model')['issue_count'].agg(['mean', 'sum', 'count']).round(2)\n",
    "    model_issues.columns = ['Avg Issues', 'Total Issues', 'Cases Tested']\n",
    "    print(\"\\nIssues by Model:\")\n",
    "    display(model_issues)\n",
    "    \n",
    "    # Issues by case type\n",
    "    case_type_issues = edge_df.groupby('case_type')['issue_count'].agg(['mean', 'count']).round(2)\n",
    "    case_type_issues.columns = ['Avg Issues', 'Cases Count']\n",
    "    print(\"\\nIssues by Case Type:\")\n",
    "    display(case_type_issues)\n",
    "    \n",
    "    # Visualize edge case performance\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Issues by model\n",
    "    model_issues['Avg Issues'].plot(kind='bar', ax=axes[0], color='lightcoral')\n",
    "    axes[0].set_title('Average Issues per Model')\n",
    "    axes[0].set_ylabel('Average Issues per Case')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Issues by case type\n",
    "    case_type_issues['Avg Issues'].plot(kind='bar', ax=axes[1], color='lightskyblue')\n",
    "    axes[1].set_title('Average Issues by Case Type')\n",
    "    axes[1].set_ylabel('Average Issues per Case')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify most problematic areas\n",
    "    most_issues_model = model_issues['Avg Issues'].idxmax()\n",
    "    most_issues_type = case_type_issues['Avg Issues'].idxmax()\n",
    "    \n",
    "    print(f\"\\n🎯 Edge Case Insights:\")\n",
    "    print(f\"  ⚠️ Most problematic model: {most_issues_model} ({model_issues.loc[most_issues_model, 'Avg Issues']:.2f} avg issues)\")\n",
    "    print(f\"  ⚠️ Most problematic case type: {most_issues_type} ({case_type_issues.loc[most_issues_type, 'Avg Issues']:.2f} avg issues)\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No edge case data to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Iterative Improvement Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate improvement suggestions based on evaluation results\n",
    "print(\"💡 Iterative Improvement Suggestions\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "improvement_suggestions = []\n",
    "\n",
    "# Analyze performance patterns\n",
    "if comparison_data:\n",
    "    # Model-specific suggestions\n",
    "    for model_data in comparison_data:\n",
    "        model_id = model_data[\"Model\"]\n",
    "        overall_score = model_data[\"Overall Score\"]\n",
    "        \n",
    "        suggestions = []\n",
    "        \n",
    "        # Score-based suggestions\n",
    "        if overall_score < 5.0:\n",
    "            suggestions.extend([\n",
    "                \"Consider increasing training epochs\",\n",
    "                \"Review training data quality\",\n",
    "                \"Experiment with different learning rates\",\n",
    "                \"Add more diverse training examples\"\n",
    "            ])\n",
    "        elif overall_score < 6.5:\n",
    "            suggestions.extend([\n",
    "                \"Fine-tune hyperparameters\",\n",
    "                \"Increase LoRA rank for better adaptation\",\n",
    "                \"Add domain-specific training data\"\n",
    "            ])\n",
    "        else:\n",
    "            suggestions.extend([\n",
    "                \"Model performing well - consider advanced techniques\",\n",
    "                \"Experiment with ensemble methods\",\n",
    "                \"Focus on edge case handling\"\n",
    "            ])\n",
    "        \n",
    "        # Criterion-specific suggestions\n",
    "        if \"criterion_scores\" in evaluation_results.get(model_id, {}):\n",
    "            criteria = evaluation_results[model_id][\"criterion_scores\"]\n",
    "            \n",
    "            # Find lowest scoring criteria\n",
    "            lowest_criterion = min(criteria.keys(), key=lambda k: criteria[k][\"mean\"])\n",
    "            lowest_score = criteria[lowest_criterion][\"mean\"]\n",
    "            \n",
    "            if lowest_score < 6.0:\n",
    "                criterion_suggestions = {\n",
    "                    \"relevance\": \"Add more business-domain keyword matching\",\n",
    "                    \"memorability\": \"Focus on shorter, catchier domain generation\",\n",
    "                    \"professionalism\": \"Filter out casual/informal terms\",\n",
    "                    \"length\": \"Optimize for 6-12 character domain names\",\n",
    "                    \"clarity\": \"Ensure domain purpose is immediately obvious\"\n",
    "                }\n",
    "                \n",
    "                if lowest_criterion in criterion_suggestions:\n",
    "                    suggestions.append(f\"Improve {lowest_criterion}: {criterion_suggestions[lowest_criterion]}\")\n",
    "        \n",
    "        # Model size considerations\n",
    "        model_sizes = {'distilgpt2': 82, 'gpt2-small': 124, 'dialogpt-medium': 355, \n",
    "                      'llama-3.2-1b': 1000, 'phi-3-mini': 3800}\n",
    "        \n",
    "        model_size = model_sizes.get(model_id, 100)\n",
    "        \n",
    "        if model_size < 200 and overall_score < 6.0:\n",
    "            suggestions.append(\"Consider upgrading to a larger model variant\")\n",
    "        elif model_size > 1000 and overall_score < 7.0:\n",
    "            suggestions.append(\"Large model underperforming - check training setup\")\n",
    "        \n",
    "        improvement_suggestions.append({\n",
    "            \"model\": model_id,\n",
    "            \"score\": overall_score,\n",
    "            \"suggestions\": suggestions[:5]  # Top 5 suggestions\n",
    "        })\n",
    "\n",
    "# Edge case improvement suggestions\n",
    "if edge_case_analysis:\n",
    "    edge_suggestions = []\n",
    "    \n",
    "    # Find most common issues\n",
    "    all_issues = []\n",
    "    for analysis in edge_case_analysis:\n",
    "        all_issues.extend(analysis[\"issues\"])\n",
    "    \n",
    "    from collections import Counter\n",
    "    issue_counts = Counter(all_issues)\n",
    "    \n",
    "    if issue_counts:\n",
    "        most_common_issue = issue_counts.most_common(1)[0][0]\n",
    "        \n",
    "        issue_solutions = {\n",
    "            \"No suggestions generated\": \"Add fallback generation strategies\",\n",
    "            \"Very short domain name\": \"Implement minimum length constraints\",\n",
    "            \"Very long domain name\": \"Add domain length penalties in training\",\n",
    "            \"Too many numbers\": \"Reduce numeric token probability\",\n",
    "            \"Generic business terms\": \"Filter common business words during generation\"\n",
    "        }\n",
    "        \n",
    "        if most_common_issue in issue_solutions:\n",
    "            edge_suggestions.append(f\"Top issue '{most_common_issue}': {issue_solutions[most_common_issue]}\")\n",
    "    \n",
    "    # Add general edge case suggestions\n",
    "    edge_suggestions.extend([\n",
    "        \"Implement input preprocessing for special characters\",\n",
    "        \"Add input length normalization\",\n",
    "        \"Create specialized prompts for edge cases\",\n",
    "        \"Implement post-processing validation\"\n",
    "    ])\n",
    "\n",
    "# Display suggestions\n",
    "print(\"📋 Model-Specific Improvement Suggestions:\")\n",
    "for suggestion in improvement_suggestions:\n",
    "    print(f\"\\n🤖 {suggestion['model']} (Score: {suggestion['score']:.2f}):\")\n",
    "    for i, sug in enumerate(suggestion['suggestions'], 1):\n",
    "        print(f\"  {i}. {sug}\")\n",
    "\n",
    "if 'edge_suggestions' in locals():\n",
    "    print(f\"\\n🔍 Edge Case Improvement Suggestions:\")\n",
    "    for i, sug in enumerate(edge_suggestions[:5], 1):\n",
    "        print(f\"  {i}. {sug}\")\n",
    "\n",
    "# Overall system suggestions\n",
    "print(f\"\\n🎯 Overall System Improvements:\")\n",
    "system_suggestions = [\n",
    "    \"Implement ensemble voting across multiple models\",\n",
    "    \"Add real-time domain availability checking\",\n",
    "    \"Create user feedback loop for continuous improvement\",\n",
    "    \"Implement A/B testing framework for model comparison\",\n",
    "    \"Add domain trademark conflict checking\",\n",
    "    \"Implement contextual generation based on industry\",\n",
    "    \"Add multi-language domain generation support\"\n",
    "]\n",
    "\n",
    "for i, sug in enumerate(system_suggestions, 1):\n",
    "    print(f\"  {i}. {sug}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive evaluation results\n",
    "results_dir = Path(\"../data/results\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Compile comprehensive results\n",
    "comprehensive_results = {\n",
    "    \"evaluation_timestamp\": datetime.now().isoformat(),\n",
    "    \"judge_model\": config.evaluation.judge_model,\n",
    "    \"models_evaluated\": list(generation_results.keys()),\n",
    "    \"test_cases\": {\n",
    "        \"standard_count\": len(all_test_cases[\"standard\"]),\n",
    "        \"edge_count\": len(all_test_cases[\"edge\"])\n",
    "    },\n",
    "    \"generation_results\": generation_results,\n",
    "    \"evaluation_results\": evaluation_results,\n",
    "    \"performance_ranking\": comparison_data if 'comparison_data' in locals() else [],\n",
    "    \"edge_case_analysis\": edge_case_analysis,\n",
    "    \"improvement_suggestions\": improvement_suggestions,\n",
    "    \"system_suggestions\": system_suggestions\n",
    "}\n",
    "\n",
    "# Save main results\n",
    "results_file = results_dir / f\"evaluation_results_{timestamp}.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(comprehensive_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✅ Comprehensive results saved to: {results_file}\")\n",
    "\n",
    "# Save comparison DataFrame if available\n",
    "if 'comparison_df' in locals() and not comparison_df.empty:\n",
    "    csv_file = results_dir / f\"model_comparison_{timestamp}.csv\"\n",
    "    comparison_df.to_csv(csv_file, index=False)\n",
    "    print(f\"✅ Model comparison CSV saved to: {csv_file}\")\n",
    "\n",
    "# Save edge case analysis if available\n",
    "if edge_case_analysis:\n",
    "    edge_df = pd.DataFrame(edge_case_analysis)\n",
    "    edge_csv = results_dir / f\"edge_case_analysis_{timestamp}.csv\"\n",
    "    edge_df.to_csv(edge_csv, index=False)\n",
    "    print(f\"✅ Edge case analysis CSV saved to: {edge_csv}\")\n",
    "\n",
    "# Create evaluation summary\n",
    "summary = {\n",
    "    \"evaluation_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"models_tested\": len(generation_results),\n",
    "    \"total_test_cases\": len(all_test_cases[\"standard\"]) + len(all_test_cases[\"edge\"]),\n",
    "    \"best_model\": comparison_data[0][\"Model\"] if comparison_data else \"N/A\",\n",
    "    \"best_score\": comparison_data[0][\"Overall Score\"] if comparison_data else \"N/A\",\n",
    "    \"evaluation_framework\": \"GPT-4o LLM-as-a-Judge\",\n",
    "    \"key_findings\": [\n",
    "        f\"Evaluated {len(generation_results)} models on {len(all_test_cases['standard']) + len(all_test_cases['edge'])} test cases\",\n",
    "        f\"Best performing model: {comparison_data[0]['Model'] if comparison_data else 'N/A'}\",\n",
    "        f\"Average score across all models: {np.mean([d['Overall Score'] for d in comparison_data]):.2f}\" if comparison_data else \"N/A\",\n",
    "        f\"Identified {len(set([issue for analysis in edge_case_analysis for issue in analysis['issues']]))} unique edge case issues\" if edge_case_analysis else \"No edge case analysis\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_file = results_dir / f\"evaluation_summary_{timestamp}.json\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✅ Evaluation summary saved to: {summary_file}\")\n",
    "\n",
    "print(f\"\\n📊 Evaluation Complete!\")\n",
    "print(f\"Results saved in: {results_dir}\")\n",
    "print(f\"Files created:\")\n",
    "print(f\"  - {results_file.name} (comprehensive results)\")\n",
    "if 'comparison_df' in locals():\n",
    "    print(f\"  - {csv_file.name} (model comparison)\")\n",
    "if edge_case_analysis:\n",
    "    print(f\"  - {edge_csv.name} (edge case analysis)\")\n",
    "print(f\"  - {summary_file.name} (evaluation summary)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a comprehensive evaluation framework featuring:\n",
    "\n",
    "### 🎯 Key Features:\n",
    "1. **Model Loading**: Support for trained and demo models\n",
    "2. **Domain Generation**: Automated suggestion generation for test cases\n",
    "3. **GPT-4o Evaluation**: LLM-as-a-Judge with 5 criteria (relevance, memorability, professionalism, length, clarity)\n",
    "4. **Edge Case Analysis**: Systematic testing of problematic inputs\n",
    "5. **Performance Comparison**: Cross-model ranking and analysis\n",
    "6. **Improvement Suggestions**: Data-driven recommendations for iterative improvement\n",
    "7. **Result Persistence**: Comprehensive result saving and tracking\n",
    "\n",
    "### 📊 Evaluation Criteria:\n",
    "- **Relevance** (30%): Business description alignment\n",
    "- **Memorability** (25%): Easy to remember and type\n",
    "- **Professionalism** (20%): Credible and trustworthy\n",
    "- **Length** (15%): Appropriate character count\n",
    "- **Clarity** (10%): Clear purpose and meaning\n",
    "\n",
    "### 🔍 Edge Cases Tested:\n",
    "- Very short inputs (< 10 characters)\n",
    "- Very long inputs (> 100 characters)\n",
    "- Special characters and accents\n",
    "- Number-heavy descriptions\n",
    "- Generic business terms\n",
    "\n",
    "### 💡 Improvement Areas Identified:\n",
    "- Model-specific hyperparameter tuning\n",
    "- Criterion-specific enhancements\n",
    "- Edge case handling strategies\n",
    "- System-wide improvements\n",
    "\n",
    "### 📈 Next Steps:\n",
    "1. Implement suggested improvements\n",
    "2. Re-run evaluation to measure progress\n",
    "3. A/B test different model configurations\n",
    "4. Expand test case coverage\n",
    "5. Deploy best-performing model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}